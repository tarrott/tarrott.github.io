{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Engineering Wiki Gardening (Projects) Blacksmith Shop (Service) Sign your work a professional pride of ownership - \"I stand behind my work\" an indicator of quality, people should see the signature and expect the work to be solid, well written, tested, and documented present the end story - visuals/graphs/charts - for managers/clients what they want/are excited to see dig into the weeds (text) afterswards collect and visualize KPIs that showed how a business function performed before and after your consulting / product / service What does it take to be a top performer? - KPIs/Metrics the customers expectations at the time of delivery and what's in the actual delivery the customers expectations are evolving as much as the project is evolving you need to spend as much time managing customers expectations as you do on the project itself Guild System (Career) Aprentice Bachelor's Degree Learn one new language and technology every year Read a technical book a quarter Journeyman Master's Degree Open-Source Software Participate in local user groups Experiment with different environments Stay current (industry leaders, blogs, journals, trade magazines, newsgroups) Master","title":"Engineering Wiki"},{"location":"#engineering-wiki","text":"","title":"Engineering Wiki"},{"location":"#gardening-projects","text":"","title":"Gardening (Projects)"},{"location":"#blacksmith-shop-service","text":"Sign your work a professional pride of ownership - \"I stand behind my work\" an indicator of quality, people should see the signature and expect the work to be solid, well written, tested, and documented present the end story - visuals/graphs/charts - for managers/clients what they want/are excited to see dig into the weeds (text) afterswards collect and visualize KPIs that showed how a business function performed before and after your consulting / product / service What does it take to be a top performer? - KPIs/Metrics the customers expectations at the time of delivery and what's in the actual delivery the customers expectations are evolving as much as the project is evolving you need to spend as much time managing customers expectations as you do on the project itself","title":"Blacksmith Shop (Service)"},{"location":"#guild-system-career","text":"","title":"Guild System (Career)"},{"location":"#aprentice","text":"Bachelor's Degree Learn one new language and technology every year Read a technical book a quarter","title":"Aprentice"},{"location":"#journeyman","text":"Master's Degree Open-Source Software Participate in local user groups Experiment with different environments Stay current (industry leaders, blogs, journals, trade magazines, newsgroups)","title":"Journeyman"},{"location":"#master","text":"","title":"Master"},{"location":"design/","text":"Design Drawing Picture Design Color Theory Typography UI / UX","title":"Design"},{"location":"design/#design","text":"","title":"Design"},{"location":"design/#drawing","text":"","title":"Drawing"},{"location":"design/#picture-design","text":"","title":"Picture Design"},{"location":"design/#color-theory","text":"","title":"Color Theory"},{"location":"design/#typography","text":"","title":"Typography"},{"location":"design/#ui-ux","text":"","title":"UI / UX"},{"location":"resources/","text":"Curriculum Books Name Author Field ~The Pragmatic Programmer Andrew Hunt & David Thomas Career ~Soft Skills John Sonmez Career Cracking the Code Interview Laakman McDowell Career The Linux Command Lines William E. Shotts Jr. Linux Designing Data-Intensive Apps... Martin Kleppmann Architecture Analysis Patterns Martin Fowler Architecture Patterns of Enterprise App Architeture Martin Fowler Architecture Designing Distributed Systems Brenden Burns Architecture Kubernetes Up and Running Brenden Burns Architecture Managing Kubernetes Brenden Burns Architecture Kubernetes Best Practices Brenden Burns Architecture Fluent Python Luciano Ramalho Software Engineering Algorithms to Live By Christian & Griffiths Software Engineering Code Complete, Second Edition Steve McConnell Software Engineering Clean Code Robert Cecil Martin Software Engineering Growing Object Oriented Software... Steve Freeman & Nat Pryce Software Engineering The Mythical Man Month Fred Brooks Software Engineering Refactoring Kent Beck & Martin Fowler Software Engineering Design Patterns Gang of Four Software Engineering Head First Design Patterns E. Freeman & K. Sierra Software Engineering Operating Systems: Three Easy Pieces A. & R. Arpaci-Dusseau Software Engineering Working Effectively with Legacy Code Michael Feathers Software Engineering UML Distilled Martin Fowler Software Engineering The Data Engineering Cookbook Andreas Kretz Data Engineering Python for Data Analysis Wes Mckinney Data Engineering Python for Finance Yves Hilpisch Data Engineering Deep Learning Courville et. all Data Engineering The Design Of Everyday Things Dom Norman Design University MOOCS Data Structures & Algorithms Stanford CS106X Data Structures Princeton Algorithms Part 1 Princeton Algorithms Part 2 ~ MIT Algorithms MIT Advanced Data Structures U Alberta Design Patterns Java / OOP UC San Diego OOP Java Programming Specialization Object Oriented Programming in Java Data Structures and Performance Advanced Data Structures in Java Mastering the Software Engineering Interview Capstone: Analyzing (Social) Network Data RICE Parallel, Concurrent, and Distributed Programming in Java Parallel Programming Concurrent Programming Distributed Programming Computer Architecture Stanford CS107 Computer Organization & Systems Stanford CS143 Computer Organization & Systems U Jerusalem Nand to Tetris, Part 1 U Jerusalem Nand to Tetris, Part 2 U Wisconsin-Madison Operating Systems: Three Easy Pieces Course Data Engineering Courses ~ Stanford Databases Stanford Machine Learning Georgia Tech Machine Learning for Trading U San Francisco Computational Linear Algebra U San Francisco Applied ML DevOps Engineering Courses U Virginia CICD DevOps Unafilliated MOOCS Design Gurus Grokking the System Design Interview ~ Gaurav Sen System Design ~ Shivang Sarawagi Web Application & Software Architecture ~ Udacity AWS Cloud Architect ~ A Cloud Guru AWS Certified Solutions Architect Associate 2020 ~ Bret Fisher : Docker & Kubernetes Jose Portilla Data Strucutres & Algorithms in Python Data Analysis & Machine Learning Khan Academy Statistics (inference & probability) Linear Algebra Calculus 1,2, & 3 Deeplearning.ai : Tensorflow [] Online Tutorials Tensorflow Docs Spark Tutorials, Data Bricks [PySpark](https://www.edx.org/course/big-data-analytics-using-spark Azure K8s Kubernetes%20Learning%20Path_Version%202.0.pdf Terraform Backtrader Tutorials GitHub Hello World Setup First Script Using Analyzers Optimization Modularization Creating Analyzers Data Replay Sentdex Python Intermediate Python Pandas Machine Learning Scikit-learn Finance Deep Learning Matplotlib Dash Web Django Flask Beauitful Soup Fun Alexa Skills Kivy/KivEnt AI SC2 Unsupervised ML Reinforcement Learning Other Golang RasPi RasPi Distributed Computing Quantum Computer Programming Other Media Podcasts Software Engineering Daily Talk Python Python Bytes The Changelog Command Line Heros Machine Learning Guide Darknet Diaries Coding Blocks .NET Professional Societies Association for Computing Machinery (ACM) IEEE Computer Society Websites https://github.com/kahun/awesome-sysadmin Architecture & Engineering https://12factor.net/ https://martinfowler.com/ https://staffeng.com/ https://free-for.dev/ https://teachyourselfcs.com/ https://learnk8s.io/production-best-practices https://github.com/andkret/Cookbook Technical Interview https://leetcode.com/ https://www.hackerrank.com/ (SQL) ~ Interview Cake https://hackernoon.com/google-interview-questions-deconstructed-the-knights-dialer-f780d516f029 Learning Labs https://linuxacademy.com/ https://www.katacoda.com/ OSS Database: dbeaver Terminal Emulator: Mac: iTerm2 Linux: Konsole Windows: MobaXterm Source Code Editors: Python/Golang: VSCode Java: IntelliJ IDEA Kubernetes: Lens","title":"Resources"},{"location":"resources/#curriculum","text":"","title":"Curriculum"},{"location":"resources/#books","text":"Name Author Field ~The Pragmatic Programmer Andrew Hunt & David Thomas Career ~Soft Skills John Sonmez Career Cracking the Code Interview Laakman McDowell Career The Linux Command Lines William E. Shotts Jr. Linux Designing Data-Intensive Apps... Martin Kleppmann Architecture Analysis Patterns Martin Fowler Architecture Patterns of Enterprise App Architeture Martin Fowler Architecture Designing Distributed Systems Brenden Burns Architecture Kubernetes Up and Running Brenden Burns Architecture Managing Kubernetes Brenden Burns Architecture Kubernetes Best Practices Brenden Burns Architecture Fluent Python Luciano Ramalho Software Engineering Algorithms to Live By Christian & Griffiths Software Engineering Code Complete, Second Edition Steve McConnell Software Engineering Clean Code Robert Cecil Martin Software Engineering Growing Object Oriented Software... Steve Freeman & Nat Pryce Software Engineering The Mythical Man Month Fred Brooks Software Engineering Refactoring Kent Beck & Martin Fowler Software Engineering Design Patterns Gang of Four Software Engineering Head First Design Patterns E. Freeman & K. Sierra Software Engineering Operating Systems: Three Easy Pieces A. & R. Arpaci-Dusseau Software Engineering Working Effectively with Legacy Code Michael Feathers Software Engineering UML Distilled Martin Fowler Software Engineering The Data Engineering Cookbook Andreas Kretz Data Engineering Python for Data Analysis Wes Mckinney Data Engineering Python for Finance Yves Hilpisch Data Engineering Deep Learning Courville et. all Data Engineering The Design Of Everyday Things Dom Norman Design","title":"Books"},{"location":"resources/#university-moocs","text":"","title":"University MOOCS"},{"location":"resources/#data-structures-algorithms","text":"Stanford CS106X Data Structures Princeton Algorithms Part 1 Princeton Algorithms Part 2 ~ MIT Algorithms MIT Advanced Data Structures U Alberta Design Patterns","title":"Data Structures &amp; Algorithms"},{"location":"resources/#java-oop","text":"UC San Diego OOP Java Programming Specialization Object Oriented Programming in Java Data Structures and Performance Advanced Data Structures in Java Mastering the Software Engineering Interview Capstone: Analyzing (Social) Network Data RICE Parallel, Concurrent, and Distributed Programming in Java Parallel Programming Concurrent Programming Distributed Programming","title":"Java / OOP"},{"location":"resources/#computer-architecture","text":"Stanford CS107 Computer Organization & Systems Stanford CS143 Computer Organization & Systems U Jerusalem Nand to Tetris, Part 1 U Jerusalem Nand to Tetris, Part 2 U Wisconsin-Madison Operating Systems: Three Easy Pieces Course","title":"Computer Architecture"},{"location":"resources/#data-engineering-courses","text":"~ Stanford Databases Stanford Machine Learning Georgia Tech Machine Learning for Trading U San Francisco Computational Linear Algebra U San Francisco Applied ML","title":"Data Engineering Courses"},{"location":"resources/#devops-engineering-courses","text":"U Virginia CICD DevOps","title":"DevOps Engineering Courses"},{"location":"resources/#unafilliated-moocs","text":"Design Gurus Grokking the System Design Interview ~ Gaurav Sen System Design ~ Shivang Sarawagi Web Application & Software Architecture ~ Udacity AWS Cloud Architect ~ A Cloud Guru AWS Certified Solutions Architect Associate 2020 ~ Bret Fisher : Docker & Kubernetes Jose Portilla Data Strucutres & Algorithms in Python Data Analysis & Machine Learning Khan Academy Statistics (inference & probability) Linear Algebra Calculus 1,2, & 3 Deeplearning.ai : Tensorflow []","title":"Unafilliated MOOCS"},{"location":"resources/#online-tutorials","text":"Tensorflow Docs Spark Tutorials, Data Bricks [PySpark](https://www.edx.org/course/big-data-analytics-using-spark Azure K8s Kubernetes%20Learning%20Path_Version%202.0.pdf Terraform Backtrader Tutorials GitHub Hello World Setup First Script Using Analyzers Optimization Modularization Creating Analyzers Data Replay","title":"Online Tutorials"},{"location":"resources/#sentdex","text":"","title":"Sentdex"},{"location":"resources/#python","text":"Intermediate Python Pandas Machine Learning Scikit-learn Finance Deep Learning Matplotlib Dash","title":"Python"},{"location":"resources/#web","text":"Django Flask Beauitful Soup","title":"Web"},{"location":"resources/#fun","text":"Alexa Skills Kivy/KivEnt AI SC2 Unsupervised ML Reinforcement Learning","title":"Fun"},{"location":"resources/#other","text":"Golang RasPi RasPi Distributed Computing Quantum Computer Programming","title":"Other"},{"location":"resources/#other-media","text":"","title":"Other Media"},{"location":"resources/#podcasts","text":"Software Engineering Daily Talk Python Python Bytes The Changelog Command Line Heros Machine Learning Guide Darknet Diaries Coding Blocks .NET","title":"Podcasts"},{"location":"resources/#professional-societies","text":"Association for Computing Machinery (ACM) IEEE Computer Society","title":"Professional Societies"},{"location":"resources/#websites","text":"https://github.com/kahun/awesome-sysadmin","title":"Websites"},{"location":"resources/#architecture-engineering","text":"https://12factor.net/ https://martinfowler.com/ https://staffeng.com/ https://free-for.dev/ https://teachyourselfcs.com/ https://learnk8s.io/production-best-practices https://github.com/andkret/Cookbook","title":"Architecture &amp; Engineering"},{"location":"resources/#technical-interview","text":"https://leetcode.com/ https://www.hackerrank.com/ (SQL) ~ Interview Cake https://hackernoon.com/google-interview-questions-deconstructed-the-knights-dialer-f780d516f029","title":"Technical Interview"},{"location":"resources/#learning-labs","text":"https://linuxacademy.com/ https://www.katacoda.com/","title":"Learning Labs"},{"location":"resources/#oss","text":"Database: dbeaver Terminal Emulator: Mac: iTerm2 Linux: Konsole Windows: MobaXterm Source Code Editors: Python/Golang: VSCode Java: IntelliJ IDEA Kubernetes: Lens","title":"OSS"},{"location":"wiki/","text":"Wiki Structure Deployment Workflow sudo pip install mkdocs Manual Vanilla mkdocs gh-deploy git commit & push master User/Org Pages cd /build mkdocs gh-deploy --config-file ../mkdocs.yml --remote-branch master git commit & push develop Custom Domain StackOverflow Answer mkdocs gh-deploy git commit & push master Specify custom domain in GitHub Pages settings Add A recorsd for github.io 185.199.108.153 185.199.109.153 185.199.110.153 185.199.111.153 Add CNAME record to point domain to GitHub Page user.github.io/repo_name Enforce HTTPS Build Server Self-hosted","title":"Wiki"},{"location":"wiki/#wiki","text":"","title":"Wiki"},{"location":"wiki/#structure","text":"","title":"Structure"},{"location":"wiki/#deployment-workflow","text":"sudo pip install mkdocs","title":"Deployment Workflow"},{"location":"wiki/#manual","text":"","title":"Manual"},{"location":"wiki/#vanilla","text":"mkdocs gh-deploy git commit & push master","title":"Vanilla"},{"location":"wiki/#userorg-pages","text":"cd /build mkdocs gh-deploy --config-file ../mkdocs.yml --remote-branch master git commit & push develop","title":"User/Org Pages"},{"location":"wiki/#custom-domain","text":"StackOverflow Answer mkdocs gh-deploy git commit & push master Specify custom domain in GitHub Pages settings Add A recorsd for github.io 185.199.108.153 185.199.109.153 185.199.110.153 185.199.111.153 Add CNAME record to point domain to GitHub Page user.github.io/repo_name Enforce HTTPS","title":"Custom Domain"},{"location":"wiki/#build-server","text":"","title":"Build Server"},{"location":"wiki/#self-hosted","text":"","title":"Self-hosted"},{"location":"architecture/case_studies/","text":"Case Studies Netflix NASA Amazon LinkedIn Google Facebook Lyft Uber","title":"Case Studies"},{"location":"architecture/case_studies/#case-studies","text":"","title":"Case Studies"},{"location":"architecture/case_studies/#netflix","text":"","title":"Netflix"},{"location":"architecture/case_studies/#nasa","text":"","title":"NASA"},{"location":"architecture/case_studies/#amazon","text":"","title":"Amazon"},{"location":"architecture/case_studies/#linkedin","text":"","title":"LinkedIn"},{"location":"architecture/case_studies/#google","text":"","title":"Google"},{"location":"architecture/case_studies/#facebook","text":"","title":"Facebook"},{"location":"architecture/case_studies/#lyft","text":"","title":"Lyft"},{"location":"architecture/case_studies/#uber","text":"","title":"Uber"},{"location":"architecture/chaos_eng/","text":"Chaos Engineering Disaster Recovery database backups server images server configurations (and instance types) network configurations IP whitelisting code repositories data repositories (files, media, artifacts, images, etc.) documentation DR Failover Plans cold standby pilot light warm standby hot standby Cloud DR Strategies geographic recovery (multi-region services) Document and Practice practice frequently document every step, including decision makers continiously update documentation based on dry-runs","title":"Chaos Engineering"},{"location":"architecture/chaos_eng/#chaos-engineering","text":"","title":"Chaos Engineering"},{"location":"architecture/chaos_eng/#disaster-recovery","text":"database backups server images server configurations (and instance types) network configurations IP whitelisting code repositories data repositories (files, media, artifacts, images, etc.) documentation","title":"Disaster Recovery"},{"location":"architecture/chaos_eng/#dr-failover-plans","text":"cold standby pilot light warm standby hot standby","title":"DR Failover Plans"},{"location":"architecture/chaos_eng/#cloud-dr-strategies","text":"geographic recovery (multi-region services)","title":"Cloud DR Strategies"},{"location":"architecture/chaos_eng/#document-and-practice","text":"practice frequently document every step, including decision makers continiously update documentation based on dry-runs","title":"Document and Practice"},{"location":"architecture/cicd/","text":"CICD Build Immutable Instances - Image Pipeline Base OS image Security hardening, patching, agents -> Hardened image Application configuration -> Application Image Security Scanning -> Released image No changes performed on instances of the image prior to launching them in environments Any further changes require building a new image and restarting the pipeline from step 1 If immutable is not an option in the environment, then use a configuration management tool (Ansible) to track changes to the environment in a source version control repository Jenkins Test Unit Tests Integration Tests End-to-end Tests Deploy ansible octopus spinnaker Provision Infrastructure as Code Terraform install terraform locally download binary download checksums & checksum signature verify the signartue of the checksum against HashiCorp's GPG key add terraform binary to PATH Create a VM on GCE example Once per project enable the GCE API: gcloud services enable compute.googleapis.com Create Terraform config Run commands at creation: metadata_startup_script logs: /var/log/daemon.log Add ssh keys: metadata = { ssh-keys = ### } Run Terraform terraform init terraform plan terraform apply Clean up Terraform config main.tf resource \"google_compute_instance\" \"default\" { name = \"vm-${random_id.instance_id.hex}\" machine_type = \"f1-micro\" zone = \"us-west1-a\" boot_disk { initialize_params { image = \"debian-cloud/debian-9\" } } # ... } Ansible Ansible Control Machine (Dockerized) ansible-console - REPL that comes with tab completion Access ENV variables: {{ lookup('env','USER') }}\" command module escapes commands, while the shell module does not Configuring VMs from Windows Create a control node VM with debian or ubuntu ssh into control node VM apt-get -y update && apt-get -y install ansible Note: run an installation script after creating a VM with Terraform sudo apt update sudo apt -y upgrade sudo apt-get install software-properties-common sudo apt-add-repository ppa:ansible/ansible sudo apt-get update sudo apt-get install ansible Point to inventory dir [defaults] inventory = /vagrant/inv host_key_checking = False Docs Ansible Docs Ansible Galaxy Docs ansible-doc copy ansible-doc -t connection anisble-doc -t shell --list ansible-doc -t shell csh list commands with ansible + tab Inventory ansible-inventory --graph --vars ansible-inventory --list Inventory definitions in YAML all: hosts: ubuntu10: ansible_host: 192.168.50.10 ansible_private_key_file: .vagrant/machines/ubuntu10/virtualbox/private_key Child group [ubuntu] ubuntu10 ubuntu11 ubuntu12 Parent group [vagrant:children] centos ubuntu [vagrant:vars] ansible_user=vagrant ansible_port=22 Ad-hoc command ansible all -m ping ansible -m command -a \"git config --global --list\" vagrant ansible -m copy -a \"src=master.gitconfig dest=~/.gitconfig\" localehost dry run: add --check show changes: add --diff show package managers: ansible -m setup -a \"filter=ansible_pkg_mgr\" all Playbook ansible-playbook playbook.yml Debug: add -v up to -vvvv name: Ensure ~/.gitconfig copied from master.gitconfig hosts: localhost tasks: - name: copy git config copy: src: \"mast.gitconfig\" dest: \"~/.gitconfig\" when: ... # conditional become: yes # escalate privilege register git_config_copy # register output to variable - name: print captured variable debug: var=git_config_copy","title":"CICD"},{"location":"architecture/cicd/#cicd","text":"","title":"CICD"},{"location":"architecture/cicd/#build","text":"","title":"Build"},{"location":"architecture/cicd/#immutable-instances-image-pipeline","text":"Base OS image Security hardening, patching, agents -> Hardened image Application configuration -> Application Image Security Scanning -> Released image No changes performed on instances of the image prior to launching them in environments Any further changes require building a new image and restarting the pipeline from step 1 If immutable is not an option in the environment, then use a configuration management tool (Ansible) to track changes to the environment in a source version control repository","title":"Immutable Instances - Image Pipeline"},{"location":"architecture/cicd/#jenkins","text":"","title":"Jenkins"},{"location":"architecture/cicd/#test","text":"","title":"Test"},{"location":"architecture/cicd/#unit-tests","text":"","title":"Unit Tests"},{"location":"architecture/cicd/#integration-tests","text":"","title":"Integration Tests"},{"location":"architecture/cicd/#end-to-end-tests","text":"","title":"End-to-end Tests"},{"location":"architecture/cicd/#deploy","text":"ansible octopus spinnaker","title":"Deploy"},{"location":"architecture/cicd/#provision","text":"","title":"Provision"},{"location":"architecture/cicd/#infrastructure-as-code","text":"","title":"Infrastructure as Code"},{"location":"architecture/cicd/#terraform","text":"install terraform locally download binary download checksums & checksum signature verify the signartue of the checksum against HashiCorp's GPG key add terraform binary to PATH","title":"Terraform"},{"location":"architecture/cicd/#create-a-vm-on-gce-example","text":"Once per project enable the GCE API: gcloud services enable compute.googleapis.com Create Terraform config Run commands at creation: metadata_startup_script logs: /var/log/daemon.log Add ssh keys: metadata = { ssh-keys = ### } Run Terraform terraform init terraform plan terraform apply Clean up Terraform config main.tf resource \"google_compute_instance\" \"default\" { name = \"vm-${random_id.instance_id.hex}\" machine_type = \"f1-micro\" zone = \"us-west1-a\" boot_disk { initialize_params { image = \"debian-cloud/debian-9\" } } # ... }","title":"Create a VM on GCE example"},{"location":"architecture/cicd/#ansible","text":"Ansible Control Machine (Dockerized) ansible-console - REPL that comes with tab completion Access ENV variables: {{ lookup('env','USER') }}\" command module escapes commands, while the shell module does not","title":"Ansible"},{"location":"architecture/cicd/#configuring-vms-from-windows","text":"Create a control node VM with debian or ubuntu ssh into control node VM apt-get -y update && apt-get -y install ansible Note: run an installation script after creating a VM with Terraform sudo apt update sudo apt -y upgrade sudo apt-get install software-properties-common sudo apt-add-repository ppa:ansible/ansible sudo apt-get update sudo apt-get install ansible Point to inventory dir [defaults] inventory = /vagrant/inv host_key_checking = False","title":"Configuring VMs from Windows"},{"location":"architecture/cicd/#docs","text":"Ansible Docs Ansible Galaxy Docs ansible-doc copy ansible-doc -t connection anisble-doc -t shell --list ansible-doc -t shell csh list commands with ansible + tab","title":"Docs"},{"location":"architecture/cicd/#inventory","text":"ansible-inventory --graph --vars ansible-inventory --list Inventory definitions in YAML all: hosts: ubuntu10: ansible_host: 192.168.50.10 ansible_private_key_file: .vagrant/machines/ubuntu10/virtualbox/private_key Child group [ubuntu] ubuntu10 ubuntu11","title":"Inventory"},{"location":"architecture/cicd/#ubuntu12","text":"Parent group [vagrant:children] centos ubuntu [vagrant:vars] ansible_user=vagrant ansible_port=22","title":"ubuntu12"},{"location":"architecture/cicd/#ad-hoc-command","text":"ansible all -m ping ansible -m command -a \"git config --global --list\" vagrant ansible -m copy -a \"src=master.gitconfig dest=~/.gitconfig\" localehost dry run: add --check show changes: add --diff show package managers: ansible -m setup -a \"filter=ansible_pkg_mgr\" all","title":"Ad-hoc command"},{"location":"architecture/cicd/#playbook","text":"ansible-playbook playbook.yml Debug: add -v up to -vvvv name: Ensure ~/.gitconfig copied from master.gitconfig hosts: localhost tasks: - name: copy git config copy: src: \"mast.gitconfig\" dest: \"~/.gitconfig\" when: ... # conditional become: yes # escalate privilege register git_config_copy # register output to variable - name: print captured variable debug: var=git_config_copy","title":"Playbook"},{"location":"architecture/highlevel_design/","text":"High-level Design System design Orthagonal Architecture decoupling (you can change the UI or database without affecting the other) two different user interfaces should be able to be supported by the same underlying code base design components that are self-contained, independent, and with a single, well-defined purpose when components are isolated then one can change wihtout affecting the others allow components to communicate based on external interfaces that are consistent module design - if one module is comprimised it will not affect the others temporal decoupling - allow for concurrency to reduce time-based dependencies of a system gain in productivity and reduces risk Iterative Design The Twelve-Factor App Website Codebase: One codebase tracked in revision control, many deploys - Dependencies: Explicitly declare and isolate dependencies - Config: Store config in the environment - Backing services: Treat backing services as attached resources - Build, release, run: Strictly separate build and run stages - Process: Execute the app as one or more stateless processes - Port binding: Export services via port binding - Concurrency: Scale out via the process model - Disposability: Maximize robustness with fast startup and graceful shutdown - Dev/prod parity: Keep development, staging, and production as similar as possible - Logs: Treat logs as event streams - Admin processes: Run admin/management tasks as one-off processes - Tiers vs. Layers tiers involve physical separation of components in a system (UI, backend server, database, cache, message queues, load balancers, search, data processing, shared services) layers of an application represent the organization of the code (user interface, business logic, service, or data access layer) Distributed Systems Scalability the ability of an application to handle & withstand increased workload without sacrificing latency if the app takes x seconds to respond to a user reqiest, it should take the same x seconds to respond to each of the million concurrent user requests on the app Vertical adding more compute power does not require code refactoring Horizontal adding more nodes/hardware to existing hardware resource pool requires code refactoring to utilize distributed resources higher availability Monolith Simpler to build, test and deploy Weaknesses: small code changes require an entire redeployment of the application requires thorough regression testing introduces single points of failure flexibility and scalability are harder to achieve limits the use of the application using heterogenous technologies tend to be stateful and therefore making it more difficult to be cloud-native and distributed Microservice Fault isolation Easier management Easier development Ease of adding new features Ease of maintenance High availability Separated responsiblity/concerns (separate teams gain increased productivity and complete ownership) Weaknesses: greatly increased complexity in management and monitoring of the system (distributed logging, networking, service discovery, CICD, alerts, tracing, health checks, etc.) requires more components or code to manage and utilize the distributed nodes requires more human resources to manage the more complex system storing consistency is hard to guarentee in a distributed environment (leans on eventual consistency) Load Balancing Caching reduces latency, stores data in memory instead of accessing disk - O(1) fetches avoids over computing and rerunning joins in SQL which greatly reduces latency can continue to serve data requests even when the database server goes down can be implanted at any application later - OS, network, CDN, database, client-side, cross-module communication between services in a microservice architecture Distributed Caching Data Storage use read-only replica databases to reduce read traffic and as a source for business/financial applications (can be promoted for disaster recovery) Data Partitioning Database Sharding Indexes Proxies Redundancy and Replication SQL vs. NoSQL RDBs: strong consistency ACID transactions great for handling relationships scalable with caching (especially read-heavy applications) NoSQL eventual consistency horizontal scalability great for read/write-heavy applications with little relationships CAP Theorem Consistent Hashing Message Queues Allows for communication in an environment with heterogeneous technology Enables asynchronous behavior in a service to run background processes, tasks, and batch jobs Cross-module communication in a microservice architecture Exchanges handle message queue logic use binding to deliver messages to queues Publisher / Subscriber Model one to many (broadcast) allow for filtering of events so that components can subscribe to only the events they need Point to Point Model one to one (entity relationship) High Availability Active-Passive HA mode: set of redundant components in standby node awaiting automatic failover Active-Active HA Mode: set of replicated components that share the workload of the system Montior: to detect single points of failure and unhealthy components in the system Automation: to allow for the system to conduct self-healing or handle increased/decreased workloads System Testing network bandwidth consumption throughput requests processed within a time range latency application memory/CPU usage end-user experience when the system is under a heavy load Reducing Bottlenecks horizontally scale workloads servers distributed databases asynchronous processes & modules wherever possible (reduce unecessary sequential steps) data compression caching (write-through cache) - only hit a database when it is really required use a CDN load balancers & efficient configuration picking the right databases code refactoring decoupling dynamic analysis / profiling (application profiler, code profiler) to see which processes are taking too long and consuming too many resources avoid unnecessary client-server requests (group them together if possible) Infrastructure Changes should include impact analysis rollback plan disaster recovery plan code review / discussion Cloud Cost Hygiene naming conventions tags (to associate resources to cost centers) group lifecycle person application IT Governance rules establish user roles and controls billing alarms Storage Costs Considerations storage capacity provisioned/used data transfer replication access patterns Types of Architecture Client-server Peer to Peer (P2P) blockchain node to node connections Federated extension of decenteralized architecture nodes are grouped and connect to pod servers that enable node discovery pods connect to each other to form the backbone of the network Event-driven reactive programming (Tornado, Nodejs, akka, etc.) Notes","title":"High-level Design"},{"location":"architecture/highlevel_design/#high-level-design","text":"System design","title":"High-level Design"},{"location":"architecture/highlevel_design/#orthagonal-architecture","text":"decoupling (you can change the UI or database without affecting the other) two different user interfaces should be able to be supported by the same underlying code base design components that are self-contained, independent, and with a single, well-defined purpose when components are isolated then one can change wihtout affecting the others allow components to communicate based on external interfaces that are consistent module design - if one module is comprimised it will not affect the others temporal decoupling - allow for concurrency to reduce time-based dependencies of a system gain in productivity and reduces risk","title":"Orthagonal Architecture"},{"location":"architecture/highlevel_design/#iterative-design","text":"","title":"Iterative Design"},{"location":"architecture/highlevel_design/#the-twelve-factor-app","text":"Website Codebase: One codebase tracked in revision control, many deploys - Dependencies: Explicitly declare and isolate dependencies - Config: Store config in the environment - Backing services: Treat backing services as attached resources - Build, release, run: Strictly separate build and run stages - Process: Execute the app as one or more stateless processes - Port binding: Export services via port binding - Concurrency: Scale out via the process model - Disposability: Maximize robustness with fast startup and graceful shutdown - Dev/prod parity: Keep development, staging, and production as similar as possible - Logs: Treat logs as event streams - Admin processes: Run admin/management tasks as one-off processes -","title":"The Twelve-Factor App"},{"location":"architecture/highlevel_design/#tiers-vs-layers","text":"tiers involve physical separation of components in a system (UI, backend server, database, cache, message queues, load balancers, search, data processing, shared services) layers of an application represent the organization of the code (user interface, business logic, service, or data access layer)","title":"Tiers vs. Layers"},{"location":"architecture/highlevel_design/#distributed-systems","text":"","title":"Distributed Systems"},{"location":"architecture/highlevel_design/#scalability","text":"the ability of an application to handle & withstand increased workload without sacrificing latency if the app takes x seconds to respond to a user reqiest, it should take the same x seconds to respond to each of the million concurrent user requests on the app","title":"Scalability"},{"location":"architecture/highlevel_design/#vertical","text":"adding more compute power does not require code refactoring","title":"Vertical"},{"location":"architecture/highlevel_design/#horizontal","text":"adding more nodes/hardware to existing hardware resource pool requires code refactoring to utilize distributed resources higher availability","title":"Horizontal"},{"location":"architecture/highlevel_design/#monolith","text":"Simpler to build, test and deploy Weaknesses: small code changes require an entire redeployment of the application requires thorough regression testing introduces single points of failure flexibility and scalability are harder to achieve limits the use of the application using heterogenous technologies tend to be stateful and therefore making it more difficult to be cloud-native and distributed","title":"Monolith"},{"location":"architecture/highlevel_design/#microservice","text":"Fault isolation Easier management Easier development Ease of adding new features Ease of maintenance High availability Separated responsiblity/concerns (separate teams gain increased productivity and complete ownership) Weaknesses: greatly increased complexity in management and monitoring of the system (distributed logging, networking, service discovery, CICD, alerts, tracing, health checks, etc.) requires more components or code to manage and utilize the distributed nodes requires more human resources to manage the more complex system storing consistency is hard to guarentee in a distributed environment (leans on eventual consistency)","title":"Microservice"},{"location":"architecture/highlevel_design/#load-balancing","text":"","title":"Load Balancing"},{"location":"architecture/highlevel_design/#caching","text":"reduces latency, stores data in memory instead of accessing disk - O(1) fetches avoids over computing and rerunning joins in SQL which greatly reduces latency can continue to serve data requests even when the database server goes down can be implanted at any application later - OS, network, CDN, database, client-side, cross-module communication between services in a microservice architecture","title":"Caching"},{"location":"architecture/highlevel_design/#distributed-caching","text":"","title":"Distributed Caching"},{"location":"architecture/highlevel_design/#data-storage","text":"use read-only replica databases to reduce read traffic and as a source for business/financial applications (can be promoted for disaster recovery)","title":"Data Storage"},{"location":"architecture/highlevel_design/#data-partitioning","text":"","title":"Data Partitioning"},{"location":"architecture/highlevel_design/#database-sharding","text":"","title":"Database Sharding"},{"location":"architecture/highlevel_design/#indexes","text":"","title":"Indexes"},{"location":"architecture/highlevel_design/#proxies","text":"","title":"Proxies"},{"location":"architecture/highlevel_design/#redundancy-and-replication","text":"","title":"Redundancy and Replication"},{"location":"architecture/highlevel_design/#sql-vs-nosql","text":"RDBs: strong consistency ACID transactions great for handling relationships scalable with caching (especially read-heavy applications) NoSQL eventual consistency horizontal scalability great for read/write-heavy applications with little relationships","title":"SQL vs. NoSQL"},{"location":"architecture/highlevel_design/#cap-theorem","text":"","title":"CAP Theorem"},{"location":"architecture/highlevel_design/#consistent-hashing","text":"","title":"Consistent Hashing"},{"location":"architecture/highlevel_design/#message-queues","text":"Allows for communication in an environment with heterogeneous technology Enables asynchronous behavior in a service to run background processes, tasks, and batch jobs Cross-module communication in a microservice architecture Exchanges handle message queue logic use binding to deliver messages to queues","title":"Message Queues"},{"location":"architecture/highlevel_design/#publisher-subscriber-model","text":"one to many (broadcast) allow for filtering of events so that components can subscribe to only the events they need","title":"Publisher / Subscriber Model"},{"location":"architecture/highlevel_design/#point-to-point-model","text":"one to one (entity relationship)","title":"Point to Point Model"},{"location":"architecture/highlevel_design/#high-availability","text":"Active-Passive HA mode: set of redundant components in standby node awaiting automatic failover Active-Active HA Mode: set of replicated components that share the workload of the system Montior: to detect single points of failure and unhealthy components in the system Automation: to allow for the system to conduct self-healing or handle increased/decreased workloads","title":"High Availability"},{"location":"architecture/highlevel_design/#system-testing","text":"network bandwidth consumption throughput requests processed within a time range latency application memory/CPU usage end-user experience when the system is under a heavy load","title":"System Testing"},{"location":"architecture/highlevel_design/#reducing-bottlenecks","text":"horizontally scale workloads servers distributed databases asynchronous processes & modules wherever possible (reduce unecessary sequential steps) data compression caching (write-through cache) - only hit a database when it is really required use a CDN load balancers & efficient configuration picking the right databases code refactoring decoupling dynamic analysis / profiling (application profiler, code profiler) to see which processes are taking too long and consuming too many resources avoid unnecessary client-server requests (group them together if possible)","title":"Reducing Bottlenecks"},{"location":"architecture/highlevel_design/#infrastructure-changes","text":"should include impact analysis rollback plan disaster recovery plan code review / discussion","title":"Infrastructure Changes"},{"location":"architecture/highlevel_design/#cloud-cost-hygiene","text":"naming conventions tags (to associate resources to cost centers) group lifecycle person application IT Governance rules establish user roles and controls billing alarms","title":"Cloud Cost Hygiene"},{"location":"architecture/highlevel_design/#storage-costs-considerations","text":"storage capacity provisioned/used data transfer replication access patterns","title":"Storage Costs Considerations"},{"location":"architecture/highlevel_design/#types-of-architecture","text":"","title":"Types of Architecture"},{"location":"architecture/highlevel_design/#client-server","text":"","title":"Client-server"},{"location":"architecture/highlevel_design/#peer-to-peer-p2p","text":"blockchain node to node connections","title":"Peer to Peer (P2P)"},{"location":"architecture/highlevel_design/#federated","text":"extension of decenteralized architecture nodes are grouped and connect to pod servers that enable node discovery pods connect to each other to form the backbone of the network","title":"Federated"},{"location":"architecture/highlevel_design/#event-driven","text":"reactive programming (Tornado, Nodejs, akka, etc.)","title":"Event-driven"},{"location":"architecture/highlevel_design/#notes","text":"","title":"Notes"},{"location":"architecture/homelab/","text":"Homelab","title":"Homelab"},{"location":"architecture/homelab/#homelab","text":"","title":"Homelab"},{"location":"architecture/linux/","text":"Linux apt-get update && apt-get upgrade yum update Configuration No password configured: sudo passwd hostname -f dpkg-reconfigure tzdata / timedatectl set-timezone fdisk System Diagnostics free -h top / htop df -h vmstat ps -ef pgrep / pidof File System Management ln -s cp -R mkdir -p touch mv rm scp tree Filepaths Filesystem Hierarchy Standard Linux Filesystem Tree Overview Daily Driver repos: ~/repos/github/ /home home sweet home, this is the place for users' home directories. Application Development /opt stores additional software not handled by the package manager. /tmp is a place for temporary files used by applications /var is dedicated to variable data, such as logs, databases, websites, and temporary spool (e-mail etc.) files that persist from one boot to the next. A notable directory it contains is /var/log where system log files are kept. System /bin is a place for most commonly used terminal commands, like ls, mount, rm, etc. /etc contains system-global configuration files, which affect the system's behavior for all users. /root is the superuser's home directory, not in /home/ to allow for booting the system even if /home/ is not available. /usr contains the majority of user utilities and applications, and partly replicates the root directory structure, containing for instance, among others, /usr/bin/ and /usr/lib. /boot contains files needed to start up the system, including the Linux kernel, a RAM disk image and bootloader configuration files. /dev contains all device files, which are not regular files but instead refer to various hardware devices on the system, including hard drives. /lib contains very important dynamic libraries and kernel modules /media is intended as a mount point for external devices, such as hard drives or removable media (floppies, CDs, DVDs). /mnt is also a place for mount points, but dedicated specifically to \"temporarily mounted\" devices, such as network filesyste Text Manipulation grep awk sed / jq .vimrc colorscheme evening set relativenumber set number set colorcolumn=80 set tabstop=2 shiftwidth=2 expandtab set backspace=indent,eol,start highlight ColorColum ctermbg=0 guibg=lightgrey Package Management apk apt / apt-get dpkg yum pacman Netowrking nmap : show exposed ports ping traceroute netstat o flag: show PIDs telnet mtr curl I flag: show header info only L flag: follow redirects Cron Scheduler cron expressions Install cron deb/ubuntu apt-get update && apt-get install cron pgrep cron or pidof cron systemctl start cron or service cron start on old systems crontab -e NOTE: Make sure to add a NEWLINE at the end different environment: cron passes a minimal set of environment variables to cronjobs view them: * * * * * env > /tmp/env.output default log location: /var/log/syslog redirect cron logs to specified locations e.g. 0 15 * * * /home/user/daily-backup.sh >> /var/log/daily-backup.log 2>&1 Bash alias alias ll='ls -l' cd - pwd ls -la cat / bat bash_profile vs. bashrc .bash_profile is executed at login, while .bashrc is executed on every shell instance can also use an .ini file and source it in the .bash_profile with a source ~/my.ini .bash_profile example: export PS1=\"\\[\\033[36m\\]\\u\\[\\033[m\\]@\\[\\033[32m\\]\\h:\\[\\033[33;1m\\]\\w\\[\\033[m\\]\\$ \" #PS1='[${curr_host}@`if [[ \"$mh\" = \"$PWD\" ]]; then echo \"$PWD\"; else echo \"~$ {PWD#$ih}\"; fi`] $ ' INFA_HOME=/app/infa/Informatica/10.2.0 JAVA_HOME=$INFA_HOME/java PATH=$PATH:$HOME/bin:$INFA_HOME/server/bin:$JAVA_HOME/lib PYTHONPATH=$PYTHONPATH:/app-san/infa/infa_shared/Scripts/dimpypkg LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$ORACLE_LIBS:$INFA_HOME/server/bin:$JAVA_HOME/lib:$ODBCHOME/lib:$PWX_HOME #LD_LIBRARY_PATH=$INFA_HOME/server/bin export INFA_HOME export JAVA_HOME export PATH export PYTHONPATH export LD_LIBRARY_PATH . ./my.ini my.ini example\" #!/bin/sh ih=/app-san/infa/infa_shared curr_host=`echo $HOSTNAME | cut -d'.' -f 1` alias lp='ls -alp' alias lpt='ls -alptr | tail' alias lpd='ls -alp | grep ^d' alias lps='ls -alpSr | tail' alias cache='cd $ih/Cache' alias lkp='cd $ih/LkpFiles' alias log='cd $ih/log' alias temp='cd $ih/Temp' alias param='cd $ih/ParamFiles' alias scripts='cd $ih/Scripts' alias slog='cd $ih/SessLogs' alias src='cd $ih/SrcFiles' alias tgt='cd $ih/TgtFiles' alias wlog='cd $ih/WorkflowLogs' alias infa='cd $INFA_HOME' Shell Scripting Script arguments Functions Distros Servers: Debian Daily Driver: Ubuntu & Arch Business: CentOS Notes https://www.linode.com/docs/tools-reference/linux-system-administration-basics/ .bashrc vs. .bash_profile .bash_profile is executed for login shells, while .bashrc is executed for interactive non-login shells. When you login (type username and password) via console, either sitting at the machine, or remotely via ssh: .bash_profile is executed to configure your shell before the initial command prompt. But, if you\u2019ve already logged into your machine and open a new terminal window (xterm) then .bashrc is executed before the window command prompt. .bashrc is also run when you start a new bash instance by typing /bin/bash in a terminal.","title":"Linux"},{"location":"architecture/linux/#linux","text":"apt-get update && apt-get upgrade yum update","title":"Linux"},{"location":"architecture/linux/#configuration","text":"No password configured: sudo passwd hostname -f dpkg-reconfigure tzdata / timedatectl set-timezone fdisk","title":"Configuration"},{"location":"architecture/linux/#system-diagnostics","text":"free -h top / htop df -h vmstat ps -ef pgrep / pidof","title":"System Diagnostics"},{"location":"architecture/linux/#file-system-management","text":"ln -s cp -R mkdir -p touch mv rm scp tree","title":"File System Management"},{"location":"architecture/linux/#filepaths","text":"Filesystem Hierarchy Standard Linux Filesystem Tree Overview","title":"Filepaths"},{"location":"architecture/linux/#daily-driver","text":"repos: ~/repos/github/ /home home sweet home, this is the place for users' home directories.","title":"Daily Driver"},{"location":"architecture/linux/#application-development","text":"/opt stores additional software not handled by the package manager. /tmp is a place for temporary files used by applications /var is dedicated to variable data, such as logs, databases, websites, and temporary spool (e-mail etc.) files that persist from one boot to the next. A notable directory it contains is /var/log where system log files are kept.","title":"Application Development"},{"location":"architecture/linux/#system","text":"/bin is a place for most commonly used terminal commands, like ls, mount, rm, etc. /etc contains system-global configuration files, which affect the system's behavior for all users. /root is the superuser's home directory, not in /home/ to allow for booting the system even if /home/ is not available. /usr contains the majority of user utilities and applications, and partly replicates the root directory structure, containing for instance, among others, /usr/bin/ and /usr/lib. /boot contains files needed to start up the system, including the Linux kernel, a RAM disk image and bootloader configuration files. /dev contains all device files, which are not regular files but instead refer to various hardware devices on the system, including hard drives. /lib contains very important dynamic libraries and kernel modules /media is intended as a mount point for external devices, such as hard drives or removable media (floppies, CDs, DVDs). /mnt is also a place for mount points, but dedicated specifically to \"temporarily mounted\" devices, such as network filesyste","title":"System"},{"location":"architecture/linux/#text-manipulation","text":"grep awk sed / jq .vimrc colorscheme evening set relativenumber set number set colorcolumn=80 set tabstop=2 shiftwidth=2 expandtab set backspace=indent,eol,start highlight ColorColum ctermbg=0 guibg=lightgrey","title":"Text Manipulation"},{"location":"architecture/linux/#package-management","text":"","title":"Package Management"},{"location":"architecture/linux/#apk","text":"","title":"apk"},{"location":"architecture/linux/#apt-apt-get","text":"","title":"apt / apt-get"},{"location":"architecture/linux/#dpkg","text":"","title":"dpkg"},{"location":"architecture/linux/#yum","text":"","title":"yum"},{"location":"architecture/linux/#pacman","text":"","title":"pacman"},{"location":"architecture/linux/#netowrking","text":"nmap : show exposed ports ping traceroute netstat o flag: show PIDs telnet mtr curl I flag: show header info only L flag: follow redirects","title":"Netowrking"},{"location":"architecture/linux/#cron-scheduler","text":"cron expressions Install cron deb/ubuntu apt-get update && apt-get install cron pgrep cron or pidof cron systemctl start cron or service cron start on old systems crontab -e NOTE: Make sure to add a NEWLINE at the end different environment: cron passes a minimal set of environment variables to cronjobs view them: * * * * * env > /tmp/env.output default log location: /var/log/syslog redirect cron logs to specified locations e.g. 0 15 * * * /home/user/daily-backup.sh >> /var/log/daily-backup.log 2>&1","title":"Cron Scheduler"},{"location":"architecture/linux/#bash","text":"alias alias ll='ls -l' cd - pwd ls -la cat / bat","title":"Bash"},{"location":"architecture/linux/#bash_profile-vs-bashrc","text":".bash_profile is executed at login, while .bashrc is executed on every shell instance can also use an .ini file and source it in the .bash_profile with a source ~/my.ini .bash_profile example: export PS1=\"\\[\\033[36m\\]\\u\\[\\033[m\\]@\\[\\033[32m\\]\\h:\\[\\033[33;1m\\]\\w\\[\\033[m\\]\\$ \" #PS1='[${curr_host}@`if [[ \"$mh\" = \"$PWD\" ]]; then echo \"$PWD\"; else echo \"~$ {PWD#$ih}\"; fi`] $ ' INFA_HOME=/app/infa/Informatica/10.2.0 JAVA_HOME=$INFA_HOME/java PATH=$PATH:$HOME/bin:$INFA_HOME/server/bin:$JAVA_HOME/lib PYTHONPATH=$PYTHONPATH:/app-san/infa/infa_shared/Scripts/dimpypkg LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$ORACLE_LIBS:$INFA_HOME/server/bin:$JAVA_HOME/lib:$ODBCHOME/lib:$PWX_HOME #LD_LIBRARY_PATH=$INFA_HOME/server/bin export INFA_HOME export JAVA_HOME export PATH export PYTHONPATH export LD_LIBRARY_PATH . ./my.ini my.ini example\" #!/bin/sh ih=/app-san/infa/infa_shared curr_host=`echo $HOSTNAME | cut -d'.' -f 1` alias lp='ls -alp' alias lpt='ls -alptr | tail' alias lpd='ls -alp | grep ^d' alias lps='ls -alpSr | tail' alias cache='cd $ih/Cache' alias lkp='cd $ih/LkpFiles' alias log='cd $ih/log' alias temp='cd $ih/Temp' alias param='cd $ih/ParamFiles' alias scripts='cd $ih/Scripts' alias slog='cd $ih/SessLogs' alias src='cd $ih/SrcFiles' alias tgt='cd $ih/TgtFiles' alias wlog='cd $ih/WorkflowLogs' alias infa='cd $INFA_HOME'","title":"bash_profile vs. bashrc"},{"location":"architecture/linux/#shell-scripting","text":"","title":"Shell Scripting"},{"location":"architecture/linux/#script-arguments","text":"","title":"Script arguments"},{"location":"architecture/linux/#functions","text":"","title":"Functions"},{"location":"architecture/linux/#distros","text":"Servers: Debian Daily Driver: Ubuntu & Arch Business: CentOS","title":"Distros"},{"location":"architecture/linux/#notes","text":"https://www.linode.com/docs/tools-reference/linux-system-administration-basics/","title":"Notes"},{"location":"architecture/linux/#bashrc-vs-bash_profile","text":".bash_profile is executed for login shells, while .bashrc is executed for interactive non-login shells. When you login (type username and password) via console, either sitting at the machine, or remotely via ssh: .bash_profile is executed to configure your shell before the initial command prompt. But, if you\u2019ve already logged into your machine and open a new terminal window (xterm) then .bashrc is executed before the window command prompt. .bashrc is also run when you start a new bash instance by typing /bin/bash in a terminal.","title":".bashrc vs. .bash_profile"},{"location":"architecture/monitoring/","text":"Monitoring The 3 pillars of Observability: Metrics Answers: \"How has the system performance changed throughout time?\" numbers related to events that happened in a specifc time range example technologies: Metricbeats or Prometheus with Grafana Logs Answers: \"What is happening in the system?\" gives insights into a single events occuring in the system example technologies: ELK stack Tracing Answers: \"How are the system components interacting with each other?\" shows how long a specific request spent in the different system components example technologies: Jaeger or Zipkin Live Metrics Stream app system platform Server Metrics CPU Utilization Memory Utilization Network Utilization Disk Performance and Read/Writes Disk Space Optimization Game Plan Run performance tests Capture and analyze metrics Select the optimal instance Monitor for outliers to make sure the optimal instance continues to be optimal Grafana Centeralized Logging ELK Stack Open Distro Apache 2.0 license Security Alerting SQL Performance Annalyzer (perftop CLI tool) Kubernetes Operator: Elastic Cloud on Kubernetes (ECK) uses the free tier Elasticsearch License Configure Elasticsearch stack in containers docker-compose quickstart repo Increase heap size / allocate more memory for ES in JVM options: Update environment variables in docker-compose file - \"ES_JAVA_OPTS=-Xms4g -Xmx4g\" Modify file descriptors: File descriptors: \"Make sure to increase the limit on the number of open files descriptors for the user running Elasticsearch to 65,536 or higher.\" run everything as root user ulimit -Hn : 4096 -> 1024000 vim /usr/lib/sysctl.d/elasticsearch.conf vim /etc/security/limits/conf restorecon /etc/security/limits.conf reboot system /usr/lib/sysctl.d/elasticsearch.conf fs.file-max = 512000 vm.max_map_count = 524288 vm.swappiness = 1 /etc/security/limits/conf * soft nofile 1024000 * hard nofile 1024000 * soft memlock unlimited * hard memlock unlimited elastic soft nofile 1024000 elastic hard nofile 1024000 elastic soft memlock unlimited elastic hard memlock unlimited root soft nofile 1024000 root hard nofile 1024000 root soft memlock unlimited root hard memlock unlimited Logstash Install additional plugins: /bin/logstash install logstash-input-jmx Run pipeline: /bin/logstash.bat -f logstash-pipeline.conf --config.reload.automatic Test pipeline configuration: /bin/logstash.bat -f logstash-pipeline.conf --config.test_and_exit Run as service on a windows server: use NSSM Ingest log file Log sample 2020-04-21 20:52:15,173 WARN | ActiveMQ Task-1 () | [NetworkConnector:?] Could not start network... 2020-04-21 20:52:15,360 INFO | main () | [AbstractService:?] ...finished service destruction 2020-04-21 20:52:15,095 INFO | pool-9-thread-1 () | [EventConsumer:?] Interrupted while waiting for event. application-pipeline.conf input { file { path => \"E:/My_Application/Config/application.log\" type => \"log\" start_position => \"beginning\" sincedb_path => \"NUL\" } } filter { grok { match => { \"message\" => \"%{DATA:date} %{DATA:time} %{DATA:log_level} \\| %{DATA:thread} \\| \\[%{DATA:service}\\] %{GREEDYDATA:log_msg}\" } add_field => { \"timestamp\" => \"%{date} %{time}\"} remove_field => [ \"date\", \"time\"] } if \"_grokparsefailure\" in [tags] { drop { } } date { match => [ \"timestamp\", \"YYYY-MM-dd HH:mm:ss,SSS\"] } mutate { add_field => { \"application_name\" => \"MY_APPLICATION\"} remove_field => [ \"timestamp\" ] } } output { elasticsearch { hosts => \"0.0.0.0:9200\" user => \"user\" password => \"passwd\" index => \"test-index-%{+YYYY.MM.dd}\" } } Ingest jmx data Requires a jmx.conf file defined in path in jmx input plugin input { jmx { path => \"C:/elk/logstash/jmx\" polling_frequency => 15 type => \"jmx\" } } jmx.conf { \"host\": \"localhost\", \"port\": 9000, \"queries\": [ { \"object_name\" : \"java.lang:type=Memory\", \"object_alias\" : \"Memory\" }, { \"object_name\" : \"java.lang:type=Runtime\", \"attributes\" : [ \"uptime\", \"StartTime\" ], \"object_alias\" : \"Runtime\" }, { \"object_name\" : \"java.lang:type=GarbageCollector,name=*\", \"attributes\" : [ \"CollectionCount\", \"CollectionTime\" ], \"object_alias\" : \"${type}.${name}\" }, { \"object_name\" : \"java.nio:type=BufferPool,name=*\", \"object_alias\" : \"${type}.${name}\" }] } Beats Filebeat Example: send docker container logs to logstash filebeat.inputs: - type: docker containers.path: /usr/share/dockerlogs/data containers.ids: - '1fb97e7e06677bc9c2aa2cee28b9a2e489a7c8ead62829d10a69e60e38d27310' #ignore_decoding_error: true json.message_key: log json.keys_under_root: true #exclude_lines: ['^\\n'] multiline.pattern: '^\\s' multiline.match: after processors: - add_docker_metadata: host: \"unix:///var/run/docker.sock\" - timestamp: field: time layouts: - '2020-02-12T06:59:01.944972082Z' filebeat.config.modules: path: ${path.config}/modules.d/*.yml reload.enabled: false output.logstash: hosts: [\"172.18.1.200:5000\"] logging.to_files: true logging.to_syslog: false Kibana JMX Example Line chart visualization Y-axis: Average X-axis: Date Histogram Split series Filters aggregation bucket: metric_path:jvm.Memory.HeapMemoryUsage.used Health Checks Correlation Token","title":"Monitoring"},{"location":"architecture/monitoring/#monitoring","text":"The 3 pillars of Observability: Metrics Answers: \"How has the system performance changed throughout time?\" numbers related to events that happened in a specifc time range example technologies: Metricbeats or Prometheus with Grafana Logs Answers: \"What is happening in the system?\" gives insights into a single events occuring in the system example technologies: ELK stack Tracing Answers: \"How are the system components interacting with each other?\" shows how long a specific request spent in the different system components example technologies: Jaeger or Zipkin","title":"Monitoring"},{"location":"architecture/monitoring/#live-metrics-stream","text":"app system platform","title":"Live Metrics Stream"},{"location":"architecture/monitoring/#server-metrics","text":"CPU Utilization Memory Utilization Network Utilization Disk Performance and Read/Writes Disk Space","title":"Server Metrics"},{"location":"architecture/monitoring/#optimization-game-plan","text":"Run performance tests Capture and analyze metrics Select the optimal instance Monitor for outliers to make sure the optimal instance continues to be optimal","title":"Optimization Game Plan"},{"location":"architecture/monitoring/#grafana","text":"","title":"Grafana"},{"location":"architecture/monitoring/#centeralized-logging","text":"","title":"Centeralized Logging"},{"location":"architecture/monitoring/#elk-stack","text":"Open Distro Apache 2.0 license Security Alerting SQL Performance Annalyzer (perftop CLI tool) Kubernetes Operator: Elastic Cloud on Kubernetes (ECK) uses the free tier Elasticsearch License","title":"ELK Stack"},{"location":"architecture/monitoring/#configure-elasticsearch-stack-in-containers","text":"docker-compose quickstart repo","title":"Configure Elasticsearch stack in containers"},{"location":"architecture/monitoring/#increase-heap-size-allocate-more-memory-for-es-in-jvm-options","text":"Update environment variables in docker-compose file - \"ES_JAVA_OPTS=-Xms4g -Xmx4g\"","title":"Increase heap size / allocate more memory for ES in JVM options:"},{"location":"architecture/monitoring/#modify-file-descriptors","text":"File descriptors: \"Make sure to increase the limit on the number of open files descriptors for the user running Elasticsearch to 65,536 or higher.\" run everything as root user ulimit -Hn : 4096 -> 1024000 vim /usr/lib/sysctl.d/elasticsearch.conf vim /etc/security/limits/conf restorecon /etc/security/limits.conf reboot system /usr/lib/sysctl.d/elasticsearch.conf fs.file-max = 512000 vm.max_map_count = 524288 vm.swappiness = 1 /etc/security/limits/conf * soft nofile 1024000 * hard nofile 1024000 * soft memlock unlimited * hard memlock unlimited elastic soft nofile 1024000 elastic hard nofile 1024000 elastic soft memlock unlimited elastic hard memlock unlimited root soft nofile 1024000 root hard nofile 1024000 root soft memlock unlimited root hard memlock unlimited","title":"Modify file descriptors:"},{"location":"architecture/monitoring/#logstash","text":"Install additional plugins: /bin/logstash install logstash-input-jmx Run pipeline: /bin/logstash.bat -f logstash-pipeline.conf --config.reload.automatic Test pipeline configuration: /bin/logstash.bat -f logstash-pipeline.conf --config.test_and_exit Run as service on a windows server: use NSSM","title":"Logstash"},{"location":"architecture/monitoring/#ingest-log-file","text":"Log sample 2020-04-21 20:52:15,173 WARN | ActiveMQ Task-1 () | [NetworkConnector:?] Could not start network... 2020-04-21 20:52:15,360 INFO | main () | [AbstractService:?] ...finished service destruction 2020-04-21 20:52:15,095 INFO | pool-9-thread-1 () | [EventConsumer:?] Interrupted while waiting for event. application-pipeline.conf input { file { path => \"E:/My_Application/Config/application.log\" type => \"log\" start_position => \"beginning\" sincedb_path => \"NUL\" } } filter { grok { match => { \"message\" => \"%{DATA:date} %{DATA:time} %{DATA:log_level} \\| %{DATA:thread} \\| \\[%{DATA:service}\\] %{GREEDYDATA:log_msg}\" } add_field => { \"timestamp\" => \"%{date} %{time}\"} remove_field => [ \"date\", \"time\"] } if \"_grokparsefailure\" in [tags] { drop { } } date { match => [ \"timestamp\", \"YYYY-MM-dd HH:mm:ss,SSS\"] } mutate { add_field => { \"application_name\" => \"MY_APPLICATION\"} remove_field => [ \"timestamp\" ] } } output { elasticsearch { hosts => \"0.0.0.0:9200\" user => \"user\" password => \"passwd\" index => \"test-index-%{+YYYY.MM.dd}\" } }","title":"Ingest log file"},{"location":"architecture/monitoring/#ingest-jmx-data","text":"Requires a jmx.conf file defined in path in jmx input plugin input { jmx { path => \"C:/elk/logstash/jmx\" polling_frequency => 15 type => \"jmx\" } } jmx.conf { \"host\": \"localhost\", \"port\": 9000, \"queries\": [ { \"object_name\" : \"java.lang:type=Memory\", \"object_alias\" : \"Memory\" }, { \"object_name\" : \"java.lang:type=Runtime\", \"attributes\" : [ \"uptime\", \"StartTime\" ], \"object_alias\" : \"Runtime\" }, { \"object_name\" : \"java.lang:type=GarbageCollector,name=*\", \"attributes\" : [ \"CollectionCount\", \"CollectionTime\" ], \"object_alias\" : \"${type}.${name}\" }, { \"object_name\" : \"java.nio:type=BufferPool,name=*\", \"object_alias\" : \"${type}.${name}\" }] }","title":"Ingest jmx data"},{"location":"architecture/monitoring/#beats","text":"","title":"Beats"},{"location":"architecture/monitoring/#filebeat","text":"Example: send docker container logs to logstash filebeat.inputs: - type: docker containers.path: /usr/share/dockerlogs/data containers.ids: - '1fb97e7e06677bc9c2aa2cee28b9a2e489a7c8ead62829d10a69e60e38d27310' #ignore_decoding_error: true json.message_key: log json.keys_under_root: true #exclude_lines: ['^\\n'] multiline.pattern: '^\\s' multiline.match: after processors: - add_docker_metadata: host: \"unix:///var/run/docker.sock\" - timestamp: field: time layouts: - '2020-02-12T06:59:01.944972082Z' filebeat.config.modules: path: ${path.config}/modules.d/*.yml reload.enabled: false output.logstash: hosts: [\"172.18.1.200:5000\"] logging.to_files: true logging.to_syslog: false","title":"Filebeat"},{"location":"architecture/monitoring/#kibana","text":"","title":"Kibana"},{"location":"architecture/monitoring/#jmx-example","text":"Line chart visualization Y-axis: Average X-axis: Date Histogram Split series Filters aggregation bucket: metric_path:jvm.Memory.HeapMemoryUsage.used","title":"JMX Example"},{"location":"architecture/monitoring/#health-checks","text":"","title":"Health Checks"},{"location":"architecture/monitoring/#correlation-token","text":"","title":"Correlation Token"},{"location":"architecture/security/","text":"Security secure access to services and infrastructure resources network access controls identity and access privileges encrypt in transit and stored data monitor & alert: user activity network traffic cloud & network configuration in all environments identify vulnerabilities in: application code (static code analysis) build artifacts VM & Container images operating systems cloud & networking configuration RSA & GPG ssh-keygen -t rsa -C user@email user in comment should match the user to login to remote system Server Security Best Practices keep instances up to date and patched enforce hardening add vulnerability monitoring implement monitoring and alerting tools Securing a Web Server SSL Test Debian fail2ban Firewall ufw Secrets sealed secrets: encrypted secrets stored in repositories Vault an alternative to storing secrets as encrypted text in repositories that offers additional features beyond secure secret storage: dynamic secrets: generating secrets for on-demand access data encryption: encrypt and decrypt data in databases without Vault actually storing it leasing and renewl: associates a lease with all secrets that provides the ability to renew the leases or revoke the secret if expired revocation: revoke single secrets or a tree of secrets such as all secrets read by a specific user or of a particular type (useful for key rolling and locking down systems) monitor access Setup Install Manual: download vault binary and adding it to PATH Mac (Homebrew): brew install vault Windows (Chocolately): choco install vault Start a DEV server vault server -dev NOTE: Never run the DEV server in production! Save configuration Save the generated unseal key into a new file called \"unseal.key\" Add environment variables: export VAULT_ADDR=\"http://127.0.0.1:8200\" export VAULT_DEV_ROOT_TOKEN_ID=\"s.TolAAwSmTYlBmE0E1M4M0ADG\" Check server satuts vault status Add secrets vault kv put secret/hello foo=world excited=yes Get secrets vault kv get secret/hello vault kv get -field=excited secret/hello vault kv get -format=json secret/hello | jq -r .data.data.excited Delete secrets vault kv delete secret/hello Network Traffic Ingress can be controlled and restricted using network ACLs, security groups, which are both effective firewalls, routing rules, and host based endpoint security tools which oftentimes contain firewall capabilities. Egress handled using internet gateways and nat gateways. As with ingress traffic, egress traffic should also be controlled and restricted for a number of reasons. Kali Linux","title":"Security"},{"location":"architecture/security/#security","text":"secure access to services and infrastructure resources network access controls identity and access privileges encrypt in transit and stored data monitor & alert: user activity network traffic cloud & network configuration in all environments identify vulnerabilities in: application code (static code analysis) build artifacts VM & Container images operating systems cloud & networking configuration","title":"Security"},{"location":"architecture/security/#rsa-gpg","text":"ssh-keygen -t rsa -C user@email user in comment should match the user to login to remote system","title":"RSA &amp; GPG"},{"location":"architecture/security/#server-security-best-practices","text":"keep instances up to date and patched enforce hardening add vulnerability monitoring implement monitoring and alerting tools","title":"Server Security Best Practices"},{"location":"architecture/security/#securing-a-web-server","text":"SSL Test","title":"Securing a Web Server"},{"location":"architecture/security/#debian","text":"fail2ban","title":"Debian"},{"location":"architecture/security/#firewall","text":"ufw","title":"Firewall"},{"location":"architecture/security/#secrets","text":"sealed secrets: encrypted secrets stored in repositories","title":"Secrets"},{"location":"architecture/security/#vault","text":"an alternative to storing secrets as encrypted text in repositories that offers additional features beyond secure secret storage: dynamic secrets: generating secrets for on-demand access data encryption: encrypt and decrypt data in databases without Vault actually storing it leasing and renewl: associates a lease with all secrets that provides the ability to renew the leases or revoke the secret if expired revocation: revoke single secrets or a tree of secrets such as all secrets read by a specific user or of a particular type (useful for key rolling and locking down systems) monitor access","title":"Vault"},{"location":"architecture/security/#setup","text":"Install Manual: download vault binary and adding it to PATH Mac (Homebrew): brew install vault Windows (Chocolately): choco install vault Start a DEV server vault server -dev NOTE: Never run the DEV server in production! Save configuration Save the generated unseal key into a new file called \"unseal.key\" Add environment variables: export VAULT_ADDR=\"http://127.0.0.1:8200\" export VAULT_DEV_ROOT_TOKEN_ID=\"s.TolAAwSmTYlBmE0E1M4M0ADG\" Check server satuts vault status","title":"Setup"},{"location":"architecture/security/#add-secrets","text":"vault kv put secret/hello foo=world excited=yes","title":"Add secrets"},{"location":"architecture/security/#get-secrets","text":"vault kv get secret/hello vault kv get -field=excited secret/hello vault kv get -format=json secret/hello | jq -r .data.data.excited","title":"Get secrets"},{"location":"architecture/security/#delete-secrets","text":"vault kv delete secret/hello","title":"Delete secrets"},{"location":"architecture/security/#network-traffic","text":"","title":"Network Traffic"},{"location":"architecture/security/#ingress","text":"can be controlled and restricted using network ACLs, security groups, which are both effective firewalls, routing rules, and host based endpoint security tools which oftentimes contain firewall capabilities.","title":"Ingress"},{"location":"architecture/security/#egress","text":"handled using internet gateways and nat gateways. As with ingress traffic, egress traffic should also be controlled and restricted for a number of reasons.","title":"Egress"},{"location":"architecture/security/#kali-linux","text":"","title":"Kali Linux"},{"location":"architecture/virtualization/","text":"Virtualization Vagrant vagrant init ubuntu/bionic64 vagrant box add centos/8 vagrant status vagrant up vagrant up <hostname> vagrant suspend stores state on RAM vagrant halt stores state on Disk vagrant destroy -f removes stored state vagrant ssh <hostname> vagrant ssh-config Boxes https://app.vagrantup.com/boxes/search Synced Folders \"By default, Vagrant shares your project directory (remember, that is the one with the Vagrantfile) to the /vagrant directory in your guest machine.\" Vagrantfile Vagrant.configure(\"3\") do |config| # create mongo VM config.vm.define \"mongo\" do |mongo| mongo.vm.box = \"bento/ubuntu-16.04\" mongo.vm.provider \"virtualbox\" do |vb| vb.memory = \"512\" end mongo.vm.network \"private_netowrk\", ip: \"10.8.0.20\" mongo.vm.provision \"file\", source: \"files/mongo.conf\", destination: \"~/mongod/conf\" # try provisioning with ansible instead of shell script mongo.vm.provision \"shell\", path: \"provisioners/install-mongo.sh\" end # create node VM config.vm.define \"node\" do |node| node.vm.box = \"bento/ubuntu-16.04\" node.vm.network \"fowarded_port\", guest: 3000, host:8000 node.vm.provider \"virtualbox\" do |vb| vb.memory = \"512\" end node.vm.network \"private_network\", ip: \"10.8.0.21\" # try provisioning with ansible instead of shell script node.vm.provision \"shell\", path: \"privisioners/install-node.sh\" end # creates 3 ubuntu VMs (5..7).each do |i| config.vm.define \"ubuntu#{i}\" do |ubuntu| ubuntu.vm.box = \"ubuntu/bionic64\" ubuntu.vm.network \"private_network\", ip: \"10.8.0.#{i}\" end end config.vm.provider \"virtualbox\" do |v| v.memory = 8192 v.cpus = 4 end end Containerization Docker docker exec docker events Docker Compose Binding the Docker Socket Running docker commands from inside a container Linux: -v /var/run/docker.sock:/var/run/docker.sock Windows: -v //var/run/docker.sock:/var/run/docker.sock Container Orchestration Kubernetes AKS Concepts Core Concepts Nodes : a single machine, cluster resource to store data, run jobs, maintain workloads and create network routes Node Pools : Control Plane : set of containers (can be multiple master nodes) that manages the cluster: contains controller manager, API server, etcd, scheduler, CoreDNS, etc. kubelet : kubernetes agent running on worker nodes kube-proxy : Container runtime : Kubetctl : CLI to configure kubernetes and deploy & manage applications Pods : can be comprised of multiple containers deployed on a single node, receives 1 IP Namespaces : Labels & Annotations : allows to filter on related resources in the cluster Service : network endpoint to connect to a pod, 4 types: ClusterIP: only available within the cluster NodePort: high port allocated on each node to be reachable outside of the cluster LoadBalancer: controls an external load balancer endpoint external to the cluster ExternalName: adds CNAME DNS record to CoreDNS to allow pods to use a specified DNS name to reach a service outside of the cluster Service Discovery : custom DNS server that all Pods use to resolve names of other services, IPs and ports ReplicaSets : used to scale pods to a desired state and hold the current status for the system DaemonSets : used to deploy a single instance of an application on each node (e.g. log/metric exporter) StatefulSets : used to deploy applications that require the use of the same node and data persistence Jobs/CronJobs : used to deploy a container to complete a specific workload and be destory on successful completion ConfigMaps & Secrets : key-value environment variables to be passed to running workloads to determine different runtime behaviors, secrets are the envrypted version Deployments : a set of metadata to describe the requirements of a running workload Scheduler : ensures that if pods or nodes encounter problems, additional pods are ran on healthy nodes Storage : an abstraction layer to manage data persistency to outlast the lifetime of a pod CRDs : custom resource definitions Minikube Install minikube Install kubectl Create clutser minikube start (defaults to 2GB RAM, 2 CPUs, 20GB disk space) Bigger applications: minikube start --memory=8192 --cpu=4 --disk-size=50g Get the cluster IP: minikube ip Profiles minikube start -p <profile> --memory=8192 --cpu=4 --disk-size=50g Invoke a kubernetes service minikube service <service_name> Kubectl Cheatsheet kubectl version kubectl apply -f filename.yml : Watch pods: kubectl get pods -w kubectl get pods --all-namespaces kubectl get service kubectl get nodes kubectl get all Docs: kubectl explain services --recursive Filter: kubectl explain services.spec YAML Configuration Requirements: kind apiVersion metadata spec Example of a Service & Deployment defined in the same file kind: Service apiVersion: v1 metadata: name: app-nginx-service spec: type: NodePort ports: - port: 80 selector: app: app-nginx --- kind: Deployment apiVersion: apps/v1 metadata: name: app-nginx-deployment spec: replicas: 3 selector: matchLabels: app: app-nginx template: metadata: labels: app: app-nginx mycustomlabel: \"true\" spec: containers: - name: nginx image: nginx:stable Testing Manual deploy: docker run equivalent kubectl run my-app --image app:latest Cleanup/ docker rm equivalent: kubectl delete deployment my-app Manual scale: kubectl scale deploy/my-app --replicas 2 ( kubectl scale deployment my-app also works) Manually create service: kubectl expose Logs of single pod: kubectl logs deploy/my-app --folow --tail 1 Logs of up to 5 pods: kubectl logs deploy/my-app -l run=my-app Inspect a specific pod: kubectl descibe pod/my-app-74dh8h72d7-2j6kv kubectl create deployment sample --image my-app:latest --dry-run -o yaml Helm Logging kubectl -n <namespace> logs -f deployment/<app-name> --all-containers=true --since=10m Stern stern -n <namespace> <app-name> -t --since 10m Kail kail -n <namespace> -d <deployment> --label size=large kail -n <namespace> --svc <service> --ignore size=large --since=2h","title":"Virtualization"},{"location":"architecture/virtualization/#virtualization","text":"","title":"Virtualization"},{"location":"architecture/virtualization/#vagrant","text":"vagrant init ubuntu/bionic64 vagrant box add centos/8 vagrant status vagrant up vagrant up <hostname> vagrant suspend stores state on RAM vagrant halt stores state on Disk vagrant destroy -f removes stored state vagrant ssh <hostname> vagrant ssh-config","title":"Vagrant"},{"location":"architecture/virtualization/#boxes","text":"https://app.vagrantup.com/boxes/search","title":"Boxes"},{"location":"architecture/virtualization/#synced-folders","text":"\"By default, Vagrant shares your project directory (remember, that is the one with the Vagrantfile) to the /vagrant directory in your guest machine.\"","title":"Synced Folders"},{"location":"architecture/virtualization/#vagrantfile","text":"Vagrant.configure(\"3\") do |config| # create mongo VM config.vm.define \"mongo\" do |mongo| mongo.vm.box = \"bento/ubuntu-16.04\" mongo.vm.provider \"virtualbox\" do |vb| vb.memory = \"512\" end mongo.vm.network \"private_netowrk\", ip: \"10.8.0.20\" mongo.vm.provision \"file\", source: \"files/mongo.conf\", destination: \"~/mongod/conf\" # try provisioning with ansible instead of shell script mongo.vm.provision \"shell\", path: \"provisioners/install-mongo.sh\" end # create node VM config.vm.define \"node\" do |node| node.vm.box = \"bento/ubuntu-16.04\" node.vm.network \"fowarded_port\", guest: 3000, host:8000 node.vm.provider \"virtualbox\" do |vb| vb.memory = \"512\" end node.vm.network \"private_network\", ip: \"10.8.0.21\" # try provisioning with ansible instead of shell script node.vm.provision \"shell\", path: \"privisioners/install-node.sh\" end # creates 3 ubuntu VMs (5..7).each do |i| config.vm.define \"ubuntu#{i}\" do |ubuntu| ubuntu.vm.box = \"ubuntu/bionic64\" ubuntu.vm.network \"private_network\", ip: \"10.8.0.#{i}\" end end config.vm.provider \"virtualbox\" do |v| v.memory = 8192 v.cpus = 4 end end","title":"Vagrantfile"},{"location":"architecture/virtualization/#containerization","text":"","title":"Containerization"},{"location":"architecture/virtualization/#docker","text":"docker exec docker events","title":"Docker"},{"location":"architecture/virtualization/#docker-compose","text":"","title":"Docker Compose"},{"location":"architecture/virtualization/#binding-the-docker-socket","text":"Running docker commands from inside a container Linux: -v /var/run/docker.sock:/var/run/docker.sock Windows: -v //var/run/docker.sock:/var/run/docker.sock","title":"Binding the Docker Socket"},{"location":"architecture/virtualization/#container-orchestration","text":"","title":"Container Orchestration"},{"location":"architecture/virtualization/#kubernetes","text":"AKS Concepts","title":"Kubernetes"},{"location":"architecture/virtualization/#core-concepts","text":"Nodes : a single machine, cluster resource to store data, run jobs, maintain workloads and create network routes Node Pools : Control Plane : set of containers (can be multiple master nodes) that manages the cluster: contains controller manager, API server, etcd, scheduler, CoreDNS, etc. kubelet : kubernetes agent running on worker nodes kube-proxy : Container runtime : Kubetctl : CLI to configure kubernetes and deploy & manage applications Pods : can be comprised of multiple containers deployed on a single node, receives 1 IP Namespaces : Labels & Annotations : allows to filter on related resources in the cluster Service : network endpoint to connect to a pod, 4 types: ClusterIP: only available within the cluster NodePort: high port allocated on each node to be reachable outside of the cluster LoadBalancer: controls an external load balancer endpoint external to the cluster ExternalName: adds CNAME DNS record to CoreDNS to allow pods to use a specified DNS name to reach a service outside of the cluster Service Discovery : custom DNS server that all Pods use to resolve names of other services, IPs and ports ReplicaSets : used to scale pods to a desired state and hold the current status for the system DaemonSets : used to deploy a single instance of an application on each node (e.g. log/metric exporter) StatefulSets : used to deploy applications that require the use of the same node and data persistence Jobs/CronJobs : used to deploy a container to complete a specific workload and be destory on successful completion ConfigMaps & Secrets : key-value environment variables to be passed to running workloads to determine different runtime behaviors, secrets are the envrypted version Deployments : a set of metadata to describe the requirements of a running workload Scheduler : ensures that if pods or nodes encounter problems, additional pods are ran on healthy nodes Storage : an abstraction layer to manage data persistency to outlast the lifetime of a pod CRDs : custom resource definitions","title":"Core Concepts"},{"location":"architecture/virtualization/#minikube","text":"Install minikube Install kubectl","title":"Minikube"},{"location":"architecture/virtualization/#create-clutser","text":"minikube start (defaults to 2GB RAM, 2 CPUs, 20GB disk space) Bigger applications: minikube start --memory=8192 --cpu=4 --disk-size=50g Get the cluster IP: minikube ip","title":"Create clutser"},{"location":"architecture/virtualization/#profiles","text":"minikube start -p <profile> --memory=8192 --cpu=4 --disk-size=50g","title":"Profiles"},{"location":"architecture/virtualization/#invoke-a-kubernetes-service","text":"minikube service <service_name>","title":"Invoke a kubernetes service"},{"location":"architecture/virtualization/#kubectl","text":"Cheatsheet kubectl version kubectl apply -f filename.yml : Watch pods: kubectl get pods -w kubectl get pods --all-namespaces kubectl get service kubectl get nodes kubectl get all Docs: kubectl explain services --recursive Filter: kubectl explain services.spec","title":"Kubectl"},{"location":"architecture/virtualization/#yaml-configuration","text":"Requirements: kind apiVersion metadata spec","title":"YAML Configuration"},{"location":"architecture/virtualization/#example-of-a-service-deployment-defined-in-the-same-file","text":"kind: Service apiVersion: v1 metadata: name: app-nginx-service spec: type: NodePort ports: - port: 80 selector: app: app-nginx --- kind: Deployment apiVersion: apps/v1 metadata: name: app-nginx-deployment spec: replicas: 3 selector: matchLabels: app: app-nginx template: metadata: labels: app: app-nginx mycustomlabel: \"true\" spec: containers: - name: nginx image: nginx:stable","title":"Example of a Service &amp; Deployment defined in the same file"},{"location":"architecture/virtualization/#testing","text":"Manual deploy: docker run equivalent kubectl run my-app --image app:latest Cleanup/ docker rm equivalent: kubectl delete deployment my-app Manual scale: kubectl scale deploy/my-app --replicas 2 ( kubectl scale deployment my-app also works) Manually create service: kubectl expose Logs of single pod: kubectl logs deploy/my-app --folow --tail 1 Logs of up to 5 pods: kubectl logs deploy/my-app -l run=my-app Inspect a specific pod: kubectl descibe pod/my-app-74dh8h72d7-2j6kv kubectl create deployment sample --image my-app:latest --dry-run -o yaml","title":"Testing"},{"location":"architecture/virtualization/#helm","text":"","title":"Helm"},{"location":"architecture/virtualization/#logging","text":"kubectl -n <namespace> logs -f deployment/<app-name> --all-containers=true --since=10m","title":"Logging"},{"location":"architecture/virtualization/#stern","text":"stern -n <namespace> <app-name> -t --since 10m","title":"Stern"},{"location":"architecture/virtualization/#kail","text":"kail -n <namespace> -d <deployment> --label size=large kail -n <namespace> --svc <service> --ignore size=large --since=2h","title":"Kail"},{"location":"data/analysis/","text":"Data Analysis NumPy Pandas Notes Import pandas as pd df = pd.read_csv(\u201cfile.csv\u201d) Outputs: (rows, columns) df.shape Outputs: first/last 5 rows & header df.head(5) or df.tail Check for null values df.isnull().values.any() Data fram correlation function df.corr() Delete column from data frame del df(\u2018column_name\u2019) Mold data (change True/False values to 1 or 0) column_map = { True: 1, False: 0 } df[\u2018column_name\u2019] = df[\u2018column_name\u2019].map(column_map) Creating column ratios (True/False ratio) num_true = len(df.loc[df[\u2018column_name\u2019] == True]) num_false = len(df.loc[df[\u2018column_name\u2019] == False]) Count: len(dataframe) Count group by distinct: df[COLUMN.value_counts() Count null values df['COLUMN'].isna().sum() len(df) - df['COLUMN'].count() Count distict values, use nunique: df['hID'].nunique() Count only non-null values, use count: df['hID'].count() Count total values including null values, use size attribute: df['hID'].size https://davidhamann.de/2017/06/26/pandas-select-elements-by-string/ https://proview-demo.nonprod.caqh.org/DirectAssure/api/POPracticeLocation/MatchReport/6053_20190606_144049 Visualization","title":"Data Analysis"},{"location":"data/analysis/#data-analysis","text":"","title":"Data Analysis"},{"location":"data/analysis/#numpy","text":"","title":"NumPy"},{"location":"data/analysis/#pandas","text":"","title":"Pandas"},{"location":"data/analysis/#notes","text":"Import pandas as pd df = pd.read_csv(\u201cfile.csv\u201d) Outputs: (rows, columns) df.shape Outputs: first/last 5 rows & header df.head(5) or df.tail Check for null values df.isnull().values.any() Data fram correlation function df.corr() Delete column from data frame del df(\u2018column_name\u2019) Mold data (change True/False values to 1 or 0) column_map = { True: 1, False: 0 } df[\u2018column_name\u2019] = df[\u2018column_name\u2019].map(column_map) Creating column ratios (True/False ratio) num_true = len(df.loc[df[\u2018column_name\u2019] == True]) num_false = len(df.loc[df[\u2018column_name\u2019] == False]) Count: len(dataframe) Count group by distinct: df[COLUMN.value_counts() Count null values df['COLUMN'].isna().sum() len(df) - df['COLUMN'].count() Count distict values, use nunique: df['hID'].nunique() Count only non-null values, use count: df['hID'].count() Count total values including null values, use size attribute: df['hID'].size https://davidhamann.de/2017/06/26/pandas-select-elements-by-string/ https://proview-demo.nonprod.caqh.org/DirectAssure/api/POPracticeLocation/MatchReport/6053_20190606_144049","title":"Notes"},{"location":"data/analysis/#visualization","text":"","title":"Visualization"},{"location":"data/big_data/","text":"Big Data Cloudera Impala hive databases not showing up? INVALIDATE METADATA; Is impala returning 0 results for a parquet table it should be able to read? Refresh it. REFRESH db_name.table search for string SELECT COUNT(*) FROM cpsc_db.cpsc_premium WHERE INSTR(organization_name, \"SNP\") != 0; Hive built-in types https://support.treasuredata.com/hc/en-us/articles/360001450728-Hive-Built-in-Operators regex replace (remove non-ASCII chars) SELECT regexp_replace(org_plan_name, '\\\\P{ASCII}', '') FROM cpsc_stage_db.cpsc_contract WHERE year=2019; DESCRIBE PARTITION desc formatted dbname.tablename partition (name=value) ADD PARTITION hive (cpsc_stage_db)> alter table cpsc_contract add partition(year=2019, month=02) > location '/data/domain/cpsc/cpsc_stage_db/cpsc_contract/year=2019/month=02'; DROP PARTITION hive (cpsc_stage_db)> alter table cpsc_contract drop if exists partition(year=2019, month=02); NOTES: this removes the hdfs folder location of the partition data TRUNCATE TABLE TRUNCATE TABLE IF EXISTS $tablename SHELL SCRIPTING hive -e (query) hive -f (file) Skip reading header line when creating hive table from CSV TBLPROPERTIES ( 'skip.header.line.count'='1' ); Creating Hive table from CSV PARTITIONED BY ( year int ) ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde' WITH SERDEPROPERTIES ( \"separatorChar\"=\",\", \"quoteChar\"=\"\\\"\") STORED AS TEXTFILE Create partitioned Hive table Create HDFS dirs (/table/year=2019/month=01) hdfs dfs -put (the csvs to the hdfs dirs) ALTER TABLE with each partition and LOCATION '/hdfs/path/to/new/dirs/' Spark PySpark sql_c = SQLContext(sc) df = sql_c.read.csv('HDFS PATH') df.printSchema() df.show() df.write.csv('HDFS PATH/mycsv.csv') remove non-ascii chars re.sub(r'[^\\x00-\\x7f]',r'', 'hi \u00bb') MLib Oozie Sentry","title":"Big Data"},{"location":"data/big_data/#big-data","text":"","title":"Big Data"},{"location":"data/big_data/#cloudera","text":"","title":"Cloudera"},{"location":"data/big_data/#impala","text":"hive databases not showing up? INVALIDATE METADATA; Is impala returning 0 results for a parquet table it should be able to read? Refresh it. REFRESH db_name.table search for string SELECT COUNT(*) FROM cpsc_db.cpsc_premium WHERE INSTR(organization_name, \"SNP\") != 0;","title":"Impala"},{"location":"data/big_data/#hive","text":"built-in types https://support.treasuredata.com/hc/en-us/articles/360001450728-Hive-Built-in-Operators","title":"Hive"},{"location":"data/big_data/#regex-replace-remove-non-ascii-chars","text":"SELECT regexp_replace(org_plan_name, '\\\\P{ASCII}', '') FROM cpsc_stage_db.cpsc_contract WHERE year=2019;","title":"regex replace (remove non-ASCII chars)"},{"location":"data/big_data/#describe-partition","text":"desc formatted dbname.tablename partition (name=value)","title":"DESCRIBE PARTITION"},{"location":"data/big_data/#add-partition","text":"hive (cpsc_stage_db)> alter table cpsc_contract add partition(year=2019, month=02) > location '/data/domain/cpsc/cpsc_stage_db/cpsc_contract/year=2019/month=02';","title":"ADD PARTITION"},{"location":"data/big_data/#drop-partition","text":"hive (cpsc_stage_db)> alter table cpsc_contract drop if exists partition(year=2019, month=02); NOTES: this removes the hdfs folder location of the partition data","title":"DROP PARTITION"},{"location":"data/big_data/#truncate-table","text":"TRUNCATE TABLE IF EXISTS $tablename","title":"TRUNCATE TABLE"},{"location":"data/big_data/#shell-scripting","text":"hive -e (query) hive -f (file)","title":"SHELL SCRIPTING"},{"location":"data/big_data/#skip-reading-header-line-when-creating-hive-table-from-csv","text":"TBLPROPERTIES ( 'skip.header.line.count'='1' );","title":"Skip reading header line when creating hive table from CSV"},{"location":"data/big_data/#creating-hive-table-from-csv","text":"PARTITIONED BY ( year int ) ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde' WITH SERDEPROPERTIES ( \"separatorChar\"=\",\", \"quoteChar\"=\"\\\"\") STORED AS TEXTFILE","title":"Creating Hive table from CSV"},{"location":"data/big_data/#create-partitioned-hive-table","text":"Create HDFS dirs (/table/year=2019/month=01) hdfs dfs -put (the csvs to the hdfs dirs) ALTER TABLE with each partition and LOCATION '/hdfs/path/to/new/dirs/'","title":"Create partitioned Hive table"},{"location":"data/big_data/#spark","text":"","title":"Spark"},{"location":"data/big_data/#pyspark","text":"sql_c = SQLContext(sc) df = sql_c.read.csv('HDFS PATH') df.printSchema() df.show() df.write.csv('HDFS PATH/mycsv.csv') remove non-ascii chars re.sub(r'[^\\x00-\\x7f]',r'', 'hi \u00bb')","title":"PySpark"},{"location":"data/big_data/#mlib","text":"","title":"MLib"},{"location":"data/big_data/#oozie","text":"","title":"Oozie"},{"location":"data/big_data/#sentry","text":"","title":"Sentry"},{"location":"data/machine_learning/","text":"Machine Learning Shallow Learning Scikit-learn Deep Learning Tensor Flow","title":"Machine Learning"},{"location":"data/machine_learning/#machine-learning","text":"","title":"Machine Learning"},{"location":"data/machine_learning/#shallow-learning","text":"","title":"Shallow Learning"},{"location":"data/machine_learning/#scikit-learn","text":"","title":"Scikit-learn"},{"location":"data/machine_learning/#deep-learning","text":"","title":"Deep Learning"},{"location":"data/machine_learning/#tensor-flow","text":"","title":"Tensor Flow"},{"location":"data/pipelines/","text":"Data Pipelines facilitate the efficient flow of data from one point to another flow of data extraction, transformation, combination, validation, converging of data from multiple streams into one facilitate parallel processing using distributed data processing data standardization (cleanup beforehand to avoid processing errors) data processing (segregate data into flows based on business requirements & route it to specific destinations) data analysis (models and machine learning) data visualization (present gathered intel to stakeholders) data storage & security (archive and encrypt data) Data Ingestion Data collection layer Data query layer Data processing layer Data visualization layer Data storage layer Data security layer Real-time streams important for time-critical data (medical, financial, etc.) can be slow and require smaller data sets Apache Spark, Storm, Kafka, Flink are high-performant due to in-memory storage abd examples of distributed data processing engines capable of processing real-time streaming data Spark integrates seemlessly with HDFS, S3, etc. Batch analyzing trends over time can include full data sets due to it not being time-critical ETL flow Hadoop implementing MapReduce is lower performant due to disk usage and is used in batch processing data Use Cases momving big data into a Hadoop cluster to process and run analytics moving data (and continuing to update) from the primary storage / legacy systems into Elasticsearch to be indexed and searched on more quickly processing and aggregating logs and metrics from servers, services, and IoT devices into Elasticsearch or other time-series databases to be visualized and run analytics on them real-time stream processing engines for real-time events (sports, financial, medical, etc.) to dump into message queues such as Kafka or stream computation frameworks (Storm, Nifi, Spark, Flink, Samza, Kinesis, etc.) to implement real-time large-scale data processing features","title":"Data Pipelines"},{"location":"data/pipelines/#data-pipelines","text":"facilitate the efficient flow of data from one point to another flow of data extraction, transformation, combination, validation, converging of data from multiple streams into one facilitate parallel processing using distributed data processing data standardization (cleanup beforehand to avoid processing errors) data processing (segregate data into flows based on business requirements & route it to specific destinations) data analysis (models and machine learning) data visualization (present gathered intel to stakeholders) data storage & security (archive and encrypt data)","title":"Data Pipelines"},{"location":"data/pipelines/#data-ingestion","text":"Data collection layer Data query layer Data processing layer Data visualization layer Data storage layer Data security layer","title":"Data Ingestion"},{"location":"data/pipelines/#real-time-streams","text":"important for time-critical data (medical, financial, etc.) can be slow and require smaller data sets Apache Spark, Storm, Kafka, Flink are high-performant due to in-memory storage abd examples of distributed data processing engines capable of processing real-time streaming data Spark integrates seemlessly with HDFS, S3, etc.","title":"Real-time streams"},{"location":"data/pipelines/#batch","text":"analyzing trends over time can include full data sets due to it not being time-critical ETL flow Hadoop implementing MapReduce is lower performant due to disk usage and is used in batch processing data","title":"Batch"},{"location":"data/pipelines/#use-cases","text":"momving big data into a Hadoop cluster to process and run analytics moving data (and continuing to update) from the primary storage / legacy systems into Elasticsearch to be indexed and searched on more quickly processing and aggregating logs and metrics from servers, services, and IoT devices into Elasticsearch or other time-series databases to be visualized and run analytics on them real-time stream processing engines for real-time events (sports, financial, medical, etc.) to dump into message queues such as Kafka or stream computation frameworks (Storm, Nifi, Spark, Flink, Samza, Kinesis, etc.) to implement real-time large-scale data processing features","title":"Use Cases"},{"location":"data/storage/","text":"Data Storage Relational Databases PostgreSQL Command Action \\l List available databases \\c dbname Connect to a new database \\dt List available tables \\d tablename Describe the details of given table \\dn List all schemas in the current database \\df List functions in the current database \\h Get help on syntax of SQL commands \\? Lists all psql slash commands \\set System variables list \\timing Shows how long a query took to execute \\x Show expanded query results \\q Quit psql MySQL SQLite Oracle SQL Server NoSQL Databases MongoDB Distributed Data Storage HDFS reread Clouder_notes doc I made during THP for HDFS & Security MGMT and Big Data ETLs Command Commands hdfs dfs \u2013ls [hdfs path] hdfs dfs -mkdir [hdfs path] hdfs dfs -rm -r [hdfs path] hdfs dfs -put [local path] [hdfs path] hdfs dfs -get [hdfs path] hdfs dfs -getfacl [hdfs path] hdfs dfs -setfacl [group:name:rwx] [hdfs path] Hbase Cassandra Message Queue ZeroMQ The Official Guide great read for anyone working with distributed systems","title":"Data Storage"},{"location":"data/storage/#data-storage","text":"","title":"Data Storage"},{"location":"data/storage/#relational-databases","text":"","title":"Relational Databases"},{"location":"data/storage/#postgresql","text":"Command Action \\l List available databases \\c dbname Connect to a new database \\dt List available tables \\d tablename Describe the details of given table \\dn List all schemas in the current database \\df List functions in the current database \\h Get help on syntax of SQL commands \\? Lists all psql slash commands \\set System variables list \\timing Shows how long a query took to execute \\x Show expanded query results \\q Quit psql","title":"PostgreSQL"},{"location":"data/storage/#mysql","text":"","title":"MySQL"},{"location":"data/storage/#sqlite","text":"","title":"SQLite"},{"location":"data/storage/#oracle","text":"","title":"Oracle"},{"location":"data/storage/#sql-server","text":"","title":"SQL Server"},{"location":"data/storage/#nosql-databases","text":"","title":"NoSQL Databases"},{"location":"data/storage/#mongodb","text":"","title":"MongoDB"},{"location":"data/storage/#distributed-data-storage","text":"","title":"Distributed Data Storage"},{"location":"data/storage/#hdfs","text":"reread Clouder_notes doc I made during THP for HDFS & Security MGMT and Big Data ETLs","title":"HDFS"},{"location":"data/storage/#command-commands","text":"hdfs dfs \u2013ls [hdfs path] hdfs dfs -mkdir [hdfs path] hdfs dfs -rm -r [hdfs path] hdfs dfs -put [local path] [hdfs path] hdfs dfs -get [hdfs path] hdfs dfs -getfacl [hdfs path] hdfs dfs -setfacl [group:name:rwx] [hdfs path]","title":"Command Commands"},{"location":"data/storage/#hbase","text":"","title":"Hbase"},{"location":"data/storage/#cassandra","text":"","title":"Cassandra"},{"location":"data/storage/#message-queue","text":"","title":"Message Queue"},{"location":"data/storage/#zeromq","text":"The Official Guide great read for anyone working with distributed systems","title":"ZeroMQ"},{"location":"math/calculus/","text":"Calculus","title":"Calculus"},{"location":"math/calculus/#calculus","text":"","title":"Calculus"},{"location":"math/linear_algebra/","text":"Linear Algebra","title":"Linear Algebra"},{"location":"math/linear_algebra/#linear-algebra","text":"","title":"Linear Algebra"},{"location":"math/statistics/","text":"Statistics","title":"Statistics"},{"location":"math/statistics/#statistics","text":"","title":"Statistics"},{"location":"software/algorithms/","text":"Algorithms algorithm categories how long does the algorithm run with 1,000 records and does it scale to 1 million? common sense estimation O(n): simple loops (1 to n) O(n^2): nested loops O(lg(n)): binary chop (halves the set it considers each time it starts the loop) O(nlg(n)): divide and conquer (partition input, work on partitions separetly, and then combine results) degrades to O(n^2) when fed sorted imput O(2^n): combinatoric (permutations) Patterns Brute Force enumerate all possible solutions, and check them for correctness Greedy Keep track of the base answer so far, in one pass through the input Hash Table Spend space to save time by using a hash map or sometimes just an array Sorting Recursion even if the algorithm does not create any data structures, the call stack will generate O(n) space complexity Dynamic Programming Graph BFS queue will find shortest path usually takes more memory than DFS DFS stack can easily be implemented with recursion","title":"Algorithms"},{"location":"software/algorithms/#algorithms","text":"algorithm categories how long does the algorithm run with 1,000 records and does it scale to 1 million? common sense estimation O(n): simple loops (1 to n) O(n^2): nested loops O(lg(n)): binary chop (halves the set it considers each time it starts the loop) O(nlg(n)): divide and conquer (partition input, work on partitions separetly, and then combine results) degrades to O(n^2) when fed sorted imput O(2^n): combinatoric (permutations)","title":"Algorithms"},{"location":"software/algorithms/#patterns","text":"Brute Force enumerate all possible solutions, and check them for correctness Greedy Keep track of the base answer so far, in one pass through the input Hash Table Spend space to save time by using a hash map or sometimes just an array","title":"Patterns"},{"location":"software/algorithms/#sorting","text":"","title":"Sorting"},{"location":"software/algorithms/#recursion","text":"even if the algorithm does not create any data structures, the call stack will generate O(n) space complexity","title":"Recursion"},{"location":"software/algorithms/#dynamic-programming","text":"","title":"Dynamic Programming"},{"location":"software/algorithms/#graph","text":"","title":"Graph"},{"location":"software/algorithms/#bfs","text":"queue will find shortest path usually takes more memory than DFS","title":"BFS"},{"location":"software/algorithms/#dfs","text":"stack can easily be implemented with recursion","title":"DFS"},{"location":"software/best_practices/","text":"Best Practices Manage Growth of Requirements track scope creep communicate expectations continiously maintain a project glossary that defines all terms and how they are used in the project go the extra mile and deliver quality of life features Tracer Bullets prototype with a real and simplified version of the project it will contain all of the structure, error checking, documentation, and self-checking that production code has, it will not be fully-functional however once an end-to-end connection among components of the system is achieved, testing with users can begin adding additional functionality afterwards becomes straightforward Temporal Coupling allow for concurrency of tasks not dependent on each other reduces time-based dependency and increase flexibility Dynamic Configuration allow for the system to be highly configurable with metadata from screen colors and prompt text to algorithms, database and UI style Testing Unit Tests shows examples of how to use all the functionality of a module provides a means to build regressions tests to validate further changes to the code test subcomponents of a module first before testing the entire module use test coverage most importantly test every state of the program rather than every line of code x / (0 to 999) only has two states - testing 0 will cause an error while all 999 other tests will not Test Harness standard way to specify setup and cleanup method for selecting indivdual tests or all available tests means of analyzing output for expected or unexpected results standardized form of failure reporting and logging status xUnit (Kent Beck & Erich Gamma) Integration Tests shows that major subsystems sthat make up the project work with each other End-to-end tests does it meet the functional requirements of the system Performance testings stress test with real-world conditions (number of users/connections/transactions per second) how well does it scale? test resource exhaustion, errors, and recovery Usability testing sit down with the end user who will use the system to understand their tasks/goals that the system is trying to help them accomplish record requirements, use cases and scenarios Alistair Cockburn template: characteristic information (goal, scope, level, preconditions, success/failed end condition, primary actor, trigger) main success scenario extensions variations related information (priority, performance target, frequency, superordinate/subordinate use cases, channel to primary actor, secondary actors) schedule open issues TDD BDD DDD Testing in Production the final release is sometimes compiled/configured differently from earlier versions and should be tested all over again Testing Tests introduce bugs and ensure that the tests complain about them catch bugs once and then add them into the tests, do not let them reappear uncaught Refactoring don't refactor and add functionality simultaneously ensure that good tests are present and run them as often as possible take short, deliberate steps and test after each change DRY Code Source Control Management Documentation & Comments comments should discuss why something is done, the code already shows how documentation is another view of the same underlying model incorporate it into the development process, don't let it become an afterthought automate html documentation using code generators API Naming Conventions Code Generators Passive templates for creating new source files performing one-off conversions from one language to another producing lookup tables and other resources that are expensive to compute at runtime Active adhearing to the DRY principle allows for a single model to produce multiple views getting two disparate environments to work together can benefit greatly from an active code generator e.g. a schema ran through a code generator to produce both the software code and documentation in html Estimating check requirements analyze risk design, implement, integrate validate with users iterate understand what is being asked understand the scope (assuming these conditions, then...) ask someone else who has done something similar before come back with an answer after making calculations estimate based on a simplified model of the problem the simplified model will inevitably introduce inaccuracies doubling the effort on refining the model might only give a slight increase to accuracy however break the model into components identify their parameters that affect how the component contributes to the overall model assign values to the parameters work out which ones have the most impact on the result and concentrate on getting those right calculate the answers multiple times with varying parameter values hedge the final answer based on conditions the units used make a difference in the interpretation of the result 1-15 days: days 3-8 weeks: weeks 8-30 weeks: months 30+ weeks: think hard before delivering an estimate go back and evaluate the estimate with the reality","title":"Best Practices"},{"location":"software/best_practices/#best-practices","text":"","title":"Best Practices"},{"location":"software/best_practices/#manage-growth-of-requirements","text":"track scope creep communicate expectations continiously maintain a project glossary that defines all terms and how they are used in the project go the extra mile and deliver quality of life features","title":"Manage Growth of Requirements"},{"location":"software/best_practices/#tracer-bullets","text":"prototype with a real and simplified version of the project it will contain all of the structure, error checking, documentation, and self-checking that production code has, it will not be fully-functional however once an end-to-end connection among components of the system is achieved, testing with users can begin adding additional functionality afterwards becomes straightforward","title":"Tracer Bullets"},{"location":"software/best_practices/#temporal-coupling","text":"allow for concurrency of tasks not dependent on each other reduces time-based dependency and increase flexibility","title":"Temporal Coupling"},{"location":"software/best_practices/#dynamic-configuration","text":"allow for the system to be highly configurable with metadata from screen colors and prompt text to algorithms, database and UI style","title":"Dynamic Configuration"},{"location":"software/best_practices/#testing","text":"","title":"Testing"},{"location":"software/best_practices/#unit-tests","text":"shows examples of how to use all the functionality of a module provides a means to build regressions tests to validate further changes to the code test subcomponents of a module first before testing the entire module use test coverage most importantly test every state of the program rather than every line of code x / (0 to 999) only has two states - testing 0 will cause an error while all 999 other tests will not","title":"Unit Tests"},{"location":"software/best_practices/#test-harness","text":"standard way to specify setup and cleanup method for selecting indivdual tests or all available tests means of analyzing output for expected or unexpected results standardized form of failure reporting and logging status xUnit (Kent Beck & Erich Gamma)","title":"Test Harness"},{"location":"software/best_practices/#integration-tests","text":"shows that major subsystems sthat make up the project work with each other","title":"Integration Tests"},{"location":"software/best_practices/#end-to-end-tests","text":"does it meet the functional requirements of the system","title":"End-to-end tests"},{"location":"software/best_practices/#performance-testings","text":"stress test with real-world conditions (number of users/connections/transactions per second) how well does it scale? test resource exhaustion, errors, and recovery","title":"Performance testings"},{"location":"software/best_practices/#usability-testing","text":"sit down with the end user who will use the system to understand their tasks/goals that the system is trying to help them accomplish record requirements, use cases and scenarios Alistair Cockburn template: characteristic information (goal, scope, level, preconditions, success/failed end condition, primary actor, trigger) main success scenario extensions variations related information (priority, performance target, frequency, superordinate/subordinate use cases, channel to primary actor, secondary actors) schedule open issues","title":"Usability testing"},{"location":"software/best_practices/#tdd","text":"","title":"TDD"},{"location":"software/best_practices/#bdd","text":"","title":"BDD"},{"location":"software/best_practices/#ddd","text":"","title":"DDD"},{"location":"software/best_practices/#testing-in-production","text":"the final release is sometimes compiled/configured differently from earlier versions and should be tested all over again","title":"Testing in Production"},{"location":"software/best_practices/#testing-tests","text":"introduce bugs and ensure that the tests complain about them catch bugs once and then add them into the tests, do not let them reappear uncaught","title":"Testing Tests"},{"location":"software/best_practices/#refactoring","text":"don't refactor and add functionality simultaneously ensure that good tests are present and run them as often as possible take short, deliberate steps and test after each change","title":"Refactoring"},{"location":"software/best_practices/#dry-code","text":"","title":"DRY Code"},{"location":"software/best_practices/#source-control-management","text":"","title":"Source Control Management"},{"location":"software/best_practices/#documentation-comments","text":"comments should discuss why something is done, the code already shows how documentation is another view of the same underlying model incorporate it into the development process, don't let it become an afterthought automate html documentation using code generators","title":"Documentation &amp; Comments"},{"location":"software/best_practices/#api-naming-conventions","text":"","title":"API Naming Conventions"},{"location":"software/best_practices/#code-generators","text":"","title":"Code Generators"},{"location":"software/best_practices/#passive","text":"templates for creating new source files performing one-off conversions from one language to another producing lookup tables and other resources that are expensive to compute at runtime","title":"Passive"},{"location":"software/best_practices/#active","text":"adhearing to the DRY principle allows for a single model to produce multiple views getting two disparate environments to work together can benefit greatly from an active code generator e.g. a schema ran through a code generator to produce both the software code and documentation in html","title":"Active"},{"location":"software/best_practices/#estimating","text":"check requirements analyze risk design, implement, integrate validate with users iterate understand what is being asked understand the scope (assuming these conditions, then...) ask someone else who has done something similar before come back with an answer after making calculations estimate based on a simplified model of the problem the simplified model will inevitably introduce inaccuracies doubling the effort on refining the model might only give a slight increase to accuracy however break the model into components identify their parameters that affect how the component contributes to the overall model assign values to the parameters work out which ones have the most impact on the result and concentrate on getting those right calculate the answers multiple times with varying parameter values hedge the final answer based on conditions the units used make a difference in the interpretation of the result 1-15 days: days 3-8 weeks: weeks 8-30 weeks: months 30+ weeks: think hard before delivering an estimate go back and evaluate the estimate with the reality","title":"Estimating"},{"location":"software/computer_science/","text":"Computer Science Logic Gates Computer Architecture Operating Systems Languages and Compilers Computer Netowrking Embedded Programming Blockchain Nand2Tetris Notes","title":"Computer Science"},{"location":"software/computer_science/#computer-science","text":"","title":"Computer Science"},{"location":"software/computer_science/#logic-gates","text":"","title":"Logic Gates"},{"location":"software/computer_science/#computer-architecture","text":"","title":"Computer Architecture"},{"location":"software/computer_science/#operating-systems","text":"","title":"Operating Systems"},{"location":"software/computer_science/#languages-and-compilers","text":"","title":"Languages and Compilers"},{"location":"software/computer_science/#computer-netowrking","text":"","title":"Computer Netowrking"},{"location":"software/computer_science/#embedded-programming","text":"","title":"Embedded Programming"},{"location":"software/computer_science/#blockchain","text":"","title":"Blockchain"},{"location":"software/computer_science/#nand2tetris-notes","text":"","title":"Nand2Tetris Notes"},{"location":"software/data_structures/","text":"Data Structures Strings Arrays Strenghts: fast lookups fast appends cache-friendly Weaknesses: fixed size costly inserts and deletes space -> O(n) lookup -> O(1) append -> O(1) insert -> O(n) delete -> O(n) Static fast lookups O(1), but each item in the array needs to be the same size, and you need a big block of uninterrupted free memory to store the array pointer-based arrays are not reflected in time cost, even though: it's slower because it's not CPU cache-friendly (continious in RAM) pointer-based array requires less uninterrupted memory and can accommodate elements that aren't all the same size Dynamic don't have to specify the size ahead of time, but the disadvantage is that some appends can be expensive Strenghts: fast lookups variable size cache-friendly Weaknesses: slow worst-case appends costly inserts and deletes space -> O(n) lookup -> O(1) append -> O(1) insert -> O(n) delete -> O(n) Linked Lists have faster prepends and faster appends than dynamic arrays O(1), but they have slower lookups O(n) Hash Tables use hash function to translate key into an index handle hash collisions by having the value at the index be a pointer to a linked list with all keys that collide (considered rare enough with complicated balancing to still have O(1) lookups) Strenghts: fast lookups O(1) on average flexible keys allowing for most data types Weaknesses: slow worst-case lookups O(n) unordered keys single-directional lookups (O(n) key lookup) not cache-friendly (due to using linked lists) space -> O(n) lookup -> O(1) (Worst-case: O(n)) insert -> O(1) (Worst-case: O(n)) delete -> O(1) (Worst-case: O(n)) Sets similar implentation of hash maps but keys do not store associated values useful for tracking groups of items\u2014nodes visited in a graph, characters seen in a string, or colors used by neighboring nodes Trees Binary Tree a tree where each node has at most 2 children the number of total nodes on each level doubles as we move down the tree the number of nodes on the last level is equal to the sum of the number of nodes on all other levels (plus 1) total nodes = 2^h - 1 height = lg(n+1) Binary Search Tree (BST) ordered binary tree the nodes to the left are smaller than the current node the nodes to the right are larger than the current node Strengths: - good performance across the board - lookups O(lg(n)) - inserts O(lg(n)) - deletes O(lg(n)) - better worst-case performance than a hash table O(n) but not as good as its average case O(1) - if balanced: - sorted in O(n) time using an in-order traversal - finding an element closest to a value can be done in O(lg(n)) Weaknesses: - poor performance if unbalanced - no constant time O(1) operations AVL Red-Black Perfect Tree height = lg((n+1)/2) + 1 = log(n+1) Graphs Representing links. Graphs are ideal for cases where you're working with things that connect to other things. Nodes and edges could, for example, respectively represent cities and highways, routers and ethernet cables, or Facebook users and their friendships Scaling challenges. Most graph algorithms are O(n*lg(n))O(n\u2217lg(n)) or even slower. Depending on the size of your graph, running algorithms across your nodes may not be feasible. directed vs. undirected (links with direction) cyclic vs. acyclic (cyclic graphs contains at least one unbroken series of nodes with no repeating nodes or edges that connects back to itself) weighted vs. unweighted (each edge contains a weight) legal coloring vs. illegal coloring (no adjacent nodes have the same color)","title":"Data Structures"},{"location":"software/data_structures/#data-structures","text":"","title":"Data Structures"},{"location":"software/data_structures/#strings","text":"","title":"Strings"},{"location":"software/data_structures/#arrays","text":"Strenghts: fast lookups fast appends cache-friendly Weaknesses: fixed size costly inserts and deletes space -> O(n) lookup -> O(1) append -> O(1) insert -> O(n) delete -> O(n)","title":"Arrays"},{"location":"software/data_structures/#static","text":"fast lookups O(1), but each item in the array needs to be the same size, and you need a big block of uninterrupted free memory to store the array pointer-based arrays are not reflected in time cost, even though: it's slower because it's not CPU cache-friendly (continious in RAM) pointer-based array requires less uninterrupted memory and can accommodate elements that aren't all the same size","title":"Static"},{"location":"software/data_structures/#dynamic","text":"don't have to specify the size ahead of time, but the disadvantage is that some appends can be expensive Strenghts: fast lookups variable size cache-friendly Weaknesses: slow worst-case appends costly inserts and deletes space -> O(n) lookup -> O(1) append -> O(1) insert -> O(n) delete -> O(n)","title":"Dynamic"},{"location":"software/data_structures/#linked-lists","text":"have faster prepends and faster appends than dynamic arrays O(1), but they have slower lookups O(n)","title":"Linked Lists"},{"location":"software/data_structures/#hash-tables","text":"use hash function to translate key into an index handle hash collisions by having the value at the index be a pointer to a linked list with all keys that collide (considered rare enough with complicated balancing to still have O(1) lookups) Strenghts: fast lookups O(1) on average flexible keys allowing for most data types Weaknesses: slow worst-case lookups O(n) unordered keys single-directional lookups (O(n) key lookup) not cache-friendly (due to using linked lists) space -> O(n) lookup -> O(1) (Worst-case: O(n)) insert -> O(1) (Worst-case: O(n)) delete -> O(1) (Worst-case: O(n))","title":"Hash Tables"},{"location":"software/data_structures/#sets","text":"similar implentation of hash maps but keys do not store associated values useful for tracking groups of items\u2014nodes visited in a graph, characters seen in a string, or colors used by neighboring nodes","title":"Sets"},{"location":"software/data_structures/#trees","text":"","title":"Trees"},{"location":"software/data_structures/#binary-tree","text":"a tree where each node has at most 2 children the number of total nodes on each level doubles as we move down the tree the number of nodes on the last level is equal to the sum of the number of nodes on all other levels (plus 1) total nodes = 2^h - 1 height = lg(n+1)","title":"Binary Tree"},{"location":"software/data_structures/#binary-search-tree-bst","text":"ordered binary tree the nodes to the left are smaller than the current node the nodes to the right are larger than the current node Strengths: - good performance across the board - lookups O(lg(n)) - inserts O(lg(n)) - deletes O(lg(n)) - better worst-case performance than a hash table O(n) but not as good as its average case O(1) - if balanced: - sorted in O(n) time using an in-order traversal - finding an element closest to a value can be done in O(lg(n)) Weaknesses: - poor performance if unbalanced - no constant time O(1) operations","title":"Binary Search Tree (BST)"},{"location":"software/data_structures/#avl","text":"","title":"AVL"},{"location":"software/data_structures/#red-black","text":"","title":"Red-Black"},{"location":"software/data_structures/#perfect-tree","text":"height = lg((n+1)/2) + 1 = log(n+1)","title":"Perfect Tree"},{"location":"software/data_structures/#graphs","text":"Representing links. Graphs are ideal for cases where you're working with things that connect to other things. Nodes and edges could, for example, respectively represent cities and highways, routers and ethernet cables, or Facebook users and their friendships Scaling challenges. Most graph algorithms are O(n*lg(n))O(n\u2217lg(n)) or even slower. Depending on the size of your graph, running algorithms across your nodes may not be feasible. directed vs. undirected (links with direction) cyclic vs. acyclic (cyclic graphs contains at least one unbroken series of nodes with no repeating nodes or edges that connects back to itself) weighted vs. unweighted (each edge contains a weight) legal coloring vs. illegal coloring (no adjacent nodes have the same color)","title":"Graphs"},{"location":"software/design_patterns/","text":"Design Patterns Singleton Factory Pattern","title":"Design Patterns"},{"location":"software/design_patterns/#design-patterns","text":"","title":"Design Patterns"},{"location":"software/design_patterns/#singleton","text":"","title":"Singleton"},{"location":"software/design_patterns/#factory-pattern","text":"","title":"Factory Pattern"},{"location":"software/functional_paradigm/","text":"Functional Paradigm Haskell","title":"Functional Paradigm"},{"location":"software/functional_paradigm/#functional-paradigm","text":"","title":"Functional Paradigm"},{"location":"software/functional_paradigm/#haskell","text":"","title":"Haskell"},{"location":"software/guis/","text":"Graphical User Interfaces Vue Resources Vue UI Libarry HTML Tempaltes html5up templated Notes Block entire subdomain from getting indexed by search engines: robot.txt in root dir User-agent: * Disallow: / Block a specific HTML page: <meta name=\"robots\" content=\"noindex\"> in <head>","title":"GUIs"},{"location":"software/guis/#graphical-user-interfaces","text":"","title":"Graphical User Interfaces"},{"location":"software/guis/#vue","text":"","title":"Vue"},{"location":"software/guis/#resources","text":"Vue UI Libarry HTML Tempaltes html5up templated","title":"Resources"},{"location":"software/guis/#notes","text":"Block entire subdomain from getting indexed by search engines: robot.txt in root dir User-agent: * Disallow: / Block a specific HTML page: <meta name=\"robots\" content=\"noindex\"> in <head>","title":"Notes"},{"location":"software/oop/","text":"Object Oriented Programming Encapsulation Abstraction Inheritance Polymorphism Java Install JDK Debian / Ubuntu apt-get install openjdk-11-jdk CentOS / Fedora / RHEL yum -y install java-11-openjdk java-11-openjdk-devel cat > /etc/profile.d/java11.sh <<EOF export JAVA_HOME=$(dirname $(dirname $(readlink $(readlink $(which javac))))) export PATH=\\$PATH:\\$JAVA_HOME/bin EOF source /etc/profile.d/java11.sh Windows OpenJDK Install page for JDK 11 OpenJDK Archive Download JDK Set PATH (e.g. add .../Java/jdk-11/bin to PATH ) Set JAVA_HOME (i.e. set to jdk installation path without /bin dir) Test with java --version command and configure the JDK in IntelliJ","title":"OOP"},{"location":"software/oop/#object-oriented-programming","text":"Encapsulation Abstraction Inheritance Polymorphism","title":"Object Oriented Programming"},{"location":"software/oop/#java","text":"","title":"Java"},{"location":"software/oop/#install-jdk","text":"","title":"Install JDK"},{"location":"software/oop/#debian-ubuntu","text":"apt-get install openjdk-11-jdk","title":"Debian / Ubuntu"},{"location":"software/oop/#centos-fedora-rhel","text":"yum -y install java-11-openjdk java-11-openjdk-devel cat > /etc/profile.d/java11.sh <<EOF export JAVA_HOME=$(dirname $(dirname $(readlink $(readlink $(which javac))))) export PATH=\\$PATH:\\$JAVA_HOME/bin EOF source /etc/profile.d/java11.sh","title":"CentOS / Fedora / RHEL"},{"location":"software/oop/#windows","text":"OpenJDK Install page for JDK 11 OpenJDK Archive Download JDK Set PATH (e.g. add .../Java/jdk-11/bin to PATH ) Set JAVA_HOME (i.e. set to jdk installation path without /bin dir) Test with java --version command and configure the JDK in IntelliJ","title":"Windows"},{"location":"software/python/","text":"Python Type Checking type checker module: pip install mypy examples: method signature: def add(a: int, b: int) -> int: variable declaration: agw: int = 99 Style Guidlines: PEP8 & Linters PEP8 PEP20: Python Zen PEP287: Doc-String PEP463: Exceptions-catching PEP484: Type Hints Formatting Docstrings for all public modules, functions, classes and methods imports separated in 3 groups by blank lines standard library 3rd party libraries local application/library Whitespace/punctuation identations: 4 spaces per lect max line length: 79 characters 2 lines between top-level functions and classes 1 line between methods inside a class Naming modules: short, lowercase names classes: CapitalizedNaming functions: lowercase_with_underscores constants: ALL_CAPS non-public names: _start_with_underscore Linters pylint checks for PEP8 and other code smells pip install pylint pylint <package> or pylint <package>/file.py add pylintrc file: pylint --generate-rcfile > pylintrc Black great for CI pipelines to enforce consistency in source code pip install black black <package> Generating Documentation PEP 257: semantics and conventions for docstrings Docstrings string as first statement statements of a module, function, class or method becomes the __doc__ atribute, callable in python always use \"\"\"three double quotes\"\"\" end phrases in a period methods should specify a return value Sphinx generates HTML, PDF, eBook, Linux man pages, etc. from source extracts docstrings from code accepts reStructured Text to build documentation pages generate API docs: sphinx-apidoc -o <package> Quickstart pip install sphinx mkdir docs && cd docs sphinx-quickstart make html clean old builds: make clean html Tip: Use reStructured Text in Docstrings to automatically generate documentation: module Docstring: \"\"\" module.py --------- This module contains a module-level docstring \"\"\" class MyClass: \"\"\" The MyClass class represents a generic class. Link to the :meth:`run` method. \"\"\" def run(self, name): \"\"\" Runs the class with the name of :attr:`name`. :param name : The name of the class :return: The new value of :attr:`name`. \"\"\" Package Management pip pip freeze > requirements.txt pip install requirements.txt pipenv Common Errors RescursionError = stack overflow Sets light_bulbs = set() light_bulbs.add('incandescent') light_bulbs.add('compact fluorescent') light_bulbs.add('LED') 'LED' in light_bulbs # True 'halogen' in light_bulbs # False","title":"Python"},{"location":"software/python/#python","text":"","title":"Python"},{"location":"software/python/#type-checking","text":"type checker module: pip install mypy examples: method signature: def add(a: int, b: int) -> int: variable declaration: agw: int = 99","title":"Type Checking"},{"location":"software/python/#style-guidlines-pep8-linters","text":"PEP8 PEP20: Python Zen PEP287: Doc-String PEP463: Exceptions-catching PEP484: Type Hints","title":"Style Guidlines: PEP8 &amp; Linters"},{"location":"software/python/#formatting","text":"Docstrings for all public modules, functions, classes and methods imports separated in 3 groups by blank lines standard library 3rd party libraries local application/library","title":"Formatting"},{"location":"software/python/#whitespacepunctuation","text":"identations: 4 spaces per lect max line length: 79 characters 2 lines between top-level functions and classes 1 line between methods inside a class","title":"Whitespace/punctuation"},{"location":"software/python/#naming","text":"modules: short, lowercase names classes: CapitalizedNaming functions: lowercase_with_underscores constants: ALL_CAPS non-public names: _start_with_underscore","title":"Naming"},{"location":"software/python/#linters","text":"","title":"Linters"},{"location":"software/python/#pylint","text":"checks for PEP8 and other code smells pip install pylint pylint <package> or pylint <package>/file.py add pylintrc file: pylint --generate-rcfile > pylintrc","title":"pylint"},{"location":"software/python/#black","text":"great for CI pipelines to enforce consistency in source code pip install black black <package>","title":"Black"},{"location":"software/python/#generating-documentation","text":"PEP 257: semantics and conventions for docstrings","title":"Generating Documentation"},{"location":"software/python/#docstrings","text":"string as first statement statements of a module, function, class or method becomes the __doc__ atribute, callable in python always use \"\"\"three double quotes\"\"\" end phrases in a period methods should specify a return value","title":"Docstrings"},{"location":"software/python/#sphinx","text":"generates HTML, PDF, eBook, Linux man pages, etc. from source extracts docstrings from code accepts reStructured Text to build documentation pages generate API docs: sphinx-apidoc -o <package>","title":"Sphinx"},{"location":"software/python/#quickstart","text":"pip install sphinx mkdir docs && cd docs sphinx-quickstart make html clean old builds: make clean html Tip: Use reStructured Text in Docstrings to automatically generate documentation: module Docstring: \"\"\" module.py --------- This module contains a module-level docstring \"\"\" class MyClass: \"\"\" The MyClass class represents a generic class. Link to the :meth:`run` method. \"\"\" def run(self, name): \"\"\" Runs the class with the name of :attr:`name`. :param name : The name of the class :return: The new value of :attr:`name`. \"\"\"","title":"Quickstart"},{"location":"software/python/#package-management","text":"","title":"Package Management"},{"location":"software/python/#pip","text":"pip freeze > requirements.txt pip install requirements.txt","title":"pip"},{"location":"software/python/#pipenv","text":"","title":"pipenv"},{"location":"software/python/#common-errors","text":"RescursionError = stack overflow","title":"Common Errors"},{"location":"software/python/#sets","text":"light_bulbs = set() light_bulbs.add('incandescent') light_bulbs.add('compact fluorescent') light_bulbs.add('LED') 'LED' in light_bulbs # True 'halogen' in light_bulbs # False","title":"Sets"}]}
{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Engineering Wiki Gardening (Projects) Blacksmith Shop (Service) Sign your work present the end story - visuals/graphs/charts - for managers/clients what they want/are excited to see dig into the weeds (text) afterswards collect and visualize KPIs that showed how a business function performed before and after your consulting / product / service What does it take to be a top performer? - KPIs/Metrics the customers expectations at the time of delivery and what's in the actual delivery the customers expectations are evolving as much as the project is evolving you need to spend as much time managing customers expectations as you do on the project itself Guild System (Career)","title":"Engineering Wiki"},{"location":"#engineering-wiki","text":"","title":"Engineering Wiki"},{"location":"#gardening-projects","text":"","title":"Gardening (Projects)"},{"location":"#blacksmith-shop-service","text":"Sign your work present the end story - visuals/graphs/charts - for managers/clients what they want/are excited to see dig into the weeds (text) afterswards collect and visualize KPIs that showed how a business function performed before and after your consulting / product / service What does it take to be a top performer? - KPIs/Metrics the customers expectations at the time of delivery and what's in the actual delivery the customers expectations are evolving as much as the project is evolving you need to spend as much time managing customers expectations as you do on the project itself","title":"Blacksmith Shop (Service)"},{"location":"#guild-system-career","text":"","title":"Guild System (Career)"},{"location":"design/","text":"Design Drawing Picture Design Color Theory Typography UI / UX","title":"Design"},{"location":"design/#design","text":"","title":"Design"},{"location":"design/#drawing","text":"","title":"Drawing"},{"location":"design/#picture-design","text":"","title":"Picture Design"},{"location":"design/#color-theory","text":"","title":"Color Theory"},{"location":"design/#typography","text":"","title":"Typography"},{"location":"design/#ui-ux","text":"","title":"UI / UX"},{"location":"resources/","text":"Curriculum Books Name Author Field The Pragmatic Programmer Andrew Hunt & David Thomas Career Soft Skills John Sonmez Career Cracking the Code Interview Laakman McDowell Career The Linux Command Lines William E. Shotts Jr. Linux Designing Distributed Systems Brenden Burns Architecture Kubernetes Up and Running Brenden Burns Architecture Managing Kubernetes Brenden Burns Architecture Kubernetes Best Practices Brenden Burns Architecture Designing Data-Intensive Apps... Martin Kleppmann Architecture Patterns of Enterprise App Architeture Martin Fowler Architecture Algorithms to Live By Christian & Griffiths Software Engineering Clean Code Robert Cecil Martin Software Engineering Growing Object Oriented Software... Steve Freeman & Nat Pryce Software Engineering Code Complete, Second Edition Steve McConnell Software Engineering Refactoring Kent Beck & Martin Fowler Software Engineering Design Patterns Gang of Four Software Engineering Head First Design Patterns E. Freeman & K. Sierra Software Engineering Analysis Patterns Martin Fowler Software Engineering Operating Systems: Three Easy Pieces A. & R. Arpaci-Dusseau Software Engineering Working Effectively with Legacy Code Michael Feathers Software Engineering The Data Engineering Cookbook Andreas Kretz Data Engineering Python for Data Analysis Wes Mckinney Data Engineering Python for Finance Yves Hilpisch Data Engineering Deep Learning Courville et. all Data Engineering The Design Of Everyday Things Dom Norman Design University MOOCS Data Structures & Algorithms Stanford CS106X Data Structures Princeton Algorithms Part 1 Princeton Algorithms Part 2 MIT Algorithms MIT Advanced Data Structures U Alberta Design Patterns Java / OOP UC San Diego OOP Java Programming Specialization Object Oriented Programming in Java Data Structures and Performance Advanced Data Structures in Java Mastering the Software Engineering Interview Capstone: Analyzing (Social) Network Data RICE Parallel, Concurrent, and Distributed Programming in Java Parallel Programming Concurrent Programming Distributed Programming Computer Architecture Stanford CS107 Computer Organization & Systems Stanford CS143 Computer Organization & Systems U Jerusalem Nand to Tetris, Part 1 U Jerusalem Nand to Tetris, Part 2 Data Engineering Courses Stanford Databases Stanford Machine Learning Georgia Tech Machine Learning for Trading U San Francisco Computational Linear Algebra U San Francisco Applied ML DevOps Engineering Courses U Virginia CICD DevOps Unafilliated MOOCS Educative Grokking the System Design Interview Gaurav Sen System Design Udacity CloudOps Engineer Bret Fisher : Docker & Kubernetes Jose Portilla Data Strucutres & Algorithms in Python Data Analysis & Machine Learning Khan Academy Statistics (inference & probability) Linear Algebra Calculus 1,2, & 3 Deeplearning.ai : Tensorflow Operating Systems: Three Easy Pieces Course Online Tutorials Tensorflow Docs Spark Tutorials, Data Bricks [PySpark](https://www.edx.org/course/big-data-analytics-using-spark Azure K8s Kubernetes%20Learning%20Path_Version%202.0.pdf [Terraform - [https://learn.hashicorp.com/terraform/azure/intro_az Backtrader Tutorials GitHub Hello World Setup First Script Using Analyzers Optimization Modularization Creating Analyzers Data Replay Sentdex Python Intermediate Python Pandas Machine Learning Scikit-learn Finance Deep Learning Matplotlib Dash Web Django Flask Beauitful Soup Fun Alexa Skills Kivy/KivEnt AI SC2 Unsupervised ML Reinforcement Learning Other Golang RasPi RasPi Distributed Computing Quantum Computer Programming Other Media Podcasts Software Engineering Daily Talk Python Python Bytes The Changelog Command Line Heros Machine Learning Guide Darknet Diaries Coding Blocks .NET Websites Architecture & Engineering https://12factor.net/ https://martinfowler.com/ https://staffeng.com/ https://free-for.dev/ https://teachyourselfcs.com/ https://learnk8s.io/production-best-practices https://github.com/andkret/Cookbook Technical Interview https://leetcode.com/ https://www.hackerrank.com/ (SQL) https://www.interviewcake.com/ https://hackernoon.com/google-interview-questions-deconstructed-the-knights-dialer-f780d516f029 Learning Labs https://linuxacademy.com/ https://www.katacoda.com/ OSS Datacase: dbeaver Terminal Emulator: Mac: iTerm2 Linux: Konsole Windows: MobaXterm Source Code Editors: Python/Golang: VSCode Java: IntelliJ IDEA Kubernetes: Lens","title":"Resources"},{"location":"resources/#curriculum","text":"","title":"Curriculum"},{"location":"resources/#books","text":"Name Author Field The Pragmatic Programmer Andrew Hunt & David Thomas Career Soft Skills John Sonmez Career Cracking the Code Interview Laakman McDowell Career The Linux Command Lines William E. Shotts Jr. Linux Designing Distributed Systems Brenden Burns Architecture Kubernetes Up and Running Brenden Burns Architecture Managing Kubernetes Brenden Burns Architecture Kubernetes Best Practices Brenden Burns Architecture Designing Data-Intensive Apps... Martin Kleppmann Architecture Patterns of Enterprise App Architeture Martin Fowler Architecture Algorithms to Live By Christian & Griffiths Software Engineering Clean Code Robert Cecil Martin Software Engineering Growing Object Oriented Software... Steve Freeman & Nat Pryce Software Engineering Code Complete, Second Edition Steve McConnell Software Engineering Refactoring Kent Beck & Martin Fowler Software Engineering Design Patterns Gang of Four Software Engineering Head First Design Patterns E. Freeman & K. Sierra Software Engineering Analysis Patterns Martin Fowler Software Engineering Operating Systems: Three Easy Pieces A. & R. Arpaci-Dusseau Software Engineering Working Effectively with Legacy Code Michael Feathers Software Engineering The Data Engineering Cookbook Andreas Kretz Data Engineering Python for Data Analysis Wes Mckinney Data Engineering Python for Finance Yves Hilpisch Data Engineering Deep Learning Courville et. all Data Engineering The Design Of Everyday Things Dom Norman Design","title":"Books"},{"location":"resources/#university-moocs","text":"","title":"University MOOCS"},{"location":"resources/#data-structures-algorithms","text":"Stanford CS106X Data Structures Princeton Algorithms Part 1 Princeton Algorithms Part 2 MIT Algorithms MIT Advanced Data Structures U Alberta Design Patterns","title":"Data Structures &amp; Algorithms"},{"location":"resources/#java-oop","text":"UC San Diego OOP Java Programming Specialization Object Oriented Programming in Java Data Structures and Performance Advanced Data Structures in Java Mastering the Software Engineering Interview Capstone: Analyzing (Social) Network Data RICE Parallel, Concurrent, and Distributed Programming in Java Parallel Programming Concurrent Programming Distributed Programming","title":"Java / OOP"},{"location":"resources/#computer-architecture","text":"Stanford CS107 Computer Organization & Systems Stanford CS143 Computer Organization & Systems U Jerusalem Nand to Tetris, Part 1 U Jerusalem Nand to Tetris, Part 2","title":"Computer Architecture"},{"location":"resources/#data-engineering-courses","text":"Stanford Databases Stanford Machine Learning Georgia Tech Machine Learning for Trading U San Francisco Computational Linear Algebra U San Francisco Applied ML","title":"Data Engineering Courses"},{"location":"resources/#devops-engineering-courses","text":"U Virginia CICD DevOps","title":"DevOps Engineering Courses"},{"location":"resources/#unafilliated-moocs","text":"Educative Grokking the System Design Interview Gaurav Sen System Design Udacity CloudOps Engineer Bret Fisher : Docker & Kubernetes Jose Portilla Data Strucutres & Algorithms in Python Data Analysis & Machine Learning Khan Academy Statistics (inference & probability) Linear Algebra Calculus 1,2, & 3 Deeplearning.ai : Tensorflow Operating Systems: Three Easy Pieces Course","title":"Unafilliated MOOCS"},{"location":"resources/#online-tutorials","text":"Tensorflow Docs Spark Tutorials, Data Bricks [PySpark](https://www.edx.org/course/big-data-analytics-using-spark Azure K8s Kubernetes%20Learning%20Path_Version%202.0.pdf [Terraform - [https://learn.hashicorp.com/terraform/azure/intro_az Backtrader Tutorials GitHub Hello World Setup First Script Using Analyzers Optimization Modularization Creating Analyzers Data Replay","title":"Online Tutorials"},{"location":"resources/#sentdex","text":"","title":"Sentdex"},{"location":"resources/#python","text":"Intermediate Python Pandas Machine Learning Scikit-learn Finance Deep Learning Matplotlib Dash","title":"Python"},{"location":"resources/#web","text":"Django Flask Beauitful Soup","title":"Web"},{"location":"resources/#fun","text":"Alexa Skills Kivy/KivEnt AI SC2 Unsupervised ML Reinforcement Learning","title":"Fun"},{"location":"resources/#other","text":"Golang RasPi RasPi Distributed Computing Quantum Computer Programming","title":"Other"},{"location":"resources/#other-media","text":"","title":"Other Media"},{"location":"resources/#podcasts","text":"Software Engineering Daily Talk Python Python Bytes The Changelog Command Line Heros Machine Learning Guide Darknet Diaries Coding Blocks .NET","title":"Podcasts"},{"location":"resources/#websites","text":"","title":"Websites"},{"location":"resources/#architecture-engineering","text":"https://12factor.net/ https://martinfowler.com/ https://staffeng.com/ https://free-for.dev/ https://teachyourselfcs.com/ https://learnk8s.io/production-best-practices https://github.com/andkret/Cookbook","title":"Architecture &amp; Engineering"},{"location":"resources/#technical-interview","text":"https://leetcode.com/ https://www.hackerrank.com/ (SQL) https://www.interviewcake.com/ https://hackernoon.com/google-interview-questions-deconstructed-the-knights-dialer-f780d516f029","title":"Technical Interview"},{"location":"resources/#learning-labs","text":"https://linuxacademy.com/ https://www.katacoda.com/","title":"Learning Labs"},{"location":"resources/#oss","text":"Datacase: dbeaver Terminal Emulator: Mac: iTerm2 Linux: Konsole Windows: MobaXterm Source Code Editors: Python/Golang: VSCode Java: IntelliJ IDEA Kubernetes: Lens","title":"OSS"},{"location":"wiki/","text":"Wiki Structure Deployment Workflow sudo pip install mkdocs Manual Vanilla mkdocs gh-deploy git commit & push master User/Org Pages cd /build mkdocs gh-deploy --config-file ..\\mkdocs.yml --remote-branch master git commit & push develop Custom Domain StackOverflow Answer mkdocs gh-deploy git commit & push master Specify custom domain in GitHub Pages settings Add A recorsd for github.io 185.199.108.153 185.199.109.153 185.199.110.153 185.199.111.153 Add CNAME record to point domain to GitHub Page user.github.io/repo_name Enforce HTTPS Build Server Self-hosted","title":"Wiki"},{"location":"wiki/#wiki","text":"","title":"Wiki"},{"location":"wiki/#structure","text":"","title":"Structure"},{"location":"wiki/#deployment-workflow","text":"sudo pip install mkdocs","title":"Deployment Workflow"},{"location":"wiki/#manual","text":"","title":"Manual"},{"location":"wiki/#vanilla","text":"mkdocs gh-deploy git commit & push master","title":"Vanilla"},{"location":"wiki/#userorg-pages","text":"cd /build mkdocs gh-deploy --config-file ..\\mkdocs.yml --remote-branch master git commit & push develop","title":"User/Org Pages"},{"location":"wiki/#custom-domain","text":"StackOverflow Answer mkdocs gh-deploy git commit & push master Specify custom domain in GitHub Pages settings Add A recorsd for github.io 185.199.108.153 185.199.109.153 185.199.110.153 185.199.111.153 Add CNAME record to point domain to GitHub Page user.github.io/repo_name Enforce HTTPS","title":"Custom Domain"},{"location":"wiki/#build-server","text":"","title":"Build Server"},{"location":"wiki/#self-hosted","text":"","title":"Self-hosted"},{"location":"architecture/case_studies/","text":"Case Studies Netflix NASA Amazon LinkedIn Google Facebook Lyft Uber","title":"Case Studies"},{"location":"architecture/case_studies/#case-studies","text":"","title":"Case Studies"},{"location":"architecture/case_studies/#netflix","text":"","title":"Netflix"},{"location":"architecture/case_studies/#nasa","text":"","title":"NASA"},{"location":"architecture/case_studies/#amazon","text":"","title":"Amazon"},{"location":"architecture/case_studies/#linkedin","text":"","title":"LinkedIn"},{"location":"architecture/case_studies/#google","text":"","title":"Google"},{"location":"architecture/case_studies/#facebook","text":"","title":"Facebook"},{"location":"architecture/case_studies/#lyft","text":"","title":"Lyft"},{"location":"architecture/case_studies/#uber","text":"","title":"Uber"},{"location":"architecture/chaos_eng/","text":"Chaos Engineering","title":"Chaos Engineering"},{"location":"architecture/chaos_eng/#chaos-engineering","text":"","title":"Chaos Engineering"},{"location":"architecture/cicd/","text":"CICD Build Jenkins Test Unit Tests Integration Tests End-to-end Tests Deploy ansible octopus spinnaker Provision Infrastructure as Code Terraform install terraform locally download binary download checksums & checksum signature verify the signartue of the checksum against HashiCorp's GPG key add terraform binary to PATH Create a VM on GCE example Once per project enable the GCE API: gcloud services enable compute.googleapis.com Create Terraform config Run commands at creation: metadata_startup_script logs: /var/log/daemon.log Add ssh keys: metadata = { ssh-keys = ### } Run Terraform terraform init terraform plan terraform apply Clean up Terraform config main.tf resource \"google_compute_instance\" \"default\" { name = \"vm-${random_id.instance_id.hex}\" machine_type = \"f1-micro\" zone = \"us-west1-a\" boot_disk { initialize_params { image = \"debian-cloud/debian-9\" } } # ... } Ansible Ansible Control Machine (Dockerized) ansible-console - REPL that comes with tab completion Access ENV variables: {{ lookup('env','USER') }}\" command module escapes commands, while the shell module does not Configuring VMs from Windows Create a control node VM with debian or ubuntu ssh into control node VM apt-get -y update && apt-get -y install ansible Note: run an installation script after creating a VM with Terraform sudo apt update sudo apt -y upgrade sudo apt-get install software-properties-common sudo apt-add-repository ppa:ansible/ansible sudo apt-get update sudo apt-get install ansible Point to inventory dir [defaults] inventory = /vagrant/inv host_key_checking = False Docs Ansible Docs Ansible Galaxy Docs ansible-doc copy ansible-doc -t connection anisble-doc -t shell --list ansible-doc -t shell csh list commands with ansible + tab Inventory ansible-inventory --graph --vars ansible-inventory --list Inventory definitions in YAML all: hosts: ubuntu10: ansible_host: 192.168.50.10 ansible_private_key_file: .vagrant/machines/ubuntu10/virtualbox/private_key Child group [ubuntu] ubuntu10 ubuntu11 ubuntu12 Parent group [vagrant:children] centos ubuntu [vagrant:vars] ansible_user=vagrant ansible_port=22 Ad-hoc command ansible all -m ping ansible -m command -a \"git config --global --list\" vagrant ansible -m copy -a \"src=master.gitconfig dest=~/.gitconfig\" localehost dry run: add --check show changes: add --diff show package managers: ansible -m setup -a \"filter=ansible_pkg_mgr\" all Playbook ansible-playbook playbook.yml Debug: add -v up to -vvvv name: Ensure ~/.gitconfig copied from master.gitconfig hosts: localhost tasks: - name: copy git config copy: src: \"mast.gitconfig\" dest: \"~/.gitconfig\" when: ... # conditional become: yes # escalate privilege register git_config_copy # register output to variable - name: print captured variable debug: var=git_config_copy","title":"CICD"},{"location":"architecture/cicd/#cicd","text":"","title":"CICD"},{"location":"architecture/cicd/#build","text":"","title":"Build"},{"location":"architecture/cicd/#jenkins","text":"","title":"Jenkins"},{"location":"architecture/cicd/#test","text":"","title":"Test"},{"location":"architecture/cicd/#unit-tests","text":"","title":"Unit Tests"},{"location":"architecture/cicd/#integration-tests","text":"","title":"Integration Tests"},{"location":"architecture/cicd/#end-to-end-tests","text":"","title":"End-to-end Tests"},{"location":"architecture/cicd/#deploy","text":"ansible octopus spinnaker","title":"Deploy"},{"location":"architecture/cicd/#provision","text":"","title":"Provision"},{"location":"architecture/cicd/#infrastructure-as-code","text":"","title":"Infrastructure as Code"},{"location":"architecture/cicd/#terraform","text":"install terraform locally download binary download checksums & checksum signature verify the signartue of the checksum against HashiCorp's GPG key add terraform binary to PATH","title":"Terraform"},{"location":"architecture/cicd/#create-a-vm-on-gce-example","text":"Once per project enable the GCE API: gcloud services enable compute.googleapis.com Create Terraform config Run commands at creation: metadata_startup_script logs: /var/log/daemon.log Add ssh keys: metadata = { ssh-keys = ### } Run Terraform terraform init terraform plan terraform apply Clean up Terraform config main.tf resource \"google_compute_instance\" \"default\" { name = \"vm-${random_id.instance_id.hex}\" machine_type = \"f1-micro\" zone = \"us-west1-a\" boot_disk { initialize_params { image = \"debian-cloud/debian-9\" } } # ... }","title":"Create a VM on GCE example"},{"location":"architecture/cicd/#ansible","text":"Ansible Control Machine (Dockerized) ansible-console - REPL that comes with tab completion Access ENV variables: {{ lookup('env','USER') }}\" command module escapes commands, while the shell module does not","title":"Ansible"},{"location":"architecture/cicd/#configuring-vms-from-windows","text":"Create a control node VM with debian or ubuntu ssh into control node VM apt-get -y update && apt-get -y install ansible Note: run an installation script after creating a VM with Terraform sudo apt update sudo apt -y upgrade sudo apt-get install software-properties-common sudo apt-add-repository ppa:ansible/ansible sudo apt-get update sudo apt-get install ansible Point to inventory dir [defaults] inventory = /vagrant/inv host_key_checking = False","title":"Configuring VMs from Windows"},{"location":"architecture/cicd/#docs","text":"Ansible Docs Ansible Galaxy Docs ansible-doc copy ansible-doc -t connection anisble-doc -t shell --list ansible-doc -t shell csh list commands with ansible + tab","title":"Docs"},{"location":"architecture/cicd/#inventory","text":"ansible-inventory --graph --vars ansible-inventory --list Inventory definitions in YAML all: hosts: ubuntu10: ansible_host: 192.168.50.10 ansible_private_key_file: .vagrant/machines/ubuntu10/virtualbox/private_key Child group [ubuntu] ubuntu10 ubuntu11","title":"Inventory"},{"location":"architecture/cicd/#ubuntu12","text":"Parent group [vagrant:children] centos ubuntu [vagrant:vars] ansible_user=vagrant ansible_port=22","title":"ubuntu12"},{"location":"architecture/cicd/#ad-hoc-command","text":"ansible all -m ping ansible -m command -a \"git config --global --list\" vagrant ansible -m copy -a \"src=master.gitconfig dest=~/.gitconfig\" localehost dry run: add --check show changes: add --diff show package managers: ansible -m setup -a \"filter=ansible_pkg_mgr\" all","title":"Ad-hoc command"},{"location":"architecture/cicd/#playbook","text":"ansible-playbook playbook.yml Debug: add -v up to -vvvv name: Ensure ~/.gitconfig copied from master.gitconfig hosts: localhost tasks: - name: copy git config copy: src: \"mast.gitconfig\" dest: \"~/.gitconfig\" when: ... # conditional become: yes # escalate privilege register git_config_copy # register output to variable - name: print captured variable debug: var=git_config_copy","title":"Playbook"},{"location":"architecture/highlevel_design/","text":"High-level Design System design Orthagonal Architecture Iterative Design The Twelve-Factor App Website Codebase: One codebase tracked in revision control, many deploys - Dependencies: Explicitly declare and isolate dependencies - Config: Store config in the environment - Backing services: Treat backing services as attached resources - Build, release, run: Strictly separate build and run stages - Process: Execute the app as one or more stateless processes - Port binding: Export services via port binding - Concurrency: Scale out via the process model - Disposability: Maximize robustness with fast startup and graceful shutdown - Dev/prod parity: Keep development, staging, and production as similar as possible - Logs: Treat logs as event streams - Admin processes: Run admin/management tasks as one-off processes - Distributed Systems Scaling Vertical Horizontal Monolith Microservice Load Balancing Consistent Hashing Message Queues Publisher / Subscriber Database Sharding Distributed Caching Notes","title":"High-level Design"},{"location":"architecture/highlevel_design/#high-level-design","text":"System design","title":"High-level Design"},{"location":"architecture/highlevel_design/#orthagonal-architecture","text":"","title":"Orthagonal Architecture"},{"location":"architecture/highlevel_design/#iterative-design","text":"","title":"Iterative Design"},{"location":"architecture/highlevel_design/#the-twelve-factor-app","text":"Website Codebase: One codebase tracked in revision control, many deploys - Dependencies: Explicitly declare and isolate dependencies - Config: Store config in the environment - Backing services: Treat backing services as attached resources - Build, release, run: Strictly separate build and run stages - Process: Execute the app as one or more stateless processes - Port binding: Export services via port binding - Concurrency: Scale out via the process model - Disposability: Maximize robustness with fast startup and graceful shutdown - Dev/prod parity: Keep development, staging, and production as similar as possible - Logs: Treat logs as event streams - Admin processes: Run admin/management tasks as one-off processes -","title":"The Twelve-Factor App"},{"location":"architecture/highlevel_design/#distributed-systems","text":"","title":"Distributed Systems"},{"location":"architecture/highlevel_design/#scaling","text":"","title":"Scaling"},{"location":"architecture/highlevel_design/#vertical","text":"","title":"Vertical"},{"location":"architecture/highlevel_design/#horizontal","text":"","title":"Horizontal"},{"location":"architecture/highlevel_design/#monolith","text":"","title":"Monolith"},{"location":"architecture/highlevel_design/#microservice","text":"","title":"Microservice"},{"location":"architecture/highlevel_design/#load-balancing","text":"","title":"Load Balancing"},{"location":"architecture/highlevel_design/#consistent-hashing","text":"","title":"Consistent Hashing"},{"location":"architecture/highlevel_design/#message-queues","text":"","title":"Message Queues"},{"location":"architecture/highlevel_design/#publisher-subscriber","text":"","title":"Publisher / Subscriber"},{"location":"architecture/highlevel_design/#database-sharding","text":"","title":"Database Sharding"},{"location":"architecture/highlevel_design/#distributed-caching","text":"","title":"Distributed Caching"},{"location":"architecture/highlevel_design/#notes","text":"","title":"Notes"},{"location":"architecture/homelab/","text":"Homelab","title":"Homelab"},{"location":"architecture/homelab/#homelab","text":"","title":"Homelab"},{"location":"architecture/linux/","text":"Linux apt-get update && apt-get upgrade yum update Configuration No password configured: sudo passwd hostname -f dpkg-reconfigure tzdata / timedatectl set-timezone fdisk System Diagnostics free -h top / htop df -h vmstat ps -ef pgrep / pidof File System Management ln -s cp -R mkdir -p touch mv rm scp tree Filepaths Filesystem Hierarchy Standard Linux Filesystem Tree Overview Daily Driver repos: ~/repos/github/ /home home sweet home, this is the place for users' home directories. Application Development /opt stores additional software not handled by the package manager. /tmp is a place for temporary files used by applications /var is dedicated to variable data, such as logs, databases, websites, and temporary spool (e-mail etc.) files that persist from one boot to the next. A notable directory it contains is /var/log where system log files are kept. System /bin is a place for most commonly used terminal commands, like ls, mount, rm, etc. /etc contains system-global configuration files, which affect the system's behavior for all users. /root is the superuser's home directory, not in /home/ to allow for booting the system even if /home/ is not available. /usr contains the majority of user utilities and applications, and partly replicates the root directory structure, containing for instance, among others, /usr/bin/ and /usr/lib. /boot contains files needed to start up the system, including the Linux kernel, a RAM disk image and bootloader configuration files. /dev contains all device files, which are not regular files but instead refer to various hardware devices on the system, including hard drives. /lib contains very important dynamic libraries and kernel modules /media is intended as a mount point for external devices, such as hard drives or removable media (floppies, CDs, DVDs). /mnt is also a place for mount points, but dedicated specifically to \"temporarily mounted\" devices, such as network filesyste Text Manipulation grep awk sed / jq .vimrc colorscheme evening set relativenumber set number set colorcolumn=80 set tabstop=2 shiftwidth=2 expandtab set backspace=indent,eol,start highlight ColorColum ctermbg=0 guibg=lightgrey Package Management apk apt / apt-get dpkg yum pacman Netowrking nmap : show exposed ports ping traceroute netstat o flag: show PIDs telnet mtr curl I flag: show header info only L flag: follow redirects Cron Scheduler cron expressions Install cron deb/ubuntu apt-get update && apt-get install cron pgrep cron or pidof cron systemctl start cron or service cron start on old systems crontab -e NOTE: Make sure to add a NEWLINE at the end different environment: cron passes a minimal set of environment variables to cronjobs view them: * * * * * env > /tmp/env.output default log location: /var/log/syslog redirect cron logs to specified locations e.g. 0 15 * * * /home/user/daily-backup.sh >> /var/log/daily-backup.log 2>&1 Bash alias alias ll='ls -l' cd - pwd ls -la cat / bat bash_profile vs. bashrc .bash_profile is executed at login, while .bashrc is executed on every shell instance can also use an .ini file and source it in the .bash_profile with a source ~/my.ini .bash_profile example: export PS1=\"\\[\\033[36m\\]\\u\\[\\033[m\\]@\\[\\033[32m\\]\\h:\\[\\033[33;1m\\]\\w\\[\\033[m\\]\\$ \" #PS1='[${curr_host}@`if [[ \"$mh\" = \"$PWD\" ]]; then echo \"$PWD\"; else echo \"~$ {PWD#$ih}\"; fi`] $ ' INFA_HOME=/app/infa/Informatica/10.2.0 JAVA_HOME=$INFA_HOME/java PATH=$PATH:$HOME/bin:$INFA_HOME/server/bin:$JAVA_HOME/lib PYTHONPATH=$PYTHONPATH:/app-san/infa/infa_shared/Scripts/dimpypkg LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$ORACLE_LIBS:$INFA_HOME/server/bin:$JAVA_HOME/lib:$ODBCHOME/lib:$PWX_HOME #LD_LIBRARY_PATH=$INFA_HOME/server/bin export INFA_HOME export JAVA_HOME export PATH export PYTHONPATH export LD_LIBRARY_PATH . ./my.ini my.ini example\" #!/bin/sh ih=/app-san/infa/infa_shared curr_host=`echo $HOSTNAME | cut -d'.' -f 1` alias lp='ls -alp' alias lpt='ls -alptr | tail' alias lpd='ls -alp | grep ^d' alias lps='ls -alpSr | tail' alias cache='cd $ih/Cache' alias lkp='cd $ih/LkpFiles' alias log='cd $ih/log' alias temp='cd $ih/Temp' alias param='cd $ih/ParamFiles' alias scripts='cd $ih/Scripts' alias slog='cd $ih/SessLogs' alias src='cd $ih/SrcFiles' alias tgt='cd $ih/TgtFiles' alias wlog='cd $ih/WorkflowLogs' alias infa='cd $INFA_HOME' Shell Scripting Script arguments Functions Distros Servers: Debian Daily Driver: Ubuntu & Arch Business: CentOS Notes https://www.linode.com/docs/tools-reference/linux-system-administration-basics/ .bashrc vs. .bash_profile .bash_profile is executed for login shells, while .bashrc is executed for interactive non-login shells. When you login (type username and password) via console, either sitting at the machine, or remotely via ssh: .bash_profile is executed to configure your shell before the initial command prompt. But, if you\u2019ve already logged into your machine and open a new terminal window (xterm) then .bashrc is executed before the window command prompt. .bashrc is also run when you start a new bash instance by typing /bin/bash in a terminal.","title":"Linux"},{"location":"architecture/linux/#linux","text":"apt-get update && apt-get upgrade yum update","title":"Linux"},{"location":"architecture/linux/#configuration","text":"No password configured: sudo passwd hostname -f dpkg-reconfigure tzdata / timedatectl set-timezone fdisk","title":"Configuration"},{"location":"architecture/linux/#system-diagnostics","text":"free -h top / htop df -h vmstat ps -ef pgrep / pidof","title":"System Diagnostics"},{"location":"architecture/linux/#file-system-management","text":"ln -s cp -R mkdir -p touch mv rm scp tree","title":"File System Management"},{"location":"architecture/linux/#filepaths","text":"Filesystem Hierarchy Standard Linux Filesystem Tree Overview","title":"Filepaths"},{"location":"architecture/linux/#daily-driver","text":"repos: ~/repos/github/ /home home sweet home, this is the place for users' home directories.","title":"Daily Driver"},{"location":"architecture/linux/#application-development","text":"/opt stores additional software not handled by the package manager. /tmp is a place for temporary files used by applications /var is dedicated to variable data, such as logs, databases, websites, and temporary spool (e-mail etc.) files that persist from one boot to the next. A notable directory it contains is /var/log where system log files are kept.","title":"Application Development"},{"location":"architecture/linux/#system","text":"/bin is a place for most commonly used terminal commands, like ls, mount, rm, etc. /etc contains system-global configuration files, which affect the system's behavior for all users. /root is the superuser's home directory, not in /home/ to allow for booting the system even if /home/ is not available. /usr contains the majority of user utilities and applications, and partly replicates the root directory structure, containing for instance, among others, /usr/bin/ and /usr/lib. /boot contains files needed to start up the system, including the Linux kernel, a RAM disk image and bootloader configuration files. /dev contains all device files, which are not regular files but instead refer to various hardware devices on the system, including hard drives. /lib contains very important dynamic libraries and kernel modules /media is intended as a mount point for external devices, such as hard drives or removable media (floppies, CDs, DVDs). /mnt is also a place for mount points, but dedicated specifically to \"temporarily mounted\" devices, such as network filesyste","title":"System"},{"location":"architecture/linux/#text-manipulation","text":"grep awk sed / jq .vimrc colorscheme evening set relativenumber set number set colorcolumn=80 set tabstop=2 shiftwidth=2 expandtab set backspace=indent,eol,start highlight ColorColum ctermbg=0 guibg=lightgrey","title":"Text Manipulation"},{"location":"architecture/linux/#package-management","text":"","title":"Package Management"},{"location":"architecture/linux/#apk","text":"","title":"apk"},{"location":"architecture/linux/#apt-apt-get","text":"","title":"apt / apt-get"},{"location":"architecture/linux/#dpkg","text":"","title":"dpkg"},{"location":"architecture/linux/#yum","text":"","title":"yum"},{"location":"architecture/linux/#pacman","text":"","title":"pacman"},{"location":"architecture/linux/#netowrking","text":"nmap : show exposed ports ping traceroute netstat o flag: show PIDs telnet mtr curl I flag: show header info only L flag: follow redirects","title":"Netowrking"},{"location":"architecture/linux/#cron-scheduler","text":"cron expressions Install cron deb/ubuntu apt-get update && apt-get install cron pgrep cron or pidof cron systemctl start cron or service cron start on old systems crontab -e NOTE: Make sure to add a NEWLINE at the end different environment: cron passes a minimal set of environment variables to cronjobs view them: * * * * * env > /tmp/env.output default log location: /var/log/syslog redirect cron logs to specified locations e.g. 0 15 * * * /home/user/daily-backup.sh >> /var/log/daily-backup.log 2>&1","title":"Cron Scheduler"},{"location":"architecture/linux/#bash","text":"alias alias ll='ls -l' cd - pwd ls -la cat / bat","title":"Bash"},{"location":"architecture/linux/#bash_profile-vs-bashrc","text":".bash_profile is executed at login, while .bashrc is executed on every shell instance can also use an .ini file and source it in the .bash_profile with a source ~/my.ini .bash_profile example: export PS1=\"\\[\\033[36m\\]\\u\\[\\033[m\\]@\\[\\033[32m\\]\\h:\\[\\033[33;1m\\]\\w\\[\\033[m\\]\\$ \" #PS1='[${curr_host}@`if [[ \"$mh\" = \"$PWD\" ]]; then echo \"$PWD\"; else echo \"~$ {PWD#$ih}\"; fi`] $ ' INFA_HOME=/app/infa/Informatica/10.2.0 JAVA_HOME=$INFA_HOME/java PATH=$PATH:$HOME/bin:$INFA_HOME/server/bin:$JAVA_HOME/lib PYTHONPATH=$PYTHONPATH:/app-san/infa/infa_shared/Scripts/dimpypkg LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$ORACLE_LIBS:$INFA_HOME/server/bin:$JAVA_HOME/lib:$ODBCHOME/lib:$PWX_HOME #LD_LIBRARY_PATH=$INFA_HOME/server/bin export INFA_HOME export JAVA_HOME export PATH export PYTHONPATH export LD_LIBRARY_PATH . ./my.ini my.ini example\" #!/bin/sh ih=/app-san/infa/infa_shared curr_host=`echo $HOSTNAME | cut -d'.' -f 1` alias lp='ls -alp' alias lpt='ls -alptr | tail' alias lpd='ls -alp | grep ^d' alias lps='ls -alpSr | tail' alias cache='cd $ih/Cache' alias lkp='cd $ih/LkpFiles' alias log='cd $ih/log' alias temp='cd $ih/Temp' alias param='cd $ih/ParamFiles' alias scripts='cd $ih/Scripts' alias slog='cd $ih/SessLogs' alias src='cd $ih/SrcFiles' alias tgt='cd $ih/TgtFiles' alias wlog='cd $ih/WorkflowLogs' alias infa='cd $INFA_HOME'","title":"bash_profile vs. bashrc"},{"location":"architecture/linux/#shell-scripting","text":"","title":"Shell Scripting"},{"location":"architecture/linux/#script-arguments","text":"","title":"Script arguments"},{"location":"architecture/linux/#functions","text":"","title":"Functions"},{"location":"architecture/linux/#distros","text":"Servers: Debian Daily Driver: Ubuntu & Arch Business: CentOS","title":"Distros"},{"location":"architecture/linux/#notes","text":"https://www.linode.com/docs/tools-reference/linux-system-administration-basics/","title":"Notes"},{"location":"architecture/linux/#bashrc-vs-bash_profile","text":".bash_profile is executed for login shells, while .bashrc is executed for interactive non-login shells. When you login (type username and password) via console, either sitting at the machine, or remotely via ssh: .bash_profile is executed to configure your shell before the initial command prompt. But, if you\u2019ve already logged into your machine and open a new terminal window (xterm) then .bashrc is executed before the window command prompt. .bashrc is also run when you start a new bash instance by typing /bin/bash in a terminal.","title":".bashrc vs. .bash_profile"},{"location":"architecture/monitoring/","text":"Monitoring The 3 pillars of Observability: Metrics Answers: \"How has the system performance changed throughout time?\" numbers related to events that happened in a specifc time range example technologies: Metricbeats or Prometheus with Grafana Logs Answers: \"What is happening in the system?\" gives insights into a single events occuring in the system example technologies: ELK stack Tracing Answers: \"How are the system components interacting with each other?\" shows how long a specific request spent in the different system components example technologies: Jaeger or Zipkin Live Metrics Stream app system platform Grafana Centeralized Logging ELK Stack Open Distro Apache 2.0 license Security Alerting SQL Performance Annalyzer (perftop CLI tool) Kubernetes Operator: Elastic Cloud on Kubernetes (ECK) uses the free tier Elasticsearch License Configure Elasticsearch stack in containers docker-compose quickstart repo Increase heap size / allocate more memory for ES in JVM options: Update environment variables in docker-compose file - \"ES_JAVA_OPTS=-Xms4g -Xmx4g\" Modify file descriptors: File descriptors: \"Make sure to increase the limit on the number of open files descriptors for the user running Elasticsearch to 65,536 or higher.\" run everything as root user ulimit -Hn : 4096 -> 1024000 vim /usr/lib/sysctl.d/elasticsearch.conf vim /etc/security/limits/conf restorecon /etc/security/limits.conf reboot system /usr/lib/sysctl.d/elasticsearch.conf fs.file-max = 512000 vm.max_map_count = 524288 vm.swappiness = 1 /etc/security/limits/conf * soft nofile 1024000 * hard nofile 1024000 * soft memlock unlimited * hard memlock unlimited elastic soft nofile 1024000 elastic hard nofile 1024000 elastic soft memlock unlimited elastic hard memlock unlimited root soft nofile 1024000 root hard nofile 1024000 root soft memlock unlimited root hard memlock unlimited Logstash Install additional plugins: /bin/logstash install logstash-input-jmx Run pipeline: /bin/logstash.bat -f logstash-pipeline.conf --config.reload.automatic Test pipeline configuration: /bin/logstash.bat -f logstash-pipeline.conf --config.test_and_exit Run as service on a windows server: use NSSM Ingest log file Log sample 2020-04-21 20:52:15,173 WARN | ActiveMQ Task-1 () | [NetworkConnector:?] Could not start network... 2020-04-21 20:52:15,360 INFO | main () | [AbstractService:?] ...finished service destruction 2020-04-21 20:52:15,095 INFO | pool-9-thread-1 () | [EventConsumer:?] Interrupted while waiting for event. application-pipeline.conf input { file { path => \"E:/My_Application/Config/application.log\" type => \"log\" start_position => \"beginning\" sincedb_path => \"NUL\" } } filter { grok { match => { \"message\" => \"%{DATA:date} %{DATA:time} %{DATA:log_level} \\| %{DATA:thread} \\| \\[%{DATA:service}\\] %{GREEDYDATA:log_msg}\" } add_field => { \"timestamp\" => \"%{date} %{time}\"} remove_field => [ \"date\", \"time\"] } if \"_grokparsefailure\" in [tags] { drop { } } date { match => [ \"timestamp\", \"YYYY-MM-dd HH:mm:ss,SSS\"] } mutate { add_field => { \"application_name\" => \"MY_APPLICATION\"} remove_field => [ \"timestamp\" ] } } output { elasticsearch { hosts => \"0.0.0.0:9200\" user => \"user\" password => \"passwd\" index => \"test-index-%{+YYYY.MM.dd}\" } } Ingest jmx data Requires a jmx.conf file defined in path in jmx input plugin input { jmx { path => \"C:/elk/logstash/jmx\" polling_frequency => 15 type => \"jmx\" } } jmx.conf { \"host\": \"localhost\", \"port\": 9000, \"queries\": [ { \"object_name\" : \"java.lang:type=Memory\", \"object_alias\" : \"Memory\" }, { \"object_name\" : \"java.lang:type=Runtime\", \"attributes\" : [ \"uptime\", \"StartTime\" ], \"object_alias\" : \"Runtime\" }, { \"object_name\" : \"java.lang:type=GarbageCollector,name=*\", \"attributes\" : [ \"CollectionCount\", \"CollectionTime\" ], \"object_alias\" : \"${type}.${name}\" }, { \"object_name\" : \"java.nio:type=BufferPool,name=*\", \"object_alias\" : \"${type}.${name}\" }] } Beats Filebeat Example: send docker container logs to logstash filebeat.inputs: - type: docker containers.path: /usr/share/dockerlogs/data containers.ids: - '1fb97e7e06677bc9c2aa2cee28b9a2e489a7c8ead62829d10a69e60e38d27310' #ignore_decoding_error: true json.message_key: log json.keys_under_root: true #exclude_lines: ['^\\n'] multiline.pattern: '^\\s' multiline.match: after processors: - add_docker_metadata: host: \"unix:///var/run/docker.sock\" - timestamp: field: time layouts: - '2020-02-12T06:59:01.944972082Z' filebeat.config.modules: path: ${path.config}/modules.d/*.yml reload.enabled: false output.logstash: hosts: [\"172.18.1.200:5000\"] logging.to_files: true logging.to_syslog: false Kibana JMX Example Line chart visualization Y-axis: Average X-axis: Date Histogram Split series Filters aggregation bucket: metric_path:jvm.Memory.HeapMemoryUsage.used Health Checks Correlation Token","title":"Monitoring"},{"location":"architecture/monitoring/#monitoring","text":"The 3 pillars of Observability: Metrics Answers: \"How has the system performance changed throughout time?\" numbers related to events that happened in a specifc time range example technologies: Metricbeats or Prometheus with Grafana Logs Answers: \"What is happening in the system?\" gives insights into a single events occuring in the system example technologies: ELK stack Tracing Answers: \"How are the system components interacting with each other?\" shows how long a specific request spent in the different system components example technologies: Jaeger or Zipkin","title":"Monitoring"},{"location":"architecture/monitoring/#live-metrics-stream","text":"app system platform","title":"Live Metrics Stream"},{"location":"architecture/monitoring/#grafana","text":"","title":"Grafana"},{"location":"architecture/monitoring/#centeralized-logging","text":"","title":"Centeralized Logging"},{"location":"architecture/monitoring/#elk-stack","text":"Open Distro Apache 2.0 license Security Alerting SQL Performance Annalyzer (perftop CLI tool) Kubernetes Operator: Elastic Cloud on Kubernetes (ECK) uses the free tier Elasticsearch License","title":"ELK Stack"},{"location":"architecture/monitoring/#configure-elasticsearch-stack-in-containers","text":"docker-compose quickstart repo","title":"Configure Elasticsearch stack in containers"},{"location":"architecture/monitoring/#increase-heap-size-allocate-more-memory-for-es-in-jvm-options","text":"Update environment variables in docker-compose file - \"ES_JAVA_OPTS=-Xms4g -Xmx4g\"","title":"Increase heap size / allocate more memory for ES in JVM options:"},{"location":"architecture/monitoring/#modify-file-descriptors","text":"File descriptors: \"Make sure to increase the limit on the number of open files descriptors for the user running Elasticsearch to 65,536 or higher.\" run everything as root user ulimit -Hn : 4096 -> 1024000 vim /usr/lib/sysctl.d/elasticsearch.conf vim /etc/security/limits/conf restorecon /etc/security/limits.conf reboot system /usr/lib/sysctl.d/elasticsearch.conf fs.file-max = 512000 vm.max_map_count = 524288 vm.swappiness = 1 /etc/security/limits/conf * soft nofile 1024000 * hard nofile 1024000 * soft memlock unlimited * hard memlock unlimited elastic soft nofile 1024000 elastic hard nofile 1024000 elastic soft memlock unlimited elastic hard memlock unlimited root soft nofile 1024000 root hard nofile 1024000 root soft memlock unlimited root hard memlock unlimited","title":"Modify file descriptors:"},{"location":"architecture/monitoring/#logstash","text":"Install additional plugins: /bin/logstash install logstash-input-jmx Run pipeline: /bin/logstash.bat -f logstash-pipeline.conf --config.reload.automatic Test pipeline configuration: /bin/logstash.bat -f logstash-pipeline.conf --config.test_and_exit Run as service on a windows server: use NSSM","title":"Logstash"},{"location":"architecture/monitoring/#ingest-log-file","text":"Log sample 2020-04-21 20:52:15,173 WARN | ActiveMQ Task-1 () | [NetworkConnector:?] Could not start network... 2020-04-21 20:52:15,360 INFO | main () | [AbstractService:?] ...finished service destruction 2020-04-21 20:52:15,095 INFO | pool-9-thread-1 () | [EventConsumer:?] Interrupted while waiting for event. application-pipeline.conf input { file { path => \"E:/My_Application/Config/application.log\" type => \"log\" start_position => \"beginning\" sincedb_path => \"NUL\" } } filter { grok { match => { \"message\" => \"%{DATA:date} %{DATA:time} %{DATA:log_level} \\| %{DATA:thread} \\| \\[%{DATA:service}\\] %{GREEDYDATA:log_msg}\" } add_field => { \"timestamp\" => \"%{date} %{time}\"} remove_field => [ \"date\", \"time\"] } if \"_grokparsefailure\" in [tags] { drop { } } date { match => [ \"timestamp\", \"YYYY-MM-dd HH:mm:ss,SSS\"] } mutate { add_field => { \"application_name\" => \"MY_APPLICATION\"} remove_field => [ \"timestamp\" ] } } output { elasticsearch { hosts => \"0.0.0.0:9200\" user => \"user\" password => \"passwd\" index => \"test-index-%{+YYYY.MM.dd}\" } }","title":"Ingest log file"},{"location":"architecture/monitoring/#ingest-jmx-data","text":"Requires a jmx.conf file defined in path in jmx input plugin input { jmx { path => \"C:/elk/logstash/jmx\" polling_frequency => 15 type => \"jmx\" } } jmx.conf { \"host\": \"localhost\", \"port\": 9000, \"queries\": [ { \"object_name\" : \"java.lang:type=Memory\", \"object_alias\" : \"Memory\" }, { \"object_name\" : \"java.lang:type=Runtime\", \"attributes\" : [ \"uptime\", \"StartTime\" ], \"object_alias\" : \"Runtime\" }, { \"object_name\" : \"java.lang:type=GarbageCollector,name=*\", \"attributes\" : [ \"CollectionCount\", \"CollectionTime\" ], \"object_alias\" : \"${type}.${name}\" }, { \"object_name\" : \"java.nio:type=BufferPool,name=*\", \"object_alias\" : \"${type}.${name}\" }] }","title":"Ingest jmx data"},{"location":"architecture/monitoring/#beats","text":"","title":"Beats"},{"location":"architecture/monitoring/#filebeat","text":"Example: send docker container logs to logstash filebeat.inputs: - type: docker containers.path: /usr/share/dockerlogs/data containers.ids: - '1fb97e7e06677bc9c2aa2cee28b9a2e489a7c8ead62829d10a69e60e38d27310' #ignore_decoding_error: true json.message_key: log json.keys_under_root: true #exclude_lines: ['^\\n'] multiline.pattern: '^\\s' multiline.match: after processors: - add_docker_metadata: host: \"unix:///var/run/docker.sock\" - timestamp: field: time layouts: - '2020-02-12T06:59:01.944972082Z' filebeat.config.modules: path: ${path.config}/modules.d/*.yml reload.enabled: false output.logstash: hosts: [\"172.18.1.200:5000\"] logging.to_files: true logging.to_syslog: false","title":"Filebeat"},{"location":"architecture/monitoring/#kibana","text":"","title":"Kibana"},{"location":"architecture/monitoring/#jmx-example","text":"Line chart visualization Y-axis: Average X-axis: Date Histogram Split series Filters aggregation bucket: metric_path:jvm.Memory.HeapMemoryUsage.used","title":"JMX Example"},{"location":"architecture/monitoring/#health-checks","text":"","title":"Health Checks"},{"location":"architecture/monitoring/#correlation-token","text":"","title":"Correlation Token"},{"location":"architecture/security/","text":"Security RSA & GPG ssh-keygen -t rsa -C user@email user in comment should match the user to login to remote system Securing a Web Server SSL Test Debian Firewall Secrets sealed secrets: encrypted secrets stored in repositories Vault an alternative to storing secrets as encrypted text in repositories that offers additional features beyond secure secret storage: dynamic secrets: generating secrets for on-demand access data encryption: encrypt and decrypt data in databases without Vault actually storing it leasing and renewl: associates a lease with all secrets that provides the ability to renew the leases or revoke the secret if expired revocation: revoke single secrets or a tree of secrets such as all secrets read by a specific user or of a particular type (useful for key rolling and locking down systems) monitor access Setup Install Manual: download vault binary and adding it to PATH Mac (Homebrew): brew install vault Windows (Chocolately): choco install vault Start a DEV server vault server -dev NOTE: Never run the DEV server in production! Save configuration Save the generated unseal key into a new file called \"unseal.key\" Add environment variables: export VAULT_ADDR=\"http://127.0.0.1:8200\" export VAULT_DEV_ROOT_TOKEN_ID=\"s.TolAAwSmTYlBmE0E1M4M0ADG\" Check server satuts vault status Add secrets vault kv put secret/hello foo=world excited=yes Get secrets vault kv get secret/hello vault kv get -field=excited secret/hello vault kv get -format=json secret/hello | jq -r .data.data.excited Delete secrets vault kv delete secret/hello Kali Linux","title":"Security"},{"location":"architecture/security/#security","text":"","title":"Security"},{"location":"architecture/security/#rsa-gpg","text":"ssh-keygen -t rsa -C user@email user in comment should match the user to login to remote system","title":"RSA &amp; GPG"},{"location":"architecture/security/#securing-a-web-server","text":"SSL Test","title":"Securing a Web Server"},{"location":"architecture/security/#debian","text":"","title":"Debian"},{"location":"architecture/security/#firewall","text":"","title":"Firewall"},{"location":"architecture/security/#secrets","text":"sealed secrets: encrypted secrets stored in repositories","title":"Secrets"},{"location":"architecture/security/#vault","text":"an alternative to storing secrets as encrypted text in repositories that offers additional features beyond secure secret storage: dynamic secrets: generating secrets for on-demand access data encryption: encrypt and decrypt data in databases without Vault actually storing it leasing and renewl: associates a lease with all secrets that provides the ability to renew the leases or revoke the secret if expired revocation: revoke single secrets or a tree of secrets such as all secrets read by a specific user or of a particular type (useful for key rolling and locking down systems) monitor access","title":"Vault"},{"location":"architecture/security/#setup","text":"Install Manual: download vault binary and adding it to PATH Mac (Homebrew): brew install vault Windows (Chocolately): choco install vault Start a DEV server vault server -dev NOTE: Never run the DEV server in production! Save configuration Save the generated unseal key into a new file called \"unseal.key\" Add environment variables: export VAULT_ADDR=\"http://127.0.0.1:8200\" export VAULT_DEV_ROOT_TOKEN_ID=\"s.TolAAwSmTYlBmE0E1M4M0ADG\" Check server satuts vault status","title":"Setup"},{"location":"architecture/security/#add-secrets","text":"vault kv put secret/hello foo=world excited=yes","title":"Add secrets"},{"location":"architecture/security/#get-secrets","text":"vault kv get secret/hello vault kv get -field=excited secret/hello vault kv get -format=json secret/hello | jq -r .data.data.excited","title":"Get secrets"},{"location":"architecture/security/#delete-secrets","text":"vault kv delete secret/hello","title":"Delete secrets"},{"location":"architecture/security/#kali-linux","text":"","title":"Kali Linux"},{"location":"architecture/virtualization/","text":"Virtualization Vagrant vagrant init ubuntu/bionic64 vagrant box add centos/8 vagrant status vagrant up vagrant up <hostname> vagrant suspend stores state on RAM vagrant halt stores state on Disk vagrant destroy -f removes stored state vagrant ssh <hostname> vagrant ssh-config Boxes https://app.vagrantup.com/boxes/search Synced Folders \"By default, Vagrant shares your project directory (remember, that is the one with the Vagrantfile) to the /vagrant directory in your guest machine.\" Vagrantfile Vagrant.configure(\"3\") do |config| # create mongo VM config.vm.define \"mongo\" do |mongo| mongo.vm.box = \"bento/ubuntu-16.04\" mongo.vm.provider \"virtualbox\" do |vb| vb.memory = \"512\" end mongo.vm.network \"private_netowrk\", ip: \"10.8.0.20\" mongo.vm.provision \"file\", source: \"files/mongo.conf\", destination: \"~/mongod/conf\" # try provisioning with ansible instead of shell script mongo.vm.provision \"shell\", path: \"provisioners/install-mongo.sh\" end # create node VM config.vm.define \"node\" do |node| node.vm.box = \"bento/ubuntu-16.04\" node.vm.network \"fowarded_port\", guest: 3000, host:8000 node.vm.provider \"virtualbox\" do |vb| vb.memory = \"512\" end node.vm.network \"private_network\", ip: \"10.8.0.21\" # try provisioning with ansible instead of shell script node.vm.provision \"shell\", path: \"privisioners/install-node.sh\" end # creates 3 ubuntu VMs (5..7).each do |i| config.vm.define \"ubuntu#{i}\" do |ubuntu| ubuntu.vm.box = \"ubuntu/bionic64\" ubuntu.vm.network \"private_network\", ip: \"10.8.0.#{i}\" end end config.vm.provider \"virtualbox\" do |v| v.memory = 8192 v.cpus = 4 end end Containerization Docker docker exec docker events Docker Compose Container Orchestration Kubernetes AKS Concepts Core Concepts Nodes : a single machine, cluster resource to store data, run jobs, maintain workloads and create network routes Node Pools : Control Plane : set of containers (can be multiple master nodes) that manages the cluster: contains controller manager, API server, etcd, scheduler, CoreDNS, etc. kubelet : kubernetes agent running on worker nodes kube-proxy : Container runtime : Kubetctl : CLI to configure kubernetes and deploy & manage applications Pods : can be comprised of multiple containers deployed on a single node, receives 1 IP Namespaces : Labels & Annotations : allows to filter on related resources in the cluster Service : network endpoint to connect to a pod, 4 types: ClusterIP: only available within the cluster NodePort: high port allocated on each node to be reachable outside of the cluster LoadBalancer: controls an external load balancer endpoint external to the cluster ExternalName: adds CNAME DNS record to CoreDNS to allow pods to use a specified DNS name to reach a service outside of the cluster Service Discovery : custom DNS server that all Pods use to resolve names of other services, IPs and ports ReplicaSets : used to scale pods to a desired state and hold the current status for the system DaemonSets : used to deploy a single instance of an application on each node (e.g. log/metric exporter) StatefulSets : used to deploy applications that require the use of the same node and data persistence Jobs/CronJobs : used to deploy a container to complete a specific workload and be destory on successful completion ConfigMaps & Secrets : key-value environment variables to be passed to running workloads to determine different runtime behaviors, secrets are the envrypted version Deployments : a set of metadata to describe the requirements of a running workload Scheduler : ensures that if pods or nodes encounter problems, additional pods are ran on healthy nodes Storage : an abstraction layer to manage data persistency to outlast the lifetime of a pod CRDs : custom resource definitions Minikube Install minikube Install kubectl Create clutser minikube start (defaults to 2GB RAM, 2 CPUs, 20GB disk space) Bigger applications: minikube start --memory=8192 --cpu=4 --disk-size=50g Get the cluster IP: minikube ip Profiles minikube start -p <profile> --memory=8192 --cpu=4 --disk-size=50g Invoke a kubernetes service minikube service <service_name> Kubectl Cheatsheet kubectl version kubectl apply -f filename.yml : Watch pods: kubectl get pods -w kubectl get pods --all-namespaces kubectl get service kubectl get all Docs: kubectl explain services --recursive Filter: kubectl explain services.spec YAML Configuration Requirements: kind apiVersion metadata spec Example of a Service & Deployment defined in the same file kind: Service apiVersion: v1 metadata: name: app-nginx-service spec: type: NodePort ports: - port: 80 selector: app: app-nginx --- kind: Deployment apiVersion: apps/v1 metadata: name: app-nginx-deployment spec: replicas: 3 selector: matchLabels: app: app-nginx template: metadata: labels: app: app-nginx mycustomlabel: \"true\" spec: containers: - name: nginx image: nginx:stable Testing Manual deploy: docker run equivalent kubectl run my-app --image app:latest Cleanup/ docker rm equivalent: kubectl delete deployment my-app Manual scale: kubectl scale deploy/my-app --replicas 2 ( kubectl scale deployment my-app also works) Manually create service: kubectl expose Logs of single pod: kubectl logs deploy/my-app --folow --tail 1 Logs of up to 5 pods: kubectl logs deploy/my-app -l run=my-app Inspect a specific pod: kubectl descibe pod/my-app-74dh8h72d7-2j6kv kubectl create deployment sample --image my-app:latest --dry-run -o yaml Helm Logging kubectl -n <namespace> logs -f deployment/<app-name> --all-containers=true --since=10m Stern stern -n <namespace> <app-name> -t --since 10m Kail kail -n <namespace> -d <deployment> --label size=large kail -n <namespace> --svc <service> --ignore size=large --since=2h","title":"Virtualization"},{"location":"architecture/virtualization/#virtualization","text":"","title":"Virtualization"},{"location":"architecture/virtualization/#vagrant","text":"vagrant init ubuntu/bionic64 vagrant box add centos/8 vagrant status vagrant up vagrant up <hostname> vagrant suspend stores state on RAM vagrant halt stores state on Disk vagrant destroy -f removes stored state vagrant ssh <hostname> vagrant ssh-config","title":"Vagrant"},{"location":"architecture/virtualization/#boxes","text":"https://app.vagrantup.com/boxes/search","title":"Boxes"},{"location":"architecture/virtualization/#synced-folders","text":"\"By default, Vagrant shares your project directory (remember, that is the one with the Vagrantfile) to the /vagrant directory in your guest machine.\"","title":"Synced Folders"},{"location":"architecture/virtualization/#vagrantfile","text":"Vagrant.configure(\"3\") do |config| # create mongo VM config.vm.define \"mongo\" do |mongo| mongo.vm.box = \"bento/ubuntu-16.04\" mongo.vm.provider \"virtualbox\" do |vb| vb.memory = \"512\" end mongo.vm.network \"private_netowrk\", ip: \"10.8.0.20\" mongo.vm.provision \"file\", source: \"files/mongo.conf\", destination: \"~/mongod/conf\" # try provisioning with ansible instead of shell script mongo.vm.provision \"shell\", path: \"provisioners/install-mongo.sh\" end # create node VM config.vm.define \"node\" do |node| node.vm.box = \"bento/ubuntu-16.04\" node.vm.network \"fowarded_port\", guest: 3000, host:8000 node.vm.provider \"virtualbox\" do |vb| vb.memory = \"512\" end node.vm.network \"private_network\", ip: \"10.8.0.21\" # try provisioning with ansible instead of shell script node.vm.provision \"shell\", path: \"privisioners/install-node.sh\" end # creates 3 ubuntu VMs (5..7).each do |i| config.vm.define \"ubuntu#{i}\" do |ubuntu| ubuntu.vm.box = \"ubuntu/bionic64\" ubuntu.vm.network \"private_network\", ip: \"10.8.0.#{i}\" end end config.vm.provider \"virtualbox\" do |v| v.memory = 8192 v.cpus = 4 end end","title":"Vagrantfile"},{"location":"architecture/virtualization/#containerization","text":"","title":"Containerization"},{"location":"architecture/virtualization/#docker","text":"docker exec docker events","title":"Docker"},{"location":"architecture/virtualization/#docker-compose","text":"","title":"Docker Compose"},{"location":"architecture/virtualization/#container-orchestration","text":"","title":"Container Orchestration"},{"location":"architecture/virtualization/#kubernetes","text":"AKS Concepts","title":"Kubernetes"},{"location":"architecture/virtualization/#core-concepts","text":"Nodes : a single machine, cluster resource to store data, run jobs, maintain workloads and create network routes Node Pools : Control Plane : set of containers (can be multiple master nodes) that manages the cluster: contains controller manager, API server, etcd, scheduler, CoreDNS, etc. kubelet : kubernetes agent running on worker nodes kube-proxy : Container runtime : Kubetctl : CLI to configure kubernetes and deploy & manage applications Pods : can be comprised of multiple containers deployed on a single node, receives 1 IP Namespaces : Labels & Annotations : allows to filter on related resources in the cluster Service : network endpoint to connect to a pod, 4 types: ClusterIP: only available within the cluster NodePort: high port allocated on each node to be reachable outside of the cluster LoadBalancer: controls an external load balancer endpoint external to the cluster ExternalName: adds CNAME DNS record to CoreDNS to allow pods to use a specified DNS name to reach a service outside of the cluster Service Discovery : custom DNS server that all Pods use to resolve names of other services, IPs and ports ReplicaSets : used to scale pods to a desired state and hold the current status for the system DaemonSets : used to deploy a single instance of an application on each node (e.g. log/metric exporter) StatefulSets : used to deploy applications that require the use of the same node and data persistence Jobs/CronJobs : used to deploy a container to complete a specific workload and be destory on successful completion ConfigMaps & Secrets : key-value environment variables to be passed to running workloads to determine different runtime behaviors, secrets are the envrypted version Deployments : a set of metadata to describe the requirements of a running workload Scheduler : ensures that if pods or nodes encounter problems, additional pods are ran on healthy nodes Storage : an abstraction layer to manage data persistency to outlast the lifetime of a pod CRDs : custom resource definitions","title":"Core Concepts"},{"location":"architecture/virtualization/#minikube","text":"Install minikube Install kubectl","title":"Minikube"},{"location":"architecture/virtualization/#create-clutser","text":"minikube start (defaults to 2GB RAM, 2 CPUs, 20GB disk space) Bigger applications: minikube start --memory=8192 --cpu=4 --disk-size=50g Get the cluster IP: minikube ip","title":"Create clutser"},{"location":"architecture/virtualization/#profiles","text":"minikube start -p <profile> --memory=8192 --cpu=4 --disk-size=50g","title":"Profiles"},{"location":"architecture/virtualization/#invoke-a-kubernetes-service","text":"minikube service <service_name>","title":"Invoke a kubernetes service"},{"location":"architecture/virtualization/#kubectl","text":"Cheatsheet kubectl version kubectl apply -f filename.yml : Watch pods: kubectl get pods -w kubectl get pods --all-namespaces kubectl get service kubectl get all Docs: kubectl explain services --recursive Filter: kubectl explain services.spec","title":"Kubectl"},{"location":"architecture/virtualization/#yaml-configuration","text":"Requirements: kind apiVersion metadata spec","title":"YAML Configuration"},{"location":"architecture/virtualization/#example-of-a-service-deployment-defined-in-the-same-file","text":"kind: Service apiVersion: v1 metadata: name: app-nginx-service spec: type: NodePort ports: - port: 80 selector: app: app-nginx --- kind: Deployment apiVersion: apps/v1 metadata: name: app-nginx-deployment spec: replicas: 3 selector: matchLabels: app: app-nginx template: metadata: labels: app: app-nginx mycustomlabel: \"true\" spec: containers: - name: nginx image: nginx:stable","title":"Example of a Service &amp; Deployment defined in the same file"},{"location":"architecture/virtualization/#testing","text":"Manual deploy: docker run equivalent kubectl run my-app --image app:latest Cleanup/ docker rm equivalent: kubectl delete deployment my-app Manual scale: kubectl scale deploy/my-app --replicas 2 ( kubectl scale deployment my-app also works) Manually create service: kubectl expose Logs of single pod: kubectl logs deploy/my-app --folow --tail 1 Logs of up to 5 pods: kubectl logs deploy/my-app -l run=my-app Inspect a specific pod: kubectl descibe pod/my-app-74dh8h72d7-2j6kv kubectl create deployment sample --image my-app:latest --dry-run -o yaml","title":"Testing"},{"location":"architecture/virtualization/#helm","text":"","title":"Helm"},{"location":"architecture/virtualization/#logging","text":"kubectl -n <namespace> logs -f deployment/<app-name> --all-containers=true --since=10m","title":"Logging"},{"location":"architecture/virtualization/#stern","text":"stern -n <namespace> <app-name> -t --since 10m","title":"Stern"},{"location":"architecture/virtualization/#kail","text":"kail -n <namespace> -d <deployment> --label size=large kail -n <namespace> --svc <service> --ignore size=large --since=2h","title":"Kail"},{"location":"data/analysis/","text":"Data Analysis NumPy Pandas Notes Import pandas as pd df = pd.read_csv(\u201cfile.csv\u201d) Outputs: (rows, columns) df.shape Outputs: first/last 5 rows & header df.head(5) or df.tail Check for null values df.isnull().values.any() Data fram correlation function df.corr() Delete column from data frame del df(\u2018column_name\u2019) Mold data (change True/False values to 1 or 0) column_map = { True: 1, False: 0 } df[\u2018column_name\u2019] = df[\u2018column_name\u2019].map(column_map) Creating column ratios (True/False ratio) num_true = len(df.loc[df[\u2018column_name\u2019] == True]) num_false = len(df.loc[df[\u2018column_name\u2019] == False]) Count: len(dataframe) Count group by distinct: df[COLUMN.value_counts() Count null values df['COLUMN'].isna().sum() len(df) - df['COLUMN'].count() Count distict values, use nunique: df['hID'].nunique() Count only non-null values, use count: df['hID'].count() Count total values including null values, use size attribute: df['hID'].size https://davidhamann.de/2017/06/26/pandas-select-elements-by-string/ https://proview-demo.nonprod.caqh.org/DirectAssure/api/POPracticeLocation/MatchReport/6053_20190606_144049 Visualization","title":"Data Analysis"},{"location":"data/analysis/#data-analysis","text":"","title":"Data Analysis"},{"location":"data/analysis/#numpy","text":"","title":"NumPy"},{"location":"data/analysis/#pandas","text":"","title":"Pandas"},{"location":"data/analysis/#notes","text":"Import pandas as pd df = pd.read_csv(\u201cfile.csv\u201d) Outputs: (rows, columns) df.shape Outputs: first/last 5 rows & header df.head(5) or df.tail Check for null values df.isnull().values.any() Data fram correlation function df.corr() Delete column from data frame del df(\u2018column_name\u2019) Mold data (change True/False values to 1 or 0) column_map = { True: 1, False: 0 } df[\u2018column_name\u2019] = df[\u2018column_name\u2019].map(column_map) Creating column ratios (True/False ratio) num_true = len(df.loc[df[\u2018column_name\u2019] == True]) num_false = len(df.loc[df[\u2018column_name\u2019] == False]) Count: len(dataframe) Count group by distinct: df[COLUMN.value_counts() Count null values df['COLUMN'].isna().sum() len(df) - df['COLUMN'].count() Count distict values, use nunique: df['hID'].nunique() Count only non-null values, use count: df['hID'].count() Count total values including null values, use size attribute: df['hID'].size https://davidhamann.de/2017/06/26/pandas-select-elements-by-string/ https://proview-demo.nonprod.caqh.org/DirectAssure/api/POPracticeLocation/MatchReport/6053_20190606_144049","title":"Notes"},{"location":"data/analysis/#visualization","text":"","title":"Visualization"},{"location":"data/big_data/","text":"Big Data Cloudera Impala hive databases not showing up? INVALIDATE METADATA; Is impala returning 0 results for a parquet table it should be able to read? Refresh it. REFRESH db_name.table search for string SELECT COUNT(*) FROM cpsc_db.cpsc_premium WHERE INSTR(organization_name, \"SNP\") != 0; Hive built-in types https://support.treasuredata.com/hc/en-us/articles/360001450728-Hive-Built-in-Operators regex replace (remove non-ASCII chars) SELECT regexp_replace(org_plan_name, '\\\\P{ASCII}', '') FROM cpsc_stage_db.cpsc_contract WHERE year=2019; DESCRIBE PARTITION desc formatted dbname.tablename partition (name=value) ADD PARTITION hive (cpsc_stage_db)> alter table cpsc_contract add partition(year=2019, month=02) > location '/data/domain/cpsc/cpsc_stage_db/cpsc_contract/year=2019/month=02'; DROP PARTITION hive (cpsc_stage_db)> alter table cpsc_contract drop if exists partition(year=2019, month=02); NOTES: this removes the hdfs folder location of the partition data TRUNCATE TABLE TRUNCATE TABLE IF EXISTS $tablename SHELL SCRIPTING hive -e (query) hive -f (file) Skip reading header line when creating hive table from CSV TBLPROPERTIES ( 'skip.header.line.count'='1' ); Creating Hive table from CSV PARTITIONED BY ( year int ) ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde' WITH SERDEPROPERTIES ( \"separatorChar\"=\",\", \"quoteChar\"=\"\\\"\") STORED AS TEXTFILE Create partitioned Hive table Create HDFS dirs (/table/year=2019/month=01) hdfs dfs -put (the csvs to the hdfs dirs) ALTER TABLE with each partition and LOCATION '/hdfs/path/to/new/dirs/' Spark PySpark sql_c = SQLContext(sc) df = sql_c.read.csv('HDFS PATH') df.printSchema() df.show() df.write.csv('HDFS PATH/mycsv.csv') remove non-ascii chars re.sub(r'[^\\x00-\\x7f]',r'', 'hi \u00bb') MLib Oozie Sentry","title":"Big Data"},{"location":"data/big_data/#big-data","text":"","title":"Big Data"},{"location":"data/big_data/#cloudera","text":"","title":"Cloudera"},{"location":"data/big_data/#impala","text":"hive databases not showing up? INVALIDATE METADATA; Is impala returning 0 results for a parquet table it should be able to read? Refresh it. REFRESH db_name.table search for string SELECT COUNT(*) FROM cpsc_db.cpsc_premium WHERE INSTR(organization_name, \"SNP\") != 0;","title":"Impala"},{"location":"data/big_data/#hive","text":"built-in types https://support.treasuredata.com/hc/en-us/articles/360001450728-Hive-Built-in-Operators","title":"Hive"},{"location":"data/big_data/#regex-replace-remove-non-ascii-chars","text":"SELECT regexp_replace(org_plan_name, '\\\\P{ASCII}', '') FROM cpsc_stage_db.cpsc_contract WHERE year=2019;","title":"regex replace (remove non-ASCII chars)"},{"location":"data/big_data/#describe-partition","text":"desc formatted dbname.tablename partition (name=value)","title":"DESCRIBE PARTITION"},{"location":"data/big_data/#add-partition","text":"hive (cpsc_stage_db)> alter table cpsc_contract add partition(year=2019, month=02) > location '/data/domain/cpsc/cpsc_stage_db/cpsc_contract/year=2019/month=02';","title":"ADD PARTITION"},{"location":"data/big_data/#drop-partition","text":"hive (cpsc_stage_db)> alter table cpsc_contract drop if exists partition(year=2019, month=02); NOTES: this removes the hdfs folder location of the partition data","title":"DROP PARTITION"},{"location":"data/big_data/#truncate-table","text":"TRUNCATE TABLE IF EXISTS $tablename","title":"TRUNCATE TABLE"},{"location":"data/big_data/#shell-scripting","text":"hive -e (query) hive -f (file)","title":"SHELL SCRIPTING"},{"location":"data/big_data/#skip-reading-header-line-when-creating-hive-table-from-csv","text":"TBLPROPERTIES ( 'skip.header.line.count'='1' );","title":"Skip reading header line when creating hive table from CSV"},{"location":"data/big_data/#creating-hive-table-from-csv","text":"PARTITIONED BY ( year int ) ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde' WITH SERDEPROPERTIES ( \"separatorChar\"=\",\", \"quoteChar\"=\"\\\"\") STORED AS TEXTFILE","title":"Creating Hive table from CSV"},{"location":"data/big_data/#create-partitioned-hive-table","text":"Create HDFS dirs (/table/year=2019/month=01) hdfs dfs -put (the csvs to the hdfs dirs) ALTER TABLE with each partition and LOCATION '/hdfs/path/to/new/dirs/'","title":"Create partitioned Hive table"},{"location":"data/big_data/#spark","text":"","title":"Spark"},{"location":"data/big_data/#pyspark","text":"sql_c = SQLContext(sc) df = sql_c.read.csv('HDFS PATH') df.printSchema() df.show() df.write.csv('HDFS PATH/mycsv.csv') remove non-ascii chars re.sub(r'[^\\x00-\\x7f]',r'', 'hi \u00bb')","title":"PySpark"},{"location":"data/big_data/#mlib","text":"","title":"MLib"},{"location":"data/big_data/#oozie","text":"","title":"Oozie"},{"location":"data/big_data/#sentry","text":"","title":"Sentry"},{"location":"data/machine_learning/","text":"Machine Learning Shallow Learning Scikit-learn Deep Learning Tensor Flow","title":"Machine Learning"},{"location":"data/machine_learning/#machine-learning","text":"","title":"Machine Learning"},{"location":"data/machine_learning/#shallow-learning","text":"","title":"Shallow Learning"},{"location":"data/machine_learning/#scikit-learn","text":"","title":"Scikit-learn"},{"location":"data/machine_learning/#deep-learning","text":"","title":"Deep Learning"},{"location":"data/machine_learning/#tensor-flow","text":"","title":"Tensor Flow"},{"location":"data/pipelines/","text":"Data Pipelines","title":"Data Pipelines"},{"location":"data/pipelines/#data-pipelines","text":"","title":"Data Pipelines"},{"location":"data/storage/","text":"Data Storage Relational Databases PostgreSQL MySQL SQLite Oracle SQL Server NoSQL Databases MongoDB Distributed Data Storage HDFS reread Clouder_notes doc I made during THP for HDFS & Security MGMT and Big Data ETLs Command Commands hdfs dfs \u2013ls [hdfs path] hdfs dfs -mkdir [hdfs path] hdfs dfs -rm -r [hdfs path] hdfs dfs -put [local path] [hdfs path] hdfs dfs -get [hdfs path] hdfs dfs -getfacl [hdfs path] hdfs dfs -setfacl [group:name:rwx] [hdfs path] Hbase Cassandra Message Queue ZeroMQ The Official Guide great read for anyone working with distributed systems","title":"Data Storage"},{"location":"data/storage/#data-storage","text":"","title":"Data Storage"},{"location":"data/storage/#relational-databases","text":"","title":"Relational Databases"},{"location":"data/storage/#postgresql","text":"","title":"PostgreSQL"},{"location":"data/storage/#mysql","text":"","title":"MySQL"},{"location":"data/storage/#sqlite","text":"","title":"SQLite"},{"location":"data/storage/#oracle","text":"","title":"Oracle"},{"location":"data/storage/#sql-server","text":"","title":"SQL Server"},{"location":"data/storage/#nosql-databases","text":"","title":"NoSQL Databases"},{"location":"data/storage/#mongodb","text":"","title":"MongoDB"},{"location":"data/storage/#distributed-data-storage","text":"","title":"Distributed Data Storage"},{"location":"data/storage/#hdfs","text":"reread Clouder_notes doc I made during THP for HDFS & Security MGMT and Big Data ETLs","title":"HDFS"},{"location":"data/storage/#command-commands","text":"hdfs dfs \u2013ls [hdfs path] hdfs dfs -mkdir [hdfs path] hdfs dfs -rm -r [hdfs path] hdfs dfs -put [local path] [hdfs path] hdfs dfs -get [hdfs path] hdfs dfs -getfacl [hdfs path] hdfs dfs -setfacl [group:name:rwx] [hdfs path]","title":"Command Commands"},{"location":"data/storage/#hbase","text":"","title":"Hbase"},{"location":"data/storage/#cassandra","text":"","title":"Cassandra"},{"location":"data/storage/#message-queue","text":"","title":"Message Queue"},{"location":"data/storage/#zeromq","text":"The Official Guide great read for anyone working with distributed systems","title":"ZeroMQ"},{"location":"math/calculus/","text":"Calculus","title":"Calculus"},{"location":"math/calculus/#calculus","text":"","title":"Calculus"},{"location":"math/linear_algebra/","text":"Linear Algebra","title":"Linear Algebra"},{"location":"math/linear_algebra/#linear-algebra","text":"","title":"Linear Algebra"},{"location":"math/statistics/","text":"Statistics","title":"Statistics"},{"location":"math/statistics/#statistics","text":"","title":"Statistics"},{"location":"software/algorithms/","text":"Algorithms algorithm categories Sorting Recursion Dynamic Programming","title":"Algorithms"},{"location":"software/algorithms/#algorithms","text":"algorithm categories","title":"Algorithms"},{"location":"software/algorithms/#sorting","text":"","title":"Sorting"},{"location":"software/algorithms/#recursion","text":"","title":"Recursion"},{"location":"software/algorithms/#dynamic-programming","text":"","title":"Dynamic Programming"},{"location":"software/best_practices/","text":"Best Practices 12 Factor App Testing Unit Tests TDD BDD DDD Refactoring Source Control Management Documentation & Comments API Naming Conventions","title":"Best Practices"},{"location":"software/best_practices/#best-practices","text":"","title":"Best Practices"},{"location":"software/best_practices/#12-factor-app","text":"","title":"12 Factor App"},{"location":"software/best_practices/#testing","text":"","title":"Testing"},{"location":"software/best_practices/#unit-tests","text":"","title":"Unit Tests"},{"location":"software/best_practices/#tdd","text":"","title":"TDD"},{"location":"software/best_practices/#bdd","text":"","title":"BDD"},{"location":"software/best_practices/#ddd","text":"","title":"DDD"},{"location":"software/best_practices/#refactoring","text":"","title":"Refactoring"},{"location":"software/best_practices/#source-control-management","text":"","title":"Source Control Management"},{"location":"software/best_practices/#documentation-comments","text":"","title":"Documentation &amp; Comments"},{"location":"software/best_practices/#api-naming-conventions","text":"","title":"API Naming Conventions"},{"location":"software/computer_science/","text":"Computer Science curriculum https://teachyourselfcs.com/+ stanford cs107 (http://web.stanford.edu/class/cs107/) nand2tetris (https://www.nand2tetris.org/) https://www.coursera.org/learn/build-a-computer https://www.youtube.com/playlist?list=PLrDd_kMiAuNmSb-CKWQqq9oBFN_KNMTaI https://www.coursera.org/learn/nand2tetris2 https://www.youtube.com/playlist?list=PLrDd_kMiAuNmllp9vuPqCuttC1XL9VyVh Logic Gates Computer Architecture Operating Systems Languages and Compilers Computer Netowrking Embedded Programming Blockchain Nand2Tetris Notes","title":"Computer Science"},{"location":"software/computer_science/#computer-science","text":"curriculum https://teachyourselfcs.com/+ stanford cs107 (http://web.stanford.edu/class/cs107/) nand2tetris (https://www.nand2tetris.org/) https://www.coursera.org/learn/build-a-computer https://www.youtube.com/playlist?list=PLrDd_kMiAuNmSb-CKWQqq9oBFN_KNMTaI https://www.coursera.org/learn/nand2tetris2 https://www.youtube.com/playlist?list=PLrDd_kMiAuNmllp9vuPqCuttC1XL9VyVh","title":"Computer Science"},{"location":"software/computer_science/#logic-gates","text":"","title":"Logic Gates"},{"location":"software/computer_science/#computer-architecture","text":"","title":"Computer Architecture"},{"location":"software/computer_science/#operating-systems","text":"","title":"Operating Systems"},{"location":"software/computer_science/#languages-and-compilers","text":"","title":"Languages and Compilers"},{"location":"software/computer_science/#computer-netowrking","text":"","title":"Computer Netowrking"},{"location":"software/computer_science/#embedded-programming","text":"","title":"Embedded Programming"},{"location":"software/computer_science/#blockchain","text":"","title":"Blockchain"},{"location":"software/computer_science/#nand2tetris-notes","text":"","title":"Nand2Tetris Notes"},{"location":"software/data_structures/","text":"Data Structures Strings Arrays Hash Tables Linked Lists Trees AST BST Graphs","title":"Data Structures"},{"location":"software/data_structures/#data-structures","text":"","title":"Data Structures"},{"location":"software/data_structures/#strings","text":"","title":"Strings"},{"location":"software/data_structures/#arrays","text":"","title":"Arrays"},{"location":"software/data_structures/#hash-tables","text":"","title":"Hash Tables"},{"location":"software/data_structures/#linked-lists","text":"","title":"Linked Lists"},{"location":"software/data_structures/#trees","text":"","title":"Trees"},{"location":"software/data_structures/#ast","text":"","title":"AST"},{"location":"software/data_structures/#bst","text":"","title":"BST"},{"location":"software/data_structures/#graphs","text":"","title":"Graphs"},{"location":"software/design_patterns/","text":"Design Patterns Singleton Factory Pattern","title":"Design Patterns"},{"location":"software/design_patterns/#design-patterns","text":"","title":"Design Patterns"},{"location":"software/design_patterns/#singleton","text":"","title":"Singleton"},{"location":"software/design_patterns/#factory-pattern","text":"","title":"Factory Pattern"},{"location":"software/functional_paradigm/","text":"Functional Paradigm Haskell","title":"Functional Paradigm"},{"location":"software/functional_paradigm/#functional-paradigm","text":"","title":"Functional Paradigm"},{"location":"software/functional_paradigm/#haskell","text":"","title":"Haskell"},{"location":"software/guis/","text":"Graphical User Interfaces Vue Resources Vue UI Libarry HTML Tempaltes html5up templated Notes Block entire subdomain from getting indexed by search engines: robot.txt in root dir User-agent: * Disallow: / Block a specific HTML page: <meta name=\"robots\" content=\"noindex\"> in <head>","title":"GUIs"},{"location":"software/guis/#graphical-user-interfaces","text":"","title":"Graphical User Interfaces"},{"location":"software/guis/#vue","text":"","title":"Vue"},{"location":"software/guis/#resources","text":"Vue UI Libarry HTML Tempaltes html5up templated","title":"Resources"},{"location":"software/guis/#notes","text":"Block entire subdomain from getting indexed by search engines: robot.txt in root dir User-agent: * Disallow: / Block a specific HTML page: <meta name=\"robots\" content=\"noindex\"> in <head>","title":"Notes"},{"location":"software/oop/","text":"Object Oriented Programming Encapsulation Abstraction Inheritance Polymorphism Java Install JDK Debian / Ubuntu apt-get install openjdk-11-jdk CentOS / Fedora / RHEL yum -y install java-11-openjdk java-11-openjdk-devel cat > /etc/profile.d/java11.sh <<EOF export JAVA_HOME=$(dirname $(dirname $(readlink $(readlink $(which javac))))) export PATH=\\$PATH:\\$JAVA_HOME/bin EOF source /etc/profile.d/java11.sh Windows OpenJDK Install page for JDK 11 OpenJDK Archive Download JDK Set PATH (e.g. add .../Java/jdk-11/bin to PATH ) Set JAVA_HOME (i.e. set to jdk installation path without /bin dir) Test with java --version command and configure the JDK in IntelliJ","title":"OOP"},{"location":"software/oop/#object-oriented-programming","text":"Encapsulation Abstraction Inheritance Polymorphism","title":"Object Oriented Programming"},{"location":"software/oop/#java","text":"","title":"Java"},{"location":"software/oop/#install-jdk","text":"","title":"Install JDK"},{"location":"software/oop/#debian-ubuntu","text":"apt-get install openjdk-11-jdk","title":"Debian / Ubuntu"},{"location":"software/oop/#centos-fedora-rhel","text":"yum -y install java-11-openjdk java-11-openjdk-devel cat > /etc/profile.d/java11.sh <<EOF export JAVA_HOME=$(dirname $(dirname $(readlink $(readlink $(which javac))))) export PATH=\\$PATH:\\$JAVA_HOME/bin EOF source /etc/profile.d/java11.sh","title":"CentOS / Fedora / RHEL"},{"location":"software/oop/#windows","text":"OpenJDK Install page for JDK 11 OpenJDK Archive Download JDK Set PATH (e.g. add .../Java/jdk-11/bin to PATH ) Set JAVA_HOME (i.e. set to jdk installation path without /bin dir) Test with java --version command and configure the JDK in IntelliJ","title":"Windows"},{"location":"software/python/","text":"Python Type Checking type checker module: pip install mypy examples: method signature: def add(a: int, b: int) -> int: variable declaration: agw: int = 99 Style Guidlines: PEP8 & Linters PEP8 PEP20: Python Zen PEP287: Doc-String PEP463: Exceptions-catching PEP484: Type Hints Formatting Docstrings for all public modules, functions, classes and methods imports separated in 3 groups by blank lines standard library 3rd party libraries local application/library Whitespace/punctuation identations: 4 spaces per lect max line length: 79 characters 2 lines between top-level functions and classes 1 line between methods inside a class Naming modules: short, lowercase names classes: CapitalizedNaming functions: lowercase_with_underscores constants: ALL_CAPS non-public names: _start_with_underscore Linters pylint checks for PEP8 and other code smells pip install pylint pylint <package> or pylint <package>/file.py add pylintrc file: pylint --generate-rcfile > pylintrc Black great for CI pipelines to enforce consistency in source code pip install black black <package> Generating Documentation PEP 257: semantics and conventions for docstrings Docstrings string as first statement statements of a module, function, class or method becomes the __doc__ atribute, callable in python always use \"\"\"three double quotes\"\"\" end phrases in a period methods should specify a return value Sphinx generates HTML, PDF, eBook, Linux man pages, etc. from source extracts docstrings from code accepts reStructured Text to build documentation pages generate API docs: sphinx-apidoc -o <package> Quickstart pip install sphinx mkdir docs && cd docs sphinx-quickstart make html clean old builds: make clean html Tip: Use reStructured Text in Docstrings to automatically generate documentation: module Docstring: ``` \"\"\" module.py --------- This module contains a module-level docstring \"\"\" class MyClass: \"\"\" The MyClass class represents a generic class. Link to the :meth: run method. \"\"\" def run(self, name): \"\"\" Runs the class with the name of :attr:`name`. :param name : The name of the class :return: The new value of :attr:`name`. \"\"\" Package Management pip pip freeze > requirements.txt pip install requirements.txt pipenv","title":"Python"},{"location":"software/python/#python","text":"","title":"Python"},{"location":"software/python/#type-checking","text":"type checker module: pip install mypy examples: method signature: def add(a: int, b: int) -> int: variable declaration: agw: int = 99","title":"Type Checking"},{"location":"software/python/#style-guidlines-pep8-linters","text":"PEP8 PEP20: Python Zen PEP287: Doc-String PEP463: Exceptions-catching PEP484: Type Hints","title":"Style Guidlines: PEP8 &amp; Linters"},{"location":"software/python/#formatting","text":"Docstrings for all public modules, functions, classes and methods imports separated in 3 groups by blank lines standard library 3rd party libraries local application/library","title":"Formatting"},{"location":"software/python/#whitespacepunctuation","text":"identations: 4 spaces per lect max line length: 79 characters 2 lines between top-level functions and classes 1 line between methods inside a class","title":"Whitespace/punctuation"},{"location":"software/python/#naming","text":"modules: short, lowercase names classes: CapitalizedNaming functions: lowercase_with_underscores constants: ALL_CAPS non-public names: _start_with_underscore","title":"Naming"},{"location":"software/python/#linters","text":"","title":"Linters"},{"location":"software/python/#pylint","text":"checks for PEP8 and other code smells pip install pylint pylint <package> or pylint <package>/file.py add pylintrc file: pylint --generate-rcfile > pylintrc","title":"pylint"},{"location":"software/python/#black","text":"great for CI pipelines to enforce consistency in source code pip install black black <package>","title":"Black"},{"location":"software/python/#generating-documentation","text":"PEP 257: semantics and conventions for docstrings","title":"Generating Documentation"},{"location":"software/python/#docstrings","text":"string as first statement statements of a module, function, class or method becomes the __doc__ atribute, callable in python always use \"\"\"three double quotes\"\"\" end phrases in a period methods should specify a return value","title":"Docstrings"},{"location":"software/python/#sphinx","text":"generates HTML, PDF, eBook, Linux man pages, etc. from source extracts docstrings from code accepts reStructured Text to build documentation pages generate API docs: sphinx-apidoc -o <package>","title":"Sphinx"},{"location":"software/python/#quickstart","text":"pip install sphinx mkdir docs && cd docs sphinx-quickstart make html clean old builds: make clean html Tip: Use reStructured Text in Docstrings to automatically generate documentation: module Docstring: ``` \"\"\" module.py --------- This module contains a module-level docstring \"\"\" class MyClass: \"\"\" The MyClass class represents a generic class. Link to the :meth: run method. \"\"\" def run(self, name): \"\"\" Runs the class with the name of :attr:`name`. :param name : The name of the class :return: The new value of :attr:`name`. \"\"\"","title":"Quickstart"},{"location":"software/python/#package-management","text":"","title":"Package Management"},{"location":"software/python/#pip","text":"pip freeze > requirements.txt pip install requirements.txt","title":"pip"},{"location":"software/python/#pipenv","text":"","title":"pipenv"}]}
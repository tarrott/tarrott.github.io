{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Engineering Wiki Key Responsibilites Software Engineer Data Engineer System Administrator installing and updating software (security updates) managing backups (working with archive files) Gardening (Projects) Blacksmith Shop (Service) Sign your work a professional pride of ownership - \"I stand behind my work\" an indicator of quality, people should see the signature and expect the work to be solid, well written, tested, and documented present the end story - visuals/graphs/charts - for managers/clients what they want/are excited to see dig into the weeds (text) afterswards collect and visualize KPIs that showed how a business function performed before and after your consulting / product / service What does it take to be a top performer? - KPIs/Metrics the customers expectations at the time of delivery and what's in the actual delivery the customers expectations are evolving as much as the project is evolving you need to spend as much time managing customers expectations as you do on the project itself Guild System (Career) Aprentice Bachelor's Degree Learn one new language and technology every year Read a technical book a quarter Journeyman Master's Degree Open-Source Software Participate in local user groups Experiment with different environments Stay current (industry leaders, blogs, journals, trade magazines, newsgroups) Master","title":"Engineering Wiki"},{"location":"#engineering-wiki","text":"","title":"Engineering Wiki"},{"location":"#key-responsibilites","text":"Software Engineer Data Engineer System Administrator installing and updating software (security updates) managing backups (working with archive files)","title":"Key Responsibilites"},{"location":"#gardening-projects","text":"","title":"Gardening (Projects)"},{"location":"#blacksmith-shop-service","text":"Sign your work a professional pride of ownership - \"I stand behind my work\" an indicator of quality, people should see the signature and expect the work to be solid, well written, tested, and documented present the end story - visuals/graphs/charts - for managers/clients what they want/are excited to see dig into the weeds (text) afterswards collect and visualize KPIs that showed how a business function performed before and after your consulting / product / service What does it take to be a top performer? - KPIs/Metrics the customers expectations at the time of delivery and what's in the actual delivery the customers expectations are evolving as much as the project is evolving you need to spend as much time managing customers expectations as you do on the project itself","title":"Blacksmith Shop (Service)"},{"location":"#guild-system-career","text":"","title":"Guild System (Career)"},{"location":"#aprentice","text":"Bachelor's Degree Learn one new language and technology every year Read a technical book a quarter","title":"Aprentice"},{"location":"#journeyman","text":"Master's Degree Open-Source Software Participate in local user groups Experiment with different environments Stay current (industry leaders, blogs, journals, trade magazines, newsgroups)","title":"Journeyman"},{"location":"#master","text":"","title":"Master"},{"location":"design/","text":"Design Drawing Picture Design Color Theory Typography UI / UX","title":"Design"},{"location":"design/#design","text":"","title":"Design"},{"location":"design/#drawing","text":"","title":"Drawing"},{"location":"design/#picture-design","text":"","title":"Picture Design"},{"location":"design/#color-theory","text":"","title":"Color Theory"},{"location":"design/#typography","text":"","title":"Typography"},{"location":"design/#ui-ux","text":"","title":"UI / UX"},{"location":"resources/","text":"Curriculum Books Name Author Field ~The Pragmatic Programmer Andrew Hunt & David Thomas Career ~Soft Skills John Sonmez Career Cracking the Code Interview Laakman McDowell Career The Linux Command Lines William E. Shotts Jr. Linux Designing Data-Intensive Apps... Martin Kleppmann Architecture Analysis Patterns Martin Fowler Architecture Patterns of Enterprise App Architeture Martin Fowler Architecture Designing Distributed Systems Brenden Burns Architecture Kubernetes Up and Running Brenden Burns Architecture Managing Kubernetes Brenden Burns Architecture Kubernetes Best Practices Brenden Burns Architecture Fluent Python Luciano Ramalho Software Engineering Algorithms to Live By Christian & Griffiths Software Engineering Code Complete, Second Edition Steve McConnell Software Engineering Clean Code Robert Cecil Martin Software Engineering Growing Object Oriented Software... Steve Freeman & Nat Pryce Software Engineering The Mythical Man Month Fred Brooks Software Engineering Refactoring Kent Beck & Martin Fowler Software Engineering Design Patterns Gang of Four Software Engineering Head First Design Patterns E. Freeman & K. Sierra Software Engineering Operating Systems: Three Easy Pieces A. & R. Arpaci-Dusseau Software Engineering Working Effectively with Legacy Code Michael Feathers Software Engineering UML Distilled Martin Fowler Software Engineering The Data Engineering Cookbook Andreas Kretz Data Engineering Python for Data Analysis Wes Mckinney Data Engineering Python for Finance Yves Hilpisch Data Engineering Deep Learning Courville et. all Data Engineering The Design Of Everyday Things Dom Norman Design University MOOCS Data Structures & Algorithms Stanford CS106X Data Structures Princeton Algorithms Part 1 Princeton Algorithms Part 2 ~ MIT Algorithms MIT Advanced Data Structures U Alberta Design Patterns Java / OOP UC San Diego OOP Java Programming Specialization Object Oriented Programming in Java Data Structures and Performance Advanced Data Structures in Java Mastering the Software Engineering Interview Capstone: Analyzing (Social) Network Data RICE Parallel, Concurrent, and Distributed Programming in Java Parallel Programming Concurrent Programming Distributed Programming Computer Architecture Stanford CS107 Computer Organization & Systems U Jerusalem Nand to Tetris, Part 1 U Jerusalem Nand to Tetris, Part 2 U Wisconsin-Madison Operating Systems: Three Easy Pieces Course Data Engineering Courses Georgia Tech Machine Learning for Trading Stanford Machine Learning U San Francisco Applied ML U San Francisco Practical Deep Learning U San Francisco Computational Linear Algebra DevOps Engineering Courses U Virginia CICD DevOps Unafilliated MOOCS Design Gurus Grokking the System Design Interview ~ Gaurav Sen System Design ~ Shivang Sarawagi Web Application & Software Architecture ~ Udacity AWS Cloud Architect ~ A Cloud Guru AWS Certified Solutions Architect Associate 2020 ~ Bret Fisher : Docker & Kubernetes Jose Portilla Data Strucutres & Algorithms in Python Data Analysis & Machine Learning Khan Academy Statistics (inference & probability) Linear Algebra Calculus 1,2, & 3 Deeplearning.ai : Tensorflow Online Tutorials Haskell Spark Tutorials, Data Bricks PySpark Kubernetes the Hard Way Terraform Backtrader Tutorials GitHub Hello World Setup First Script Using Analyzers Optimization Modularization Creating Analyzers Data Replay Sentdex Python Intermediate Python Pandas Machine Learning Scikit-learn Finance Deep Learning Matplotlib Dash Web Django Flask Beauitful Soup Fun Alexa Skills Kivy/KivEnt AI SC2 Unsupervised ML Reinforcement Learning Other Golang RasPi RasPi Distributed Computing Quantum Computer Programming Other Media Podcasts Software Engineering Daily Talk Python Python Bytes The Changelog Command Line Heros Machine Learning Guide Darknet Diaries Coding Blocks .NET Professional Societies Association for Computing Machinery (ACM) IEEE Computer Society Websites https://github.com/kahun/awesome-sysadmin Architecture & Engineering https://12factor.net/ https://martinfowler.com/ https://staffeng.com/ https://free-for.dev/ https://teachyourselfcs.com/ https://learnk8s.io/production-best-practices https://github.com/andkret/Cookbook Technical Interview https://leetcode.com/ https://www.hackerrank.com/ (SQL) ~ Interview Cake https://hackernoon.com/google-interview-questions-deconstructed-the-knights-dialer-f780d516f029 Learning Labs https://linuxacademy.com/ https://www.katacoda.com/ OSS Database: dbeaver Terminal Emulator: Mac: iTerm2 Linux: Konsole Windows: MobaXterm Source Code Editors: Python/Golang: VSCode Java: IntelliJ IDEA Kubernetes: Lens","title":"Resources"},{"location":"resources/#curriculum","text":"","title":"Curriculum"},{"location":"resources/#books","text":"Name Author Field ~The Pragmatic Programmer Andrew Hunt & David Thomas Career ~Soft Skills John Sonmez Career Cracking the Code Interview Laakman McDowell Career The Linux Command Lines William E. Shotts Jr. Linux Designing Data-Intensive Apps... Martin Kleppmann Architecture Analysis Patterns Martin Fowler Architecture Patterns of Enterprise App Architeture Martin Fowler Architecture Designing Distributed Systems Brenden Burns Architecture Kubernetes Up and Running Brenden Burns Architecture Managing Kubernetes Brenden Burns Architecture Kubernetes Best Practices Brenden Burns Architecture Fluent Python Luciano Ramalho Software Engineering Algorithms to Live By Christian & Griffiths Software Engineering Code Complete, Second Edition Steve McConnell Software Engineering Clean Code Robert Cecil Martin Software Engineering Growing Object Oriented Software... Steve Freeman & Nat Pryce Software Engineering The Mythical Man Month Fred Brooks Software Engineering Refactoring Kent Beck & Martin Fowler Software Engineering Design Patterns Gang of Four Software Engineering Head First Design Patterns E. Freeman & K. Sierra Software Engineering Operating Systems: Three Easy Pieces A. & R. Arpaci-Dusseau Software Engineering Working Effectively with Legacy Code Michael Feathers Software Engineering UML Distilled Martin Fowler Software Engineering The Data Engineering Cookbook Andreas Kretz Data Engineering Python for Data Analysis Wes Mckinney Data Engineering Python for Finance Yves Hilpisch Data Engineering Deep Learning Courville et. all Data Engineering The Design Of Everyday Things Dom Norman Design","title":"Books"},{"location":"resources/#university-moocs","text":"","title":"University MOOCS"},{"location":"resources/#data-structures-algorithms","text":"Stanford CS106X Data Structures Princeton Algorithms Part 1 Princeton Algorithms Part 2 ~ MIT Algorithms MIT Advanced Data Structures U Alberta Design Patterns","title":"Data Structures &amp; Algorithms"},{"location":"resources/#java-oop","text":"UC San Diego OOP Java Programming Specialization Object Oriented Programming in Java Data Structures and Performance Advanced Data Structures in Java Mastering the Software Engineering Interview Capstone: Analyzing (Social) Network Data RICE Parallel, Concurrent, and Distributed Programming in Java Parallel Programming Concurrent Programming Distributed Programming","title":"Java / OOP"},{"location":"resources/#computer-architecture","text":"Stanford CS107 Computer Organization & Systems U Jerusalem Nand to Tetris, Part 1 U Jerusalem Nand to Tetris, Part 2 U Wisconsin-Madison Operating Systems: Three Easy Pieces Course","title":"Computer Architecture"},{"location":"resources/#data-engineering-courses","text":"Georgia Tech Machine Learning for Trading Stanford Machine Learning U San Francisco Applied ML U San Francisco Practical Deep Learning U San Francisco Computational Linear Algebra","title":"Data Engineering Courses"},{"location":"resources/#devops-engineering-courses","text":"U Virginia CICD DevOps","title":"DevOps Engineering Courses"},{"location":"resources/#unafilliated-moocs","text":"Design Gurus Grokking the System Design Interview ~ Gaurav Sen System Design ~ Shivang Sarawagi Web Application & Software Architecture ~ Udacity AWS Cloud Architect ~ A Cloud Guru AWS Certified Solutions Architect Associate 2020 ~ Bret Fisher : Docker & Kubernetes Jose Portilla Data Strucutres & Algorithms in Python Data Analysis & Machine Learning Khan Academy Statistics (inference & probability) Linear Algebra Calculus 1,2, & 3 Deeplearning.ai : Tensorflow","title":"Unafilliated MOOCS"},{"location":"resources/#online-tutorials","text":"Haskell Spark Tutorials, Data Bricks PySpark Kubernetes the Hard Way Terraform Backtrader Tutorials GitHub Hello World Setup First Script Using Analyzers Optimization Modularization Creating Analyzers Data Replay","title":"Online Tutorials"},{"location":"resources/#sentdex","text":"","title":"Sentdex"},{"location":"resources/#python","text":"Intermediate Python Pandas Machine Learning Scikit-learn Finance Deep Learning Matplotlib Dash","title":"Python"},{"location":"resources/#web","text":"Django Flask Beauitful Soup","title":"Web"},{"location":"resources/#fun","text":"Alexa Skills Kivy/KivEnt AI SC2 Unsupervised ML Reinforcement Learning","title":"Fun"},{"location":"resources/#other","text":"Golang RasPi RasPi Distributed Computing Quantum Computer Programming","title":"Other"},{"location":"resources/#other-media","text":"","title":"Other Media"},{"location":"resources/#podcasts","text":"Software Engineering Daily Talk Python Python Bytes The Changelog Command Line Heros Machine Learning Guide Darknet Diaries Coding Blocks .NET","title":"Podcasts"},{"location":"resources/#professional-societies","text":"Association for Computing Machinery (ACM) IEEE Computer Society","title":"Professional Societies"},{"location":"resources/#websites","text":"https://github.com/kahun/awesome-sysadmin","title":"Websites"},{"location":"resources/#architecture-engineering","text":"https://12factor.net/ https://martinfowler.com/ https://staffeng.com/ https://free-for.dev/ https://teachyourselfcs.com/ https://learnk8s.io/production-best-practices https://github.com/andkret/Cookbook","title":"Architecture &amp; Engineering"},{"location":"resources/#technical-interview","text":"https://leetcode.com/ https://www.hackerrank.com/ (SQL) ~ Interview Cake https://hackernoon.com/google-interview-questions-deconstructed-the-knights-dialer-f780d516f029","title":"Technical Interview"},{"location":"resources/#learning-labs","text":"https://linuxacademy.com/ https://www.katacoda.com/","title":"Learning Labs"},{"location":"resources/#oss","text":"Database: dbeaver Terminal Emulator: Mac: iTerm2 Linux: Konsole Windows: MobaXterm Source Code Editors: Python/Golang: VSCode Java: IntelliJ IDEA Kubernetes: Lens","title":"OSS"},{"location":"wiki/","text":"Wiki Structure Deployment Workflow sudo pip install mkdocs Manual Vanilla mkdocs gh-deploy git commit & push master User/Org Pages cd /site mkdocs gh-deploy --config-file ../mkdocs.yml --remote-branch master git commit & push develop Custom Domain StackOverflow Answer mkdocs gh-deploy git commit & push master Specify custom domain in GitHub Pages settings Add A recorsd for github.io 185.199.108.153 185.199.109.153 185.199.110.153 185.199.111.153 Add CNAME record to point domain to GitHub Page user.github.io/repo_name Enforce HTTPS Build Server Self-hosted","title":"Wiki"},{"location":"wiki/#wiki","text":"","title":"Wiki"},{"location":"wiki/#structure","text":"","title":"Structure"},{"location":"wiki/#deployment-workflow","text":"sudo pip install mkdocs","title":"Deployment Workflow"},{"location":"wiki/#manual","text":"","title":"Manual"},{"location":"wiki/#vanilla","text":"mkdocs gh-deploy git commit & push master","title":"Vanilla"},{"location":"wiki/#userorg-pages","text":"cd /site mkdocs gh-deploy --config-file ../mkdocs.yml --remote-branch master git commit & push develop","title":"User/Org Pages"},{"location":"wiki/#custom-domain","text":"StackOverflow Answer mkdocs gh-deploy git commit & push master Specify custom domain in GitHub Pages settings Add A recorsd for github.io 185.199.108.153 185.199.109.153 185.199.110.153 185.199.111.153 Add CNAME record to point domain to GitHub Page user.github.io/repo_name Enforce HTTPS","title":"Custom Domain"},{"location":"wiki/#build-server","text":"","title":"Build Server"},{"location":"wiki/#self-hosted","text":"","title":"Self-hosted"},{"location":"architecture/case_studies/","text":"Case Studies Netflix NASA Amazon LinkedIn Google Facebook Lyft Uber","title":"Case Studies"},{"location":"architecture/case_studies/#case-studies","text":"","title":"Case Studies"},{"location":"architecture/case_studies/#netflix","text":"","title":"Netflix"},{"location":"architecture/case_studies/#nasa","text":"","title":"NASA"},{"location":"architecture/case_studies/#amazon","text":"","title":"Amazon"},{"location":"architecture/case_studies/#linkedin","text":"","title":"LinkedIn"},{"location":"architecture/case_studies/#google","text":"","title":"Google"},{"location":"architecture/case_studies/#facebook","text":"","title":"Facebook"},{"location":"architecture/case_studies/#lyft","text":"","title":"Lyft"},{"location":"architecture/case_studies/#uber","text":"","title":"Uber"},{"location":"architecture/chaos_eng/","text":"Chaos Engineering Disaster Recovery database backups server images server configurations (and instance types) network configurations IP whitelisting code repositories data repositories (files, media, artifacts, images, etc.) documentation DR Failover Plans cold standby pilot light warm standby hot standby Cloud DR Strategies geographic recovery (multi-region services) Document and Practice practice frequently document every step, including decision makers continiously update documentation based on dry-runs","title":"Chaos Engineering"},{"location":"architecture/chaos_eng/#chaos-engineering","text":"","title":"Chaos Engineering"},{"location":"architecture/chaos_eng/#disaster-recovery","text":"database backups server images server configurations (and instance types) network configurations IP whitelisting code repositories data repositories (files, media, artifacts, images, etc.) documentation","title":"Disaster Recovery"},{"location":"architecture/chaos_eng/#dr-failover-plans","text":"cold standby pilot light warm standby hot standby","title":"DR Failover Plans"},{"location":"architecture/chaos_eng/#cloud-dr-strategies","text":"geographic recovery (multi-region services)","title":"Cloud DR Strategies"},{"location":"architecture/chaos_eng/#document-and-practice","text":"practice frequently document every step, including decision makers continiously update documentation based on dry-runs","title":"Document and Practice"},{"location":"architecture/cicd/","text":"CICD Build Immutable Instances - Image Pipeline Base OS image Security hardening, patching, agents -> Hardened image Application configuration -> Application Image Security Scanning -> Released image No changes performed on instances of the image prior to launching them in environments Any further changes require building a new image and restarting the pipeline from step 1 If immutable is not an option in the environment, then use a configuration management tool (Ansible) to track changes to the environment in a source version control repository Jenkins Test Unit Tests Integration Tests End-to-end Tests Deploy Automated deployment characteristics: It can be triggered by just one action, like one command on the command line and it will do the job. The steps will be pre-defined, reproducible and predictable. There is little or no human intervention from the start to the end. It should show the deployment progress as it happens, better feedback It should be atomic, which means either all the steps are completed or nothing happens. Some good to have features for automated deployment tools are: It should be able to deploy the same code to multiple servers Each deployment should be done from a given branch/tag/commit of a Version Control System (VCS) like git It should trigger notifications in the form of email/chat message Everyone should be able to view which branch/tag is deployed When a deployment is in progress, it should stop other deployments to start Rollback of the last deployment should be easy and fast. Source ansible octopus spinnaker Provision Infrastructure as Code Terraform install terraform locally download binary download checksums & checksum signature verify the signartue of the checksum against HashiCorp's GPG key add terraform binary to PATH Create a VM on GCE example Once per project enable the GCE API: gcloud services enable compute.googleapis.com Create Terraform config Run commands at creation: metadata_startup_script logs: /var/log/daemon.log Add ssh keys: metadata = { ssh-keys = ### } Run Terraform terraform init terraform plan terraform apply Clean up Terraform config main.tf resource \"google_compute_instance\" \"default\" { name = \"vm-${random_id.instance_id.hex}\" machine_type = \"f1-micro\" zone = \"us-west1-a\" boot_disk { initialize_params { image = \"debian-cloud/debian-9\" } } # ... } Ansible Ansible Control Machine (Dockerized) ansible-console - REPL that comes with tab completion Access ENV variables: {{ lookup('env','USER') }}\" command module escapes commands, while the shell module does not Configuring VMs from Windows Create a control node VM with debian or ubuntu ssh into control node VM apt-get -y update && apt-get -y install ansible Note: run an installation script after creating a VM with Terraform sudo apt update sudo apt -y upgrade sudo apt-get install software-properties-common sudo apt-add-repository ppa:ansible/ansible sudo apt-get update sudo apt-get install ansible Point to inventory dir [defaults] inventory = /vagrant/inv host_key_checking = False Docs Ansible Docs Ansible Galaxy Docs ansible-doc copy ansible-doc -t connection anisble-doc -t shell --list ansible-doc -t shell csh list commands with ansible + tab Inventory ansible-inventory --graph --vars ansible-inventory --list Inventory definitions in YAML all: hosts: ubuntu10: ansible_host: 192.168.50.10 ansible_private_key_file: .vagrant/machines/ubuntu10/virtualbox/private_key Child group [ubuntu] ubuntu10 ubuntu11 ubuntu12 Parent group [vagrant:children] centos ubuntu [vagrant:vars] ansible_user=vagrant ansible_port=22 Ad-hoc command ansible all -m ping ansible -m command -a \"git config --global --list\" vagrant ansible -m copy -a \"src=master.gitconfig dest=~/.gitconfig\" localehost dry run: add --check show changes: add --diff show package managers: ansible -m setup -a \"filter=ansible_pkg_mgr\" all Playbook ansible-playbook playbook.yml Debug: add -v up to -vvvv name: Ensure ~/.gitconfig copied from master.gitconfig hosts: localhost tasks: - name: copy git config copy: src: \"mast.gitconfig\" dest: \"~/.gitconfig\" when: ... # conditional become: yes # escalate privilege register git_config_copy # register output to variable - name: print captured variable debug: var=git_config_copy","title":"CICD"},{"location":"architecture/cicd/#cicd","text":"","title":"CICD"},{"location":"architecture/cicd/#build","text":"","title":"Build"},{"location":"architecture/cicd/#immutable-instances-image-pipeline","text":"Base OS image Security hardening, patching, agents -> Hardened image Application configuration -> Application Image Security Scanning -> Released image No changes performed on instances of the image prior to launching them in environments Any further changes require building a new image and restarting the pipeline from step 1 If immutable is not an option in the environment, then use a configuration management tool (Ansible) to track changes to the environment in a source version control repository","title":"Immutable Instances - Image Pipeline"},{"location":"architecture/cicd/#jenkins","text":"","title":"Jenkins"},{"location":"architecture/cicd/#test","text":"","title":"Test"},{"location":"architecture/cicd/#unit-tests","text":"","title":"Unit Tests"},{"location":"architecture/cicd/#integration-tests","text":"","title":"Integration Tests"},{"location":"architecture/cicd/#end-to-end-tests","text":"","title":"End-to-end Tests"},{"location":"architecture/cicd/#deploy","text":"Automated deployment characteristics: It can be triggered by just one action, like one command on the command line and it will do the job. The steps will be pre-defined, reproducible and predictable. There is little or no human intervention from the start to the end. It should show the deployment progress as it happens, better feedback It should be atomic, which means either all the steps are completed or nothing happens. Some good to have features for automated deployment tools are: It should be able to deploy the same code to multiple servers Each deployment should be done from a given branch/tag/commit of a Version Control System (VCS) like git It should trigger notifications in the form of email/chat message Everyone should be able to view which branch/tag is deployed When a deployment is in progress, it should stop other deployments to start Rollback of the last deployment should be easy and fast. Source ansible octopus spinnaker","title":"Deploy"},{"location":"architecture/cicd/#provision","text":"","title":"Provision"},{"location":"architecture/cicd/#infrastructure-as-code","text":"","title":"Infrastructure as Code"},{"location":"architecture/cicd/#terraform","text":"install terraform locally download binary download checksums & checksum signature verify the signartue of the checksum against HashiCorp's GPG key add terraform binary to PATH","title":"Terraform"},{"location":"architecture/cicd/#create-a-vm-on-gce-example","text":"Once per project enable the GCE API: gcloud services enable compute.googleapis.com Create Terraform config Run commands at creation: metadata_startup_script logs: /var/log/daemon.log Add ssh keys: metadata = { ssh-keys = ### } Run Terraform terraform init terraform plan terraform apply Clean up Terraform config main.tf resource \"google_compute_instance\" \"default\" { name = \"vm-${random_id.instance_id.hex}\" machine_type = \"f1-micro\" zone = \"us-west1-a\" boot_disk { initialize_params { image = \"debian-cloud/debian-9\" } } # ... }","title":"Create a VM on GCE example"},{"location":"architecture/cicd/#ansible","text":"Ansible Control Machine (Dockerized) ansible-console - REPL that comes with tab completion Access ENV variables: {{ lookup('env','USER') }}\" command module escapes commands, while the shell module does not","title":"Ansible"},{"location":"architecture/cicd/#configuring-vms-from-windows","text":"Create a control node VM with debian or ubuntu ssh into control node VM apt-get -y update && apt-get -y install ansible Note: run an installation script after creating a VM with Terraform sudo apt update sudo apt -y upgrade sudo apt-get install software-properties-common sudo apt-add-repository ppa:ansible/ansible sudo apt-get update sudo apt-get install ansible Point to inventory dir [defaults] inventory = /vagrant/inv host_key_checking = False","title":"Configuring VMs from Windows"},{"location":"architecture/cicd/#docs","text":"Ansible Docs Ansible Galaxy Docs ansible-doc copy ansible-doc -t connection anisble-doc -t shell --list ansible-doc -t shell csh list commands with ansible + tab","title":"Docs"},{"location":"architecture/cicd/#inventory","text":"ansible-inventory --graph --vars ansible-inventory --list Inventory definitions in YAML all: hosts: ubuntu10: ansible_host: 192.168.50.10 ansible_private_key_file: .vagrant/machines/ubuntu10/virtualbox/private_key Child group [ubuntu] ubuntu10 ubuntu11","title":"Inventory"},{"location":"architecture/cicd/#ubuntu12","text":"Parent group [vagrant:children] centos ubuntu [vagrant:vars] ansible_user=vagrant ansible_port=22","title":"ubuntu12"},{"location":"architecture/cicd/#ad-hoc-command","text":"ansible all -m ping ansible -m command -a \"git config --global --list\" vagrant ansible -m copy -a \"src=master.gitconfig dest=~/.gitconfig\" localehost dry run: add --check show changes: add --diff show package managers: ansible -m setup -a \"filter=ansible_pkg_mgr\" all","title":"Ad-hoc command"},{"location":"architecture/cicd/#playbook","text":"ansible-playbook playbook.yml Debug: add -v up to -vvvv name: Ensure ~/.gitconfig copied from master.gitconfig hosts: localhost tasks: - name: copy git config copy: src: \"mast.gitconfig\" dest: \"~/.gitconfig\" when: ... # conditional become: yes # escalate privilege register git_config_copy # register output to variable - name: print captured variable debug: var=git_config_copy","title":"Playbook"},{"location":"architecture/highlevel_design/","text":"High-level Design System design Orthagonal Architecture decoupling (you can change the UI or database without affecting the other) two different user interfaces should be able to be supported by the same underlying code base design components that are self-contained, independent, and with a single, well-defined purpose when components are isolated then one can change wihtout affecting the others allow components to communicate based on external interfaces that are consistent module design - if one module is comprimised it will not affect the others temporal decoupling - allow for concurrency to reduce time-based dependencies of a system gain in productivity and reduces risk Iterative Design The Twelve-Factor App Website Codebase: One codebase tracked in revision control, many deploys - Dependencies: Explicitly declare and isolate dependencies - Config: Store config in the environment - Backing services: Treat backing services as attached resources - Build, release, run: Strictly separate build and run stages - Process: Execute the app as one or more stateless processes - Port binding: Export services via port binding - Concurrency: Scale out via the process model - Disposability: Maximize robustness with fast startup and graceful shutdown - Dev/prod parity: Keep development, staging, and production as similar as possible - Logs: Treat logs as event streams - Admin processes: Run admin/management tasks as one-off processes - Tiers vs. Layers tiers involve physical separation of components in a system (UI, backend server, database, cache, message queues, load balancers, search, data processing, shared services) layers of an application represent the organization of the code (user interface, business logic, service, or data access layer) Distributed Systems Scalability the ability of an application to handle & withstand increased workload without sacrificing latency if the app takes x seconds to respond to a user reqiest, it should take the same x seconds to respond to each of the million concurrent user requests on the app Vertical adding more compute power does not require code refactoring Horizontal adding more nodes/hardware to existing hardware resource pool requires code refactoring to utilize distributed resources higher availability Monolith Should be the default choice unless there exists a very good reason to incorporate a microservices architecture - (Sam Newman & Martin Fowler goto; talk)[https://www.youtube.com/watch?v=GBTdnfD6s5Q] Simpler to build, test and deploy Weaknesses: small code changes require an entire redeployment of the application requires thorough regression testing introduces single points of failure flexibility and scalability are harder to achieve limits the use of the application using heterogenous technologies tend to be stateful and therefore making it more difficult to be cloud-native and distributed Microservice Can be incorporated incrementally by migrating small parts of the monolith application to microservices like a dial not an on/off switch - (Sam Newman & Martin Fowler goto; talk)[https://www.youtube.com/watch?v=GBTdnfD6s5Q] Fault isolation Easier management Isolated development and testing Ease of adding new features Ease of maintenance High availability Separated responsiblity/concerns (separate teams gain increased productivity and complete ownership) Increased scalability options Weaknesses: greatly increased complexity in management and monitoring of the system (distributed logging, networking, service discovery, CICD, alerts, tracing, health checks, etc.) requires more components or code to manage and utilize the distributed nodes requires more human resources to manage the more complex system storing consistency is hard to guarentee in a distributed environment (leans on eventual consistency) Load Balancing Caching reduces latency, stores data in memory instead of accessing disk - O(1) fetches avoids over computing and rerunning joins in SQL which greatly reduces latency can continue to serve data requests even when the database server goes down can be implanted at any application later - OS, network, CDN, database, client-side, cross-module communication between services in a microservice architecture Distributed Caching Data Storage use read-only replica databases to reduce read traffic and as a source for business/financial applications (can be promoted for disaster recovery) Data Partitioning Database Sharding Indexes Proxies Redundancy and Replication SQL vs. NoSQL RDBs: strong consistency ACID transactions great for handling relationships scalable with caching (especially read-heavy applications) NoSQL eventual consistency horizontal scalability great for read/write-heavy applications with little relationships CAP Theorem Consistent Hashing Message Queues Allows for communication in an environment with heterogeneous technology Enables asynchronous behavior in a service to run background processes, tasks, and batch jobs Cross-module communication in a microservice architecture Exchanges handle message queue logic use binding to deliver messages to queues Publisher / Subscriber Model one to many (broadcast) allow for filtering of events so that components can subscribe to only the events they need Point to Point Model one to one (entity relationship) High Availability Active-Passive HA mode: set of redundant components in standby node awaiting automatic failover Active-Active HA Mode: set of replicated components that share the workload of the system Montior: to detect single points of failure and unhealthy components in the system Automation: to allow for the system to conduct self-healing or handle increased/decreased workloads System Testing network bandwidth consumption throughput requests processed within a time range latency application memory/CPU usage end-user experience when the system is under a heavy load Reducing Bottlenecks horizontally scale workloads servers distributed databases asynchronous processes & modules wherever possible (reduce unecessary sequential steps) data compression caching (write-through cache) - only hit a database when it is really required use a CDN load balancers & efficient configuration picking the right databases code refactoring decoupling dynamic analysis / profiling (application profiler, code profiler) to see which processes are taking too long and consuming too many resources avoid unnecessary client-server requests (group them together if possible) Infrastructure Changes should include impact analysis rollback plan disaster recovery plan code review / discussion Cloud Cost Hygiene naming conventions tags (to associate resources to cost centers) group lifecycle person application IT Governance rules establish user roles and controls billing alarms Storage Costs Considerations storage capacity provisioned/used data transfer replication access patterns Types of Architecture Client-server Peer to Peer (P2P) blockchain node to node connections Federated extension of decenteralized architecture nodes are grouped and connect to pod servers that enable node discovery pods connect to each other to form the backbone of the network Event-driven reactive programming (Tornado, Nodejs, akka, etc.) Notes","title":"High-level Design"},{"location":"architecture/highlevel_design/#high-level-design","text":"System design","title":"High-level Design"},{"location":"architecture/highlevel_design/#orthagonal-architecture","text":"decoupling (you can change the UI or database without affecting the other) two different user interfaces should be able to be supported by the same underlying code base design components that are self-contained, independent, and with a single, well-defined purpose when components are isolated then one can change wihtout affecting the others allow components to communicate based on external interfaces that are consistent module design - if one module is comprimised it will not affect the others temporal decoupling - allow for concurrency to reduce time-based dependencies of a system gain in productivity and reduces risk","title":"Orthagonal Architecture"},{"location":"architecture/highlevel_design/#iterative-design","text":"","title":"Iterative Design"},{"location":"architecture/highlevel_design/#the-twelve-factor-app","text":"Website Codebase: One codebase tracked in revision control, many deploys - Dependencies: Explicitly declare and isolate dependencies - Config: Store config in the environment - Backing services: Treat backing services as attached resources - Build, release, run: Strictly separate build and run stages - Process: Execute the app as one or more stateless processes - Port binding: Export services via port binding - Concurrency: Scale out via the process model - Disposability: Maximize robustness with fast startup and graceful shutdown - Dev/prod parity: Keep development, staging, and production as similar as possible - Logs: Treat logs as event streams - Admin processes: Run admin/management tasks as one-off processes -","title":"The Twelve-Factor App"},{"location":"architecture/highlevel_design/#tiers-vs-layers","text":"tiers involve physical separation of components in a system (UI, backend server, database, cache, message queues, load balancers, search, data processing, shared services) layers of an application represent the organization of the code (user interface, business logic, service, or data access layer)","title":"Tiers vs. Layers"},{"location":"architecture/highlevel_design/#distributed-systems","text":"","title":"Distributed Systems"},{"location":"architecture/highlevel_design/#scalability","text":"the ability of an application to handle & withstand increased workload without sacrificing latency if the app takes x seconds to respond to a user reqiest, it should take the same x seconds to respond to each of the million concurrent user requests on the app","title":"Scalability"},{"location":"architecture/highlevel_design/#vertical","text":"adding more compute power does not require code refactoring","title":"Vertical"},{"location":"architecture/highlevel_design/#horizontal","text":"adding more nodes/hardware to existing hardware resource pool requires code refactoring to utilize distributed resources higher availability","title":"Horizontal"},{"location":"architecture/highlevel_design/#monolith","text":"Should be the default choice unless there exists a very good reason to incorporate a microservices architecture - (Sam Newman & Martin Fowler goto; talk)[https://www.youtube.com/watch?v=GBTdnfD6s5Q] Simpler to build, test and deploy Weaknesses: small code changes require an entire redeployment of the application requires thorough regression testing introduces single points of failure flexibility and scalability are harder to achieve limits the use of the application using heterogenous technologies tend to be stateful and therefore making it more difficult to be cloud-native and distributed","title":"Monolith"},{"location":"architecture/highlevel_design/#microservice","text":"Can be incorporated incrementally by migrating small parts of the monolith application to microservices like a dial not an on/off switch - (Sam Newman & Martin Fowler goto; talk)[https://www.youtube.com/watch?v=GBTdnfD6s5Q] Fault isolation Easier management Isolated development and testing Ease of adding new features Ease of maintenance High availability Separated responsiblity/concerns (separate teams gain increased productivity and complete ownership) Increased scalability options Weaknesses: greatly increased complexity in management and monitoring of the system (distributed logging, networking, service discovery, CICD, alerts, tracing, health checks, etc.) requires more components or code to manage and utilize the distributed nodes requires more human resources to manage the more complex system storing consistency is hard to guarentee in a distributed environment (leans on eventual consistency)","title":"Microservice"},{"location":"architecture/highlevel_design/#load-balancing","text":"","title":"Load Balancing"},{"location":"architecture/highlevel_design/#caching","text":"reduces latency, stores data in memory instead of accessing disk - O(1) fetches avoids over computing and rerunning joins in SQL which greatly reduces latency can continue to serve data requests even when the database server goes down can be implanted at any application later - OS, network, CDN, database, client-side, cross-module communication between services in a microservice architecture","title":"Caching"},{"location":"architecture/highlevel_design/#distributed-caching","text":"","title":"Distributed Caching"},{"location":"architecture/highlevel_design/#data-storage","text":"use read-only replica databases to reduce read traffic and as a source for business/financial applications (can be promoted for disaster recovery)","title":"Data Storage"},{"location":"architecture/highlevel_design/#data-partitioning","text":"","title":"Data Partitioning"},{"location":"architecture/highlevel_design/#database-sharding","text":"","title":"Database Sharding"},{"location":"architecture/highlevel_design/#indexes","text":"","title":"Indexes"},{"location":"architecture/highlevel_design/#proxies","text":"","title":"Proxies"},{"location":"architecture/highlevel_design/#redundancy-and-replication","text":"","title":"Redundancy and Replication"},{"location":"architecture/highlevel_design/#sql-vs-nosql","text":"RDBs: strong consistency ACID transactions great for handling relationships scalable with caching (especially read-heavy applications) NoSQL eventual consistency horizontal scalability great for read/write-heavy applications with little relationships","title":"SQL vs. NoSQL"},{"location":"architecture/highlevel_design/#cap-theorem","text":"","title":"CAP Theorem"},{"location":"architecture/highlevel_design/#consistent-hashing","text":"","title":"Consistent Hashing"},{"location":"architecture/highlevel_design/#message-queues","text":"Allows for communication in an environment with heterogeneous technology Enables asynchronous behavior in a service to run background processes, tasks, and batch jobs Cross-module communication in a microservice architecture Exchanges handle message queue logic use binding to deliver messages to queues","title":"Message Queues"},{"location":"architecture/highlevel_design/#publisher-subscriber-model","text":"one to many (broadcast) allow for filtering of events so that components can subscribe to only the events they need","title":"Publisher / Subscriber Model"},{"location":"architecture/highlevel_design/#point-to-point-model","text":"one to one (entity relationship)","title":"Point to Point Model"},{"location":"architecture/highlevel_design/#high-availability","text":"Active-Passive HA mode: set of redundant components in standby node awaiting automatic failover Active-Active HA Mode: set of replicated components that share the workload of the system Montior: to detect single points of failure and unhealthy components in the system Automation: to allow for the system to conduct self-healing or handle increased/decreased workloads","title":"High Availability"},{"location":"architecture/highlevel_design/#system-testing","text":"network bandwidth consumption throughput requests processed within a time range latency application memory/CPU usage end-user experience when the system is under a heavy load","title":"System Testing"},{"location":"architecture/highlevel_design/#reducing-bottlenecks","text":"horizontally scale workloads servers distributed databases asynchronous processes & modules wherever possible (reduce unecessary sequential steps) data compression caching (write-through cache) - only hit a database when it is really required use a CDN load balancers & efficient configuration picking the right databases code refactoring decoupling dynamic analysis / profiling (application profiler, code profiler) to see which processes are taking too long and consuming too many resources avoid unnecessary client-server requests (group them together if possible)","title":"Reducing Bottlenecks"},{"location":"architecture/highlevel_design/#infrastructure-changes","text":"should include impact analysis rollback plan disaster recovery plan code review / discussion","title":"Infrastructure Changes"},{"location":"architecture/highlevel_design/#cloud-cost-hygiene","text":"naming conventions tags (to associate resources to cost centers) group lifecycle person application IT Governance rules establish user roles and controls billing alarms","title":"Cloud Cost Hygiene"},{"location":"architecture/highlevel_design/#storage-costs-considerations","text":"storage capacity provisioned/used data transfer replication access patterns","title":"Storage Costs Considerations"},{"location":"architecture/highlevel_design/#types-of-architecture","text":"","title":"Types of Architecture"},{"location":"architecture/highlevel_design/#client-server","text":"","title":"Client-server"},{"location":"architecture/highlevel_design/#peer-to-peer-p2p","text":"blockchain node to node connections","title":"Peer to Peer (P2P)"},{"location":"architecture/highlevel_design/#federated","text":"extension of decenteralized architecture nodes are grouped and connect to pod servers that enable node discovery pods connect to each other to form the backbone of the network","title":"Federated"},{"location":"architecture/highlevel_design/#event-driven","text":"reactive programming (Tornado, Nodejs, akka, etc.)","title":"Event-driven"},{"location":"architecture/highlevel_design/#notes","text":"","title":"Notes"},{"location":"architecture/homelab/","text":"Homelab","title":"Homelab"},{"location":"architecture/homelab/#homelab","text":"","title":"Homelab"},{"location":"architecture/linux/","text":"Linux command docs: man find run previous command with sudo: sudo !! Package Management comparison apk apt view installed packages: apt list --installed specific packge: apt list -a pkgNameHere updates apt update apt list --upgradable apt upgrade update dev version (e.g. 18.04.3 to 18.04.5): apt dist-upgrade & reboot update specific package: e.g. apache2: add-apt-repository ppa:ondrej/apache2 && apt update && apt install apache2 remove package files no longer needed: apt autoclean dpkg yum - `yum update` pacman Compile from Source Debian/Ubuntu install essential build tools: sudo apt install build-essential nmap example check current nmap version: nmap -V get tarball link for the latest development release for nmap from nmap.org download section under \"source code distribution\" wget -v https://nmap.org/dist/nmap-7.91.tar.bz2 uncompress the bzip2 file with: tar -jxvf nmap-7.91.tar.bz2 -j flag filters the archive through bzip2 -x flag for extract -v flag for verbose -f flag specifies the following filename read the INSTALL file to get compile instructions: ./configure make sudo make install (uses root privileges because it installs files outside of home directory) update the index of files the locate command uses: sudo updatedb find nmap installs: locate /bin/nmap In general, /bin is for key parts of the operating system, /usr/bin for less critical utilities and /usr/local/bin for software chosen to be installed manually. When a command is run it will search through each of the directories given in the PATH environment variable, and use the first match. So, if /bin/nmap exists, it will run instead of /usr/local/bin Security Risk NOTE: since namp was installed outside of the apt system, this binary will not get updates when apt update is ran. This is okay for a utility like namp, but for an exposed service (apache, mysql, etc.) it is important to track security alerts for the application (including its dependencies), and patch it to the latest versions when they're available. This is both tedius and a serious security risk if not done. Configuration hostnamectl set-hostname <name> the line preserve_hostname might need to be changed to true in /etc/cloud/cloud.cfg to save hostname for cloud servers after server reboot old way hostname: edit /etc/hostname & rename /etc/host entries Disable hibernation: sudo systemctl mask sleep.target suspend.target hibernate.target hybrid-sleep.target No password configured: sudo passwd hostname -f dpkg-reconfigure tzdata / timedatectl set-timezone fdisk cmd prompt export PS1=\"\\[\\033[36m\\]\\u\\[\\033[m\\]@\\[\\033[32m\\]\\h:\\[\\033[33;1m\\]\\w\\[\\033[m\\]\\$ \" SSH Server Config edit: /etc/ssh/sshd_config disable root login: PermitRootLogin no enable key checking: PubkeyAuthentication yes disable password logins: PasswordAuthentication no PasswordAuthentication no UsePAM no SSH Client Config ~/.ssh/config host shortcutname HostName server.domain.com Port 5555 User username User Administration add user: adduser <user> add group: addgroup <group> asign group: adduser <user> <group> delete user: deluser <user> useradd , usermod , and userdel are low level utlities and the above methods should usually be used by sysadmins assign group: usermod -aG <group> <username> give sudo: Deb: usermod -aG sudo <user> Fed: usermod -aG wheel <user> copy ssh key: ssh-copy-id -i $HOME/.ssh/id_rsa.pub <user@ip> give permissions to run specific commands: visudo # Allow user \"helper\" to run \"sudo reboot\" # ...and don't prompt for a password # helper ALL = NOPASSWD:/sbin/reboot Cloud VMs GCP setup root pass: sudo passwd System Diagnostics uptime -p uptime also shows load average and number of users currently logged in \"The three numbers show the load averages for the last minute, 5 minutes, and 15 minutes respectively. A load average of 1 reflects the full workload of a single processor on the system.\" CPU number of processors: grep processor /proc/cpuinfo | wc -l Memory free -h cat /proc/meminfo vmstat -s vmstat 5 10 displays a system\u2019s virtual memory statistics 10 times at 5-second intervals. so shows the amount of data being moved to the swap to free up memory si shows the amount of data being pulled from the swap back into memory When a server is constantly swapping into and out of memory, that indicates that the load on it is too great for the available resources. If your system consumes most of its swap area, that might mean that the server is trying to do more than its available memory permits not that it\u2019s low on memory. top and htop sort by CPU (P) or Memory (M) Disk df -h Processes ps -ef pgrep and pidof services (process name): service --status-all if services is unavailable check /usr/sbin/service add to path Finding files locate file anywhere in the filesystem: locate access.log requires updated db: sudo updatedb executable location: which vim location of file within a specific dir containing name: find /var -name access.log files that have been edited in the last 3 hours: find /home -mtime -3 search through a directory of text files: grep -R -i \"PermitRootLogin\" /etc/* Log Rotation logrotate usually automatically configured and ran by cron in /etc/cron.daily/logrotate overall logrotate config file stored in /etc/logrotate.conf individual logrotate recipes are stored in /etc/logrotate.d/ for example, the /etc/logratote.d/apache2 config file could be edited so that the log file is emailed to an auditor each time it is rotated File System Management check disk usage: ncdu cp -R mkdir -p touch mv rm scp tree set the sticky bit for a directory: chmod 1777 <dir_name> only the owner of each file in this directory can delete the file Links Hard links ln /path/to/target/file hardlinkname can only link to files, not directories cannot link to remote files on different partitions/disks links reference the same inode as the target file, and therefore the physical location on disk if the target file is moved or deleted, the hard link will still retain a copy of the original contents Symbolic links ln -s /path/to/target softlinkname can additionally link to directories, remote files, and files on different partitions/disks links will no longer reference the target file if it has been moved or deleted symlinks get their own inode because they are referencing abstract filenames/dirs and not the target's physical location if the target is recreated with the same name then the symlink will reference this new target ACLS allow for more details filesystem permissions (e.g. allowing a user who is not the owner or in the file's group to access the file) getfacl setfacl Filepaths man hier for filesystem hierarchy Filesystem Hierarchy Standard Linux Filesystem Tree Overview Daily Driver repos: ~/repos/github/ /home home sweet home, this is the place for users' home directories. Application Development /opt stores additional software not handled by the package manager. /tmp is a place for temporary files used by applications /var is dedicated to variable data, such as logs, databases, websites, and temporary spool (e-mail etc.) files that persist from one boot to the next. A notable directory it contains is /var/log where system log files are kept. System /bin is a place for most commonly used terminal commands, like ls, mount, rm, etc. /etc contains system-global configuration files, which affect the system's behavior for all users. /root is the superuser's home directory, not in /home/ to allow for booting the system even if /home/ is not available. /usr contains the majority of user utilities and applications, and partly replicates the root directory structure, containing for instance, among others, /usr/bin/ and /usr/lib. /boot contains files needed to start up the system, including the Linux kernel, a RAM disk image and bootloader configuration files. /dev contains all device files, which are not regular files but instead refer to various hardware devices on the system, including hard drives. /lib contains very important dynamic libraries and kernel modules /media is intended as a mount point for external devices, such as hard drives or removable media (floppies, CDs, DVDs). /mnt is also a place for mount points, but dedicated specifically to \"temporarily mounted\" devices, such as network filesyste Reading Files less move to top and bottom of file: gg and G search: / then n and N for (next & previous selection) more tail and head follow a log while it is written to: tail -f /var/log/apache2/access.log Text Manipulation grep filter for lines in specific file: grep \"authenticating /var/log/apache2/access.log Recursive, case insensitive grep: grep -R -i \"PermitRootLogin\" /etc/* find all lines that contain a specific string: cat /var/log/apache2/access.log | grep '0.0.0.0' | wc -l use -v to find the inverse line that do NOT contain a specific string: cat /var/log/apache2/access.log | grep -v '0.0.0.0' | wc -l awk sed print line containing first occurance of a string: sed -n '/STRING/p /var/log/apache/access.log | head -1 print line containing last occurance of a string: sed -n '/STRING/p /var/log/apache/access.log | tail -1 jq `cut cut -d' ' -f2 : -d delimter and -f field number will get the second word when piped from stdin display the 10th word onwards (use space as the delimiter): grep \"authenticating\" /var/log/auth.log| grep \"root\"| cut -f 10- -d\" \" sort : sort order uniq : filter unique values .vimrc colorscheme evening set relativenumber set number set colorcolumn=80 set tabstop=2 shiftwidth=2 expandtab set backspace=indent,eol,start highlight ColorColum ctermbg=0 guibg=lightgrey Archiving Files & Directories gather files and directories into one place with tar: tar -cvf myinits.tar /etc/init.d/ compress the tarball with gzip: gzip myinits.tar in 1 step: tar -cvzf myinits.tgz /etc/init.d/ - c switch: create an archive file - v switch: print verbose output - z switch: compress the result - f switch: create the output tarball with the following filename uncompress compressed tarball: tar -xvf myinits.tgz - C switch: specify a different directory to uncompress to Netowrking find local ip: ip a ping telnet nmap [localhost or IP] : show exposed ports ss : more modern replacement for netstat view open ports (both internal and exposed): ss -tlp netstat : See if the service is listening on the correct socket view open ports: netstat -tulpn o flag: show PIDs traceroute mtr curl I flag: show header info only L flag: follow redirects Firewalls netfilter iptables view traffic rules: iptables -L nftables ufw setup: ufw allow http && ufw enable Cron Scheduler cron expressions Install cron deb/ubuntu apt-get update && apt-get install cron pgrep cron or pidof cron systemctl start cron or service cron start on old systems crontab -e NOTE: Make sure to add a NEWLINE at the end different environment: cron passes a minimal set of environment variables to cronjobs view them: * * * * * env > /tmp/env.output default log location: /var/log/syslog redirect cron logs to specified locations e.g. 0 15 * * * /home/user/daily-backup.sh >> /var/log/daily-backup.log 2>&1 SFTP current working directory: pwd upload files: put path_to_local_file remote_file download files: get path_to_remote_file local_file Bash alias alias ll='ls -l' cd - pwd ls -la cat / bat Git Delete remote branch: git push -d <remote_name> <branch_name> Delete local branch: git push -D <branch_name> Make git commands (e.g. pull) more verbose: GIT_SSH_COMMAND=\"ssh -vv\" View files modified by git pull: git diff --name-only HEAD@{0} HEAD@{1} bash_profile vs. bashrc .bash_profile is executed at login, while .bashrc is executed on every shell instance can also use an .ini file and source it in the .bash_profile with a source ~/my.ini .bash_profile example: export PS1=\"\\[\\033[36m\\]\\u\\[\\033[m\\]@\\[\\033[32m\\]\\h:\\[\\033[33;1m\\]\\w\\[\\033[m\\]\\$ \" #PS1='[${curr_host}@`if [[ \"$mh\" = \"$PWD\" ]]; then echo \"$PWD\"; else echo \"~$ {PWD#$ih}\"; fi`] $ ' INFA_HOME=/app/infa/Informatica/10.2.0 JAVA_HOME=$INFA_HOME/java PATH=$PATH:$HOME/bin:$INFA_HOME/server/bin:$JAVA_HOME/lib PYTHONPATH=$PYTHONPATH:/app-san/infa/infa_shared/Scripts/dimpypkg LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$ORACLE_LIBS:$INFA_HOME/server/bin:$JAVA_HOME/lib:$ODBCHOME/lib:$PWX_HOME #LD_LIBRARY_PATH=$INFA_HOME/server/bin export INFA_HOME export JAVA_HOME export PATH export PYTHONPATH export LD_LIBRARY_PATH . ./my.ini my.ini example\" #!/bin/sh ih=/app-san/infa/infa_shared curr_host=`echo $HOSTNAME | cut -d'.' -f 1` alias lp='ls -alp' alias lpt='ls -alptr | tail' alias lpd='ls -alp | grep ^d' alias lps='ls -alpSr | tail' alias cache='cd $ih/Cache' alias lkp='cd $ih/LkpFiles' alias log='cd $ih/log' alias temp='cd $ih/Temp' alias param='cd $ih/ParamFiles' alias scripts='cd $ih/Scripts' alias slog='cd $ih/SessLogs' alias src='cd $ih/SrcFiles' alias tgt='cd $ih/TgtFiles' alias wlog='cd $ih/WorkflowLogs' alias infa='cd $INFA_HOME' Shell Scripting run the shell script as a custom command, such as topattackers write the shell script and make it executeable: chmod +x topattackers run it with ./topattackers allow for it to be run anywhere throughout the system by moving it somewhere the PATH variable sees it best place to store it is where manually installed software lives: sudo mv ./topattackers /usr/local/bin/topattackers now the script can be called by simply typing the command topattackers Common commands print to screen: echo follow by a string or variable get user input: read followed by the variable name the input will be stored to use a variable with $varname make program wait: sleep followed by an int for the seconds write to file: echo $varname > text.txt > will overwrite output to the following filename >> will append output to the following filename Script arguments $0 $1, $2 etc. Functions Distros Servers: Debian Daily Driver: Ubuntu & Arch Business: CentOS Notes https://www.linode.com/docs/tools-reference/linux-system-administration-basics/ .bashrc vs. .bash_profile .bash_profile is executed for login shells, while .bashrc is executed for interactive non-login shells. When you login (type username and password) via console, either sitting at the machine, or remotely via ssh: .bash_profile is executed to configure your shell before the initial command prompt. But, if you\u2019ve already logged into your machine and open a new terminal window (xterm) then .bashrc is executed before the window command prompt. .bashrc is also run when you start a new bash instance by typing /bin/bash in a terminal.","title":"Linux"},{"location":"architecture/linux/#linux","text":"command docs: man find run previous command with sudo: sudo !!","title":"Linux"},{"location":"architecture/linux/#package-management","text":"comparison","title":"Package Management"},{"location":"architecture/linux/#apk","text":"","title":"apk"},{"location":"architecture/linux/#apt","text":"view installed packages: apt list --installed specific packge: apt list -a pkgNameHere updates apt update apt list --upgradable apt upgrade update dev version (e.g. 18.04.3 to 18.04.5): apt dist-upgrade & reboot update specific package: e.g. apache2: add-apt-repository ppa:ondrej/apache2 && apt update && apt install apache2 remove package files no longer needed: apt autoclean","title":"apt"},{"location":"architecture/linux/#dpkg","text":"","title":"dpkg"},{"location":"architecture/linux/#yum","text":"- `yum update`","title":"yum"},{"location":"architecture/linux/#pacman","text":"","title":"pacman"},{"location":"architecture/linux/#compile-from-source","text":"","title":"Compile from Source"},{"location":"architecture/linux/#debianubuntu","text":"install essential build tools: sudo apt install build-essential","title":"Debian/Ubuntu"},{"location":"architecture/linux/#nmap-example","text":"check current nmap version: nmap -V get tarball link for the latest development release for nmap from nmap.org download section under \"source code distribution\" wget -v https://nmap.org/dist/nmap-7.91.tar.bz2 uncompress the bzip2 file with: tar -jxvf nmap-7.91.tar.bz2 -j flag filters the archive through bzip2 -x flag for extract -v flag for verbose -f flag specifies the following filename read the INSTALL file to get compile instructions: ./configure make sudo make install (uses root privileges because it installs files outside of home directory) update the index of files the locate command uses: sudo updatedb find nmap installs: locate /bin/nmap In general, /bin is for key parts of the operating system, /usr/bin for less critical utilities and /usr/local/bin for software chosen to be installed manually. When a command is run it will search through each of the directories given in the PATH environment variable, and use the first match. So, if /bin/nmap exists, it will run instead of /usr/local/bin Security Risk NOTE: since namp was installed outside of the apt system, this binary will not get updates when apt update is ran. This is okay for a utility like namp, but for an exposed service (apache, mysql, etc.) it is important to track security alerts for the application (including its dependencies), and patch it to the latest versions when they're available. This is both tedius and a serious security risk if not done.","title":"nmap example"},{"location":"architecture/linux/#configuration","text":"hostnamectl set-hostname <name> the line preserve_hostname might need to be changed to true in /etc/cloud/cloud.cfg to save hostname for cloud servers after server reboot old way hostname: edit /etc/hostname & rename /etc/host entries Disable hibernation: sudo systemctl mask sleep.target suspend.target hibernate.target hybrid-sleep.target No password configured: sudo passwd hostname -f dpkg-reconfigure tzdata / timedatectl set-timezone fdisk cmd prompt export PS1=\"\\[\\033[36m\\]\\u\\[\\033[m\\]@\\[\\033[32m\\]\\h:\\[\\033[33;1m\\]\\w\\[\\033[m\\]\\$ \"","title":"Configuration"},{"location":"architecture/linux/#ssh-server-config","text":"edit: /etc/ssh/sshd_config disable root login: PermitRootLogin no enable key checking: PubkeyAuthentication yes disable password logins: PasswordAuthentication no PasswordAuthentication no UsePAM no","title":"SSH Server Config"},{"location":"architecture/linux/#ssh-client-config","text":"~/.ssh/config host shortcutname HostName server.domain.com Port 5555 User username","title":"SSH Client Config"},{"location":"architecture/linux/#user-administration","text":"add user: adduser <user> add group: addgroup <group> asign group: adduser <user> <group> delete user: deluser <user> useradd , usermod , and userdel are low level utlities and the above methods should usually be used by sysadmins assign group: usermod -aG <group> <username> give sudo: Deb: usermod -aG sudo <user> Fed: usermod -aG wheel <user> copy ssh key: ssh-copy-id -i $HOME/.ssh/id_rsa.pub <user@ip> give permissions to run specific commands: visudo # Allow user \"helper\" to run \"sudo reboot\" # ...and don't prompt for a password # helper ALL = NOPASSWD:/sbin/reboot","title":"User Administration"},{"location":"architecture/linux/#cloud-vms","text":"","title":"Cloud VMs"},{"location":"architecture/linux/#gcp","text":"setup root pass: sudo passwd","title":"GCP"},{"location":"architecture/linux/#system-diagnostics","text":"uptime -p uptime also shows load average and number of users currently logged in \"The three numbers show the load averages for the last minute, 5 minutes, and 15 minutes respectively. A load average of 1 reflects the full workload of a single processor on the system.\"","title":"System Diagnostics"},{"location":"architecture/linux/#cpu","text":"number of processors: grep processor /proc/cpuinfo | wc -l","title":"CPU"},{"location":"architecture/linux/#memory","text":"free -h cat /proc/meminfo vmstat -s vmstat 5 10 displays a system\u2019s virtual memory statistics 10 times at 5-second intervals. so shows the amount of data being moved to the swap to free up memory si shows the amount of data being pulled from the swap back into memory When a server is constantly swapping into and out of memory, that indicates that the load on it is too great for the available resources. If your system consumes most of its swap area, that might mean that the server is trying to do more than its available memory permits not that it\u2019s low on memory. top and htop sort by CPU (P) or Memory (M)","title":"Memory"},{"location":"architecture/linux/#disk","text":"df -h","title":"Disk"},{"location":"architecture/linux/#processes","text":"ps -ef pgrep and pidof services (process name): service --status-all if services is unavailable check /usr/sbin/service add to path","title":"Processes"},{"location":"architecture/linux/#finding-files","text":"locate file anywhere in the filesystem: locate access.log requires updated db: sudo updatedb executable location: which vim location of file within a specific dir containing name: find /var -name access.log files that have been edited in the last 3 hours: find /home -mtime -3 search through a directory of text files: grep -R -i \"PermitRootLogin\" /etc/*","title":"Finding files"},{"location":"architecture/linux/#log-rotation","text":"logrotate usually automatically configured and ran by cron in /etc/cron.daily/logrotate overall logrotate config file stored in /etc/logrotate.conf individual logrotate recipes are stored in /etc/logrotate.d/ for example, the /etc/logratote.d/apache2 config file could be edited so that the log file is emailed to an auditor each time it is rotated","title":"Log Rotation"},{"location":"architecture/linux/#file-system-management","text":"check disk usage: ncdu cp -R mkdir -p touch mv rm scp tree set the sticky bit for a directory: chmod 1777 <dir_name> only the owner of each file in this directory can delete the file","title":"File System Management"},{"location":"architecture/linux/#links","text":"","title":"Links"},{"location":"architecture/linux/#hard-links","text":"ln /path/to/target/file hardlinkname can only link to files, not directories cannot link to remote files on different partitions/disks links reference the same inode as the target file, and therefore the physical location on disk if the target file is moved or deleted, the hard link will still retain a copy of the original contents","title":"Hard links"},{"location":"architecture/linux/#symbolic-links","text":"ln -s /path/to/target softlinkname can additionally link to directories, remote files, and files on different partitions/disks links will no longer reference the target file if it has been moved or deleted symlinks get their own inode because they are referencing abstract filenames/dirs and not the target's physical location if the target is recreated with the same name then the symlink will reference this new target","title":"Symbolic links"},{"location":"architecture/linux/#acls","text":"allow for more details filesystem permissions (e.g. allowing a user who is not the owner or in the file's group to access the file) getfacl setfacl","title":"ACLS"},{"location":"architecture/linux/#filepaths","text":"man hier for filesystem hierarchy Filesystem Hierarchy Standard Linux Filesystem Tree Overview","title":"Filepaths"},{"location":"architecture/linux/#daily-driver","text":"repos: ~/repos/github/ /home home sweet home, this is the place for users' home directories.","title":"Daily Driver"},{"location":"architecture/linux/#application-development","text":"/opt stores additional software not handled by the package manager. /tmp is a place for temporary files used by applications /var is dedicated to variable data, such as logs, databases, websites, and temporary spool (e-mail etc.) files that persist from one boot to the next. A notable directory it contains is /var/log where system log files are kept.","title":"Application Development"},{"location":"architecture/linux/#system","text":"/bin is a place for most commonly used terminal commands, like ls, mount, rm, etc. /etc contains system-global configuration files, which affect the system's behavior for all users. /root is the superuser's home directory, not in /home/ to allow for booting the system even if /home/ is not available. /usr contains the majority of user utilities and applications, and partly replicates the root directory structure, containing for instance, among others, /usr/bin/ and /usr/lib. /boot contains files needed to start up the system, including the Linux kernel, a RAM disk image and bootloader configuration files. /dev contains all device files, which are not regular files but instead refer to various hardware devices on the system, including hard drives. /lib contains very important dynamic libraries and kernel modules /media is intended as a mount point for external devices, such as hard drives or removable media (floppies, CDs, DVDs). /mnt is also a place for mount points, but dedicated specifically to \"temporarily mounted\" devices, such as network filesyste","title":"System"},{"location":"architecture/linux/#reading-files","text":"less move to top and bottom of file: gg and G search: / then n and N for (next & previous selection) more tail and head follow a log while it is written to: tail -f /var/log/apache2/access.log","title":"Reading Files"},{"location":"architecture/linux/#text-manipulation","text":"grep filter for lines in specific file: grep \"authenticating /var/log/apache2/access.log Recursive, case insensitive grep: grep -R -i \"PermitRootLogin\" /etc/* find all lines that contain a specific string: cat /var/log/apache2/access.log | grep '0.0.0.0' | wc -l use -v to find the inverse line that do NOT contain a specific string: cat /var/log/apache2/access.log | grep -v '0.0.0.0' | wc -l awk sed print line containing first occurance of a string: sed -n '/STRING/p /var/log/apache/access.log | head -1 print line containing last occurance of a string: sed -n '/STRING/p /var/log/apache/access.log | tail -1 jq `cut cut -d' ' -f2 : -d delimter and -f field number will get the second word when piped from stdin display the 10th word onwards (use space as the delimiter): grep \"authenticating\" /var/log/auth.log| grep \"root\"| cut -f 10- -d\" \" sort : sort order uniq : filter unique values .vimrc colorscheme evening set relativenumber set number set colorcolumn=80 set tabstop=2 shiftwidth=2 expandtab set backspace=indent,eol,start highlight ColorColum ctermbg=0 guibg=lightgrey","title":"Text Manipulation"},{"location":"architecture/linux/#archiving-files-directories","text":"gather files and directories into one place with tar: tar -cvf myinits.tar /etc/init.d/ compress the tarball with gzip: gzip myinits.tar in 1 step: tar -cvzf myinits.tgz /etc/init.d/ - c switch: create an archive file - v switch: print verbose output - z switch: compress the result - f switch: create the output tarball with the following filename uncompress compressed tarball: tar -xvf myinits.tgz - C switch: specify a different directory to uncompress to","title":"Archiving Files &amp; Directories"},{"location":"architecture/linux/#netowrking","text":"find local ip: ip a ping telnet nmap [localhost or IP] : show exposed ports ss : more modern replacement for netstat view open ports (both internal and exposed): ss -tlp netstat : See if the service is listening on the correct socket view open ports: netstat -tulpn o flag: show PIDs traceroute mtr curl I flag: show header info only L flag: follow redirects","title":"Netowrking"},{"location":"architecture/linux/#firewalls","text":"netfilter iptables view traffic rules: iptables -L nftables ufw setup: ufw allow http && ufw enable","title":"Firewalls"},{"location":"architecture/linux/#cron-scheduler","text":"cron expressions Install cron deb/ubuntu apt-get update && apt-get install cron pgrep cron or pidof cron systemctl start cron or service cron start on old systems crontab -e NOTE: Make sure to add a NEWLINE at the end different environment: cron passes a minimal set of environment variables to cronjobs view them: * * * * * env > /tmp/env.output default log location: /var/log/syslog redirect cron logs to specified locations e.g. 0 15 * * * /home/user/daily-backup.sh >> /var/log/daily-backup.log 2>&1","title":"Cron Scheduler"},{"location":"architecture/linux/#sftp","text":"current working directory: pwd upload files: put path_to_local_file remote_file download files: get path_to_remote_file local_file","title":"SFTP"},{"location":"architecture/linux/#bash","text":"alias alias ll='ls -l' cd - pwd ls -la cat / bat","title":"Bash"},{"location":"architecture/linux/#git","text":"Delete remote branch: git push -d <remote_name> <branch_name> Delete local branch: git push -D <branch_name> Make git commands (e.g. pull) more verbose: GIT_SSH_COMMAND=\"ssh -vv\" View files modified by git pull: git diff --name-only HEAD@{0} HEAD@{1}","title":"Git"},{"location":"architecture/linux/#bash_profile-vs-bashrc","text":".bash_profile is executed at login, while .bashrc is executed on every shell instance can also use an .ini file and source it in the .bash_profile with a source ~/my.ini .bash_profile example: export PS1=\"\\[\\033[36m\\]\\u\\[\\033[m\\]@\\[\\033[32m\\]\\h:\\[\\033[33;1m\\]\\w\\[\\033[m\\]\\$ \" #PS1='[${curr_host}@`if [[ \"$mh\" = \"$PWD\" ]]; then echo \"$PWD\"; else echo \"~$ {PWD#$ih}\"; fi`] $ ' INFA_HOME=/app/infa/Informatica/10.2.0 JAVA_HOME=$INFA_HOME/java PATH=$PATH:$HOME/bin:$INFA_HOME/server/bin:$JAVA_HOME/lib PYTHONPATH=$PYTHONPATH:/app-san/infa/infa_shared/Scripts/dimpypkg LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$ORACLE_LIBS:$INFA_HOME/server/bin:$JAVA_HOME/lib:$ODBCHOME/lib:$PWX_HOME #LD_LIBRARY_PATH=$INFA_HOME/server/bin export INFA_HOME export JAVA_HOME export PATH export PYTHONPATH export LD_LIBRARY_PATH . ./my.ini my.ini example\" #!/bin/sh ih=/app-san/infa/infa_shared curr_host=`echo $HOSTNAME | cut -d'.' -f 1` alias lp='ls -alp' alias lpt='ls -alptr | tail' alias lpd='ls -alp | grep ^d' alias lps='ls -alpSr | tail' alias cache='cd $ih/Cache' alias lkp='cd $ih/LkpFiles' alias log='cd $ih/log' alias temp='cd $ih/Temp' alias param='cd $ih/ParamFiles' alias scripts='cd $ih/Scripts' alias slog='cd $ih/SessLogs' alias src='cd $ih/SrcFiles' alias tgt='cd $ih/TgtFiles' alias wlog='cd $ih/WorkflowLogs' alias infa='cd $INFA_HOME'","title":"bash_profile vs. bashrc"},{"location":"architecture/linux/#shell-scripting","text":"run the shell script as a custom command, such as topattackers write the shell script and make it executeable: chmod +x topattackers run it with ./topattackers allow for it to be run anywhere throughout the system by moving it somewhere the PATH variable sees it best place to store it is where manually installed software lives: sudo mv ./topattackers /usr/local/bin/topattackers now the script can be called by simply typing the command topattackers","title":"Shell Scripting"},{"location":"architecture/linux/#common-commands","text":"print to screen: echo follow by a string or variable get user input: read followed by the variable name the input will be stored to use a variable with $varname make program wait: sleep followed by an int for the seconds write to file: echo $varname > text.txt > will overwrite output to the following filename >> will append output to the following filename","title":"Common commands"},{"location":"architecture/linux/#script-arguments","text":"$0 $1, $2 etc.","title":"Script arguments"},{"location":"architecture/linux/#functions","text":"","title":"Functions"},{"location":"architecture/linux/#distros","text":"Servers: Debian Daily Driver: Ubuntu & Arch Business: CentOS","title":"Distros"},{"location":"architecture/linux/#notes","text":"https://www.linode.com/docs/tools-reference/linux-system-administration-basics/","title":"Notes"},{"location":"architecture/linux/#bashrc-vs-bash_profile","text":".bash_profile is executed for login shells, while .bashrc is executed for interactive non-login shells. When you login (type username and password) via console, either sitting at the machine, or remotely via ssh: .bash_profile is executed to configure your shell before the initial command prompt. But, if you\u2019ve already logged into your machine and open a new terminal window (xterm) then .bashrc is executed before the window command prompt. .bashrc is also run when you start a new bash instance by typing /bin/bash in a terminal.","title":".bashrc vs. .bash_profile"},{"location":"architecture/monitoring/","text":"Monitoring The 3 pillars of Observability: Metrics Answers: \"How has the system performance changed throughout time?\" numbers related to events that happened in a specifc time range example technologies: Metricbeats or Prometheus with Grafana Logs Answers: \"What is happening in the system?\" gives insights into a single events occuring in the system example technologies: ELK stack Tracing Answers: \"How are the system components interacting with each other?\" shows how long a specific request spent in the different system components example technologies: Jaeger or Zipkin Live Metrics Stream app system platform Server Metrics CPU Utilization Memory Utilization Network Utilization Disk Performance and Read/Writes Disk Space Optimization Game Plan Run performance tests Capture and analyze metrics Select the optimal instance Monitor for outliers to make sure the optimal instance continues to be optimal Grafana Centeralized Logging ELK Stack Open Distro Apache 2.0 license Security Alerting SQL Performance Annalyzer (perftop CLI tool) Kubernetes Operator: Elastic Cloud on Kubernetes (ECK) uses the free tier Elasticsearch License Configure Elasticsearch stack in containers docker-compose quickstart repo Increase heap size / allocate more memory for ES in JVM options: Update environment variables in docker-compose file - \"ES_JAVA_OPTS=-Xms4g -Xmx4g\" Modify file descriptors: File descriptors: \"Make sure to increase the limit on the number of open files descriptors for the user running Elasticsearch to 65,536 or higher.\" run everything as root user ulimit -Hn : 4096 -> 1024000 vim /usr/lib/sysctl.d/elasticsearch.conf vim /etc/security/limits/conf restorecon /etc/security/limits.conf reboot system /usr/lib/sysctl.d/elasticsearch.conf fs.file-max = 512000 vm.max_map_count = 524288 vm.swappiness = 1 /etc/security/limits/conf * soft nofile 1024000 * hard nofile 1024000 * soft memlock unlimited * hard memlock unlimited elastic soft nofile 1024000 elastic hard nofile 1024000 elastic soft memlock unlimited elastic hard memlock unlimited root soft nofile 1024000 root hard nofile 1024000 root soft memlock unlimited root hard memlock unlimited Logstash Install additional plugins: /bin/logstash install logstash-input-jmx Run pipeline: /bin/logstash.bat -f logstash-pipeline.conf --config.reload.automatic Test pipeline configuration: /bin/logstash.bat -f logstash-pipeline.conf --config.test_and_exit Run as service on a windows server: use NSSM Ingest log file Log sample 2020-04-21 20:52:15,173 WARN | ActiveMQ Task-1 () | [NetworkConnector:?] Could not start network... 2020-04-21 20:52:15,360 INFO | main () | [AbstractService:?] ...finished service destruction 2020-04-21 20:52:15,095 INFO | pool-9-thread-1 () | [EventConsumer:?] Interrupted while waiting for event. application-pipeline.conf input { file { path => \"E:/My_Application/Config/application.log\" type => \"log\" start_position => \"beginning\" sincedb_path => \"NUL\" } } filter { grok { match => { \"message\" => \"%{DATA:date} %{DATA:time} %{DATA:log_level} \\| %{DATA:thread} \\| \\[%{DATA:service}\\] %{GREEDYDATA:log_msg}\" } add_field => { \"timestamp\" => \"%{date} %{time}\"} remove_field => [ \"date\", \"time\"] } if \"_grokparsefailure\" in [tags] { drop { } } date { match => [ \"timestamp\", \"YYYY-MM-dd HH:mm:ss,SSS\"] } mutate { add_field => { \"application_name\" => \"MY_APPLICATION\"} remove_field => [ \"timestamp\" ] } } output { elasticsearch { hosts => \"0.0.0.0:9200\" user => \"user\" password => \"passwd\" index => \"test-index-%{+YYYY.MM.dd}\" } } Ingest jmx data Requires a jmx.conf file defined in path in jmx input plugin input { jmx { path => \"C:/elk/logstash/jmx\" polling_frequency => 15 type => \"jmx\" } } jmx.conf { \"host\": \"localhost\", \"port\": 9000, \"queries\": [ { \"object_name\" : \"java.lang:type=Memory\", \"object_alias\" : \"Memory\" }, { \"object_name\" : \"java.lang:type=Runtime\", \"attributes\" : [ \"uptime\", \"StartTime\" ], \"object_alias\" : \"Runtime\" }, { \"object_name\" : \"java.lang:type=GarbageCollector,name=*\", \"attributes\" : [ \"CollectionCount\", \"CollectionTime\" ], \"object_alias\" : \"${type}.${name}\" }, { \"object_name\" : \"java.nio:type=BufferPool,name=*\", \"object_alias\" : \"${type}.${name}\" }] } Beats Filebeat Example: send docker container logs to logstash filebeat.inputs: - type: docker containers.path: /usr/share/dockerlogs/data containers.ids: - '1fb97e7e06677bc9c2aa2cee28b9a2e489a7c8ead62829d10a69e60e38d27310' #ignore_decoding_error: true json.message_key: log json.keys_under_root: true #exclude_lines: ['^\\n'] multiline.pattern: '^\\s' multiline.match: after processors: - add_docker_metadata: host: \"unix:///var/run/docker.sock\" - timestamp: field: time layouts: - '2020-02-12T06:59:01.944972082Z' filebeat.config.modules: path: ${path.config}/modules.d/*.yml reload.enabled: false output.logstash: hosts: [\"172.18.1.200:5000\"] logging.to_files: true logging.to_syslog: false Kibana JMX Example Line chart visualization Y-axis: Average X-axis: Date Histogram Split series Filters aggregation bucket: metric_path:jvm.Memory.HeapMemoryUsage.used Health Checks Correlation Token -- Hetrix Tools Update Agent: wget https://raw.github.com/hetrixtools/agent/master/hetrixtools_update.sh && bash hetrixtools_update.sh Install Django Agent: Install LAMP Stack Agent: wget https://raw.github.com/hetrixtools/agent/master/hetrixtools_install.sh && bash hetrixtools_install.sh c27ea72ad602c327a2cd4f07133ee8a2 0 mysql,apache2,ssh 0 0 1 80,443,3306 -- DataDog uninstall agent: sudo apt-get remove --purge datadog-agent -y","title":"Monitoring"},{"location":"architecture/monitoring/#monitoring","text":"The 3 pillars of Observability: Metrics Answers: \"How has the system performance changed throughout time?\" numbers related to events that happened in a specifc time range example technologies: Metricbeats or Prometheus with Grafana Logs Answers: \"What is happening in the system?\" gives insights into a single events occuring in the system example technologies: ELK stack Tracing Answers: \"How are the system components interacting with each other?\" shows how long a specific request spent in the different system components example technologies: Jaeger or Zipkin","title":"Monitoring"},{"location":"architecture/monitoring/#live-metrics-stream","text":"app system platform","title":"Live Metrics Stream"},{"location":"architecture/monitoring/#server-metrics","text":"CPU Utilization Memory Utilization Network Utilization Disk Performance and Read/Writes Disk Space","title":"Server Metrics"},{"location":"architecture/monitoring/#optimization-game-plan","text":"Run performance tests Capture and analyze metrics Select the optimal instance Monitor for outliers to make sure the optimal instance continues to be optimal","title":"Optimization Game Plan"},{"location":"architecture/monitoring/#grafana","text":"","title":"Grafana"},{"location":"architecture/monitoring/#centeralized-logging","text":"","title":"Centeralized Logging"},{"location":"architecture/monitoring/#elk-stack","text":"Open Distro Apache 2.0 license Security Alerting SQL Performance Annalyzer (perftop CLI tool) Kubernetes Operator: Elastic Cloud on Kubernetes (ECK) uses the free tier Elasticsearch License","title":"ELK Stack"},{"location":"architecture/monitoring/#configure-elasticsearch-stack-in-containers","text":"docker-compose quickstart repo","title":"Configure Elasticsearch stack in containers"},{"location":"architecture/monitoring/#increase-heap-size-allocate-more-memory-for-es-in-jvm-options","text":"Update environment variables in docker-compose file - \"ES_JAVA_OPTS=-Xms4g -Xmx4g\"","title":"Increase heap size / allocate more memory for ES in JVM options:"},{"location":"architecture/monitoring/#modify-file-descriptors","text":"File descriptors: \"Make sure to increase the limit on the number of open files descriptors for the user running Elasticsearch to 65,536 or higher.\" run everything as root user ulimit -Hn : 4096 -> 1024000 vim /usr/lib/sysctl.d/elasticsearch.conf vim /etc/security/limits/conf restorecon /etc/security/limits.conf reboot system /usr/lib/sysctl.d/elasticsearch.conf fs.file-max = 512000 vm.max_map_count = 524288 vm.swappiness = 1 /etc/security/limits/conf * soft nofile 1024000 * hard nofile 1024000 * soft memlock unlimited * hard memlock unlimited elastic soft nofile 1024000 elastic hard nofile 1024000 elastic soft memlock unlimited elastic hard memlock unlimited root soft nofile 1024000 root hard nofile 1024000 root soft memlock unlimited root hard memlock unlimited","title":"Modify file descriptors:"},{"location":"architecture/monitoring/#logstash","text":"Install additional plugins: /bin/logstash install logstash-input-jmx Run pipeline: /bin/logstash.bat -f logstash-pipeline.conf --config.reload.automatic Test pipeline configuration: /bin/logstash.bat -f logstash-pipeline.conf --config.test_and_exit Run as service on a windows server: use NSSM","title":"Logstash"},{"location":"architecture/monitoring/#ingest-log-file","text":"Log sample 2020-04-21 20:52:15,173 WARN | ActiveMQ Task-1 () | [NetworkConnector:?] Could not start network... 2020-04-21 20:52:15,360 INFO | main () | [AbstractService:?] ...finished service destruction 2020-04-21 20:52:15,095 INFO | pool-9-thread-1 () | [EventConsumer:?] Interrupted while waiting for event. application-pipeline.conf input { file { path => \"E:/My_Application/Config/application.log\" type => \"log\" start_position => \"beginning\" sincedb_path => \"NUL\" } } filter { grok { match => { \"message\" => \"%{DATA:date} %{DATA:time} %{DATA:log_level} \\| %{DATA:thread} \\| \\[%{DATA:service}\\] %{GREEDYDATA:log_msg}\" } add_field => { \"timestamp\" => \"%{date} %{time}\"} remove_field => [ \"date\", \"time\"] } if \"_grokparsefailure\" in [tags] { drop { } } date { match => [ \"timestamp\", \"YYYY-MM-dd HH:mm:ss,SSS\"] } mutate { add_field => { \"application_name\" => \"MY_APPLICATION\"} remove_field => [ \"timestamp\" ] } } output { elasticsearch { hosts => \"0.0.0.0:9200\" user => \"user\" password => \"passwd\" index => \"test-index-%{+YYYY.MM.dd}\" } }","title":"Ingest log file"},{"location":"architecture/monitoring/#ingest-jmx-data","text":"Requires a jmx.conf file defined in path in jmx input plugin input { jmx { path => \"C:/elk/logstash/jmx\" polling_frequency => 15 type => \"jmx\" } } jmx.conf { \"host\": \"localhost\", \"port\": 9000, \"queries\": [ { \"object_name\" : \"java.lang:type=Memory\", \"object_alias\" : \"Memory\" }, { \"object_name\" : \"java.lang:type=Runtime\", \"attributes\" : [ \"uptime\", \"StartTime\" ], \"object_alias\" : \"Runtime\" }, { \"object_name\" : \"java.lang:type=GarbageCollector,name=*\", \"attributes\" : [ \"CollectionCount\", \"CollectionTime\" ], \"object_alias\" : \"${type}.${name}\" }, { \"object_name\" : \"java.nio:type=BufferPool,name=*\", \"object_alias\" : \"${type}.${name}\" }] }","title":"Ingest jmx data"},{"location":"architecture/monitoring/#beats","text":"","title":"Beats"},{"location":"architecture/monitoring/#filebeat","text":"Example: send docker container logs to logstash filebeat.inputs: - type: docker containers.path: /usr/share/dockerlogs/data containers.ids: - '1fb97e7e06677bc9c2aa2cee28b9a2e489a7c8ead62829d10a69e60e38d27310' #ignore_decoding_error: true json.message_key: log json.keys_under_root: true #exclude_lines: ['^\\n'] multiline.pattern: '^\\s' multiline.match: after processors: - add_docker_metadata: host: \"unix:///var/run/docker.sock\" - timestamp: field: time layouts: - '2020-02-12T06:59:01.944972082Z' filebeat.config.modules: path: ${path.config}/modules.d/*.yml reload.enabled: false output.logstash: hosts: [\"172.18.1.200:5000\"] logging.to_files: true logging.to_syslog: false","title":"Filebeat"},{"location":"architecture/monitoring/#kibana","text":"","title":"Kibana"},{"location":"architecture/monitoring/#jmx-example","text":"Line chart visualization Y-axis: Average X-axis: Date Histogram Split series Filters aggregation bucket: metric_path:jvm.Memory.HeapMemoryUsage.used","title":"JMX Example"},{"location":"architecture/monitoring/#health-checks","text":"","title":"Health Checks"},{"location":"architecture/monitoring/#correlation-token","text":"--","title":"Correlation Token"},{"location":"architecture/monitoring/#hetrix-tools","text":"Update Agent: wget https://raw.github.com/hetrixtools/agent/master/hetrixtools_update.sh && bash hetrixtools_update.sh Install Django Agent: Install LAMP Stack Agent: wget https://raw.github.com/hetrixtools/agent/master/hetrixtools_install.sh && bash hetrixtools_install.sh c27ea72ad602c327a2cd4f07133ee8a2 0 mysql,apache2,ssh 0 0 1 80,443,3306 --","title":"Hetrix Tools"},{"location":"architecture/monitoring/#datadog","text":"uninstall agent: sudo apt-get remove --purge datadog-agent -y","title":"DataDog"},{"location":"architecture/security/","text":"Security secure access to services and infrastructure resources network access controls identity and access privileges encrypt in transit and stored data monitor & alert: user activity network traffic cloud & network configuration in all environments identify vulnerabilities in: application code (static code analysis) build artifacts VM & Container images operating systems cloud & networking configuration Webserver Security Audit open ports (should be limited to 80 & 443) SSH/RDP (22 & 3389) should be using IP whitelisting or a VPN for access check for up-to-date packages apache2: curl --head | grep Server JS libraries (e.g. jQuery jQuery().jquery in browser console) RSA & GPG ssh-keygen -t rsa -C user@email user in comment should match the user to login to remote system Server Security Best Practices keep instances up to date and patched enforce hardening add vulnerability monitoring implement monitoring and alerting tools Securing a Web Server SSL Test Debian fail2ban Firewall ufw Mandatory Access Control (MAC) opposed to Discretionary Access Control(DAC) SELinux sets policies to manage how users, files, directories, memory, sockets, tcp/udp ports, etc. interact with each other policies can stop a process (e.g. chmod -R 777 /home ) from running, even as root Securing Applications apparmor - only allow commands/applications to run specific functionality Secrets sealed secrets: encrypted secrets stored in repositories Vault an alternative to storing secrets as encrypted text in repositories that offers additional features beyond secure secret storage: dynamic secrets: generating secrets for on-demand access data encryption: encrypt and decrypt data in databases without Vault actually storing it leasing and renewl: associates a lease with all secrets that provides the ability to renew the leases or revoke the secret if expired revocation: revoke single secrets or a tree of secrets such as all secrets read by a specific user or of a particular type (useful for key rolling and locking down systems) monitor access Setup Install Manual: download vault binary and adding it to PATH Mac (Homebrew): brew install vault Windows (Chocolately): choco install vault Start a DEV server vault server -dev NOTE: Never run the DEV server in production! Save configuration Save the generated unseal key into a new file called \"unseal.key\" Add environment variables: export VAULT_ADDR=\"http://127.0.0.1:8200\" export VAULT_DEV_ROOT_TOKEN_ID=\"s.TolAAwSmTYlBmE0E1M4M0ADG\" Check server satuts vault status Add secrets vault kv put secret/hello foo=world excited=yes Get secrets vault kv get secret/hello vault kv get -field=excited secret/hello vault kv get -format=json secret/hello | jq -r .data.data.excited Delete secrets vault kv delete secret/hello Network Traffic Ingress can be controlled and restricted using network ACLs, security groups, which are both effective firewalls, routing rules, and host based endpoint security tools which oftentimes contain firewall capabilities. Egress handled using internet gateways and nat gateways. As with ingress traffic, egress traffic should also be controlled and restricted for a number of reasons. Security Scripts recent logins and sudo usage logged in /var/log/auth.log #!/bin/bash # # attacker - prints out the last failed login attempt # echo \"The last failed login attempt came from IP address:\" grep -i \"disconnected from\" /var/log/auth.log|tail -1| cut -d: -f4| cut -f7 -d\" \" # ## topattack - list the most persistent attackers # if [ -z \"$1\" ]; then echo -e \"\\nUsage: `basename $0` <num> - Lists the top <num> attackers by IP\" exit 0 fi echo \" \" echo \"Persistant recent attackers\" echo \" \" echo \"Attempts IP \" echo \"-----------------------\" grep \"Disconnected from authenticating user root\" /var/log/auth.log|cut -d: -f 4 | Kali Linux","title":"Security"},{"location":"architecture/security/#security","text":"secure access to services and infrastructure resources network access controls identity and access privileges encrypt in transit and stored data monitor & alert: user activity network traffic cloud & network configuration in all environments identify vulnerabilities in: application code (static code analysis) build artifacts VM & Container images operating systems cloud & networking configuration","title":"Security"},{"location":"architecture/security/#webserver-security-audit","text":"open ports (should be limited to 80 & 443) SSH/RDP (22 & 3389) should be using IP whitelisting or a VPN for access check for up-to-date packages apache2: curl --head | grep Server JS libraries (e.g. jQuery jQuery().jquery in browser console)","title":"Webserver Security Audit"},{"location":"architecture/security/#rsa-gpg","text":"ssh-keygen -t rsa -C user@email user in comment should match the user to login to remote system","title":"RSA &amp; GPG"},{"location":"architecture/security/#server-security-best-practices","text":"keep instances up to date and patched enforce hardening add vulnerability monitoring implement monitoring and alerting tools","title":"Server Security Best Practices"},{"location":"architecture/security/#securing-a-web-server","text":"SSL Test","title":"Securing a Web Server"},{"location":"architecture/security/#debian","text":"fail2ban","title":"Debian"},{"location":"architecture/security/#firewall","text":"ufw","title":"Firewall"},{"location":"architecture/security/#mandatory-access-control-mac","text":"opposed to Discretionary Access Control(DAC)","title":"Mandatory Access Control (MAC)"},{"location":"architecture/security/#selinux","text":"sets policies to manage how users, files, directories, memory, sockets, tcp/udp ports, etc. interact with each other policies can stop a process (e.g. chmod -R 777 /home ) from running, even as root","title":"SELinux"},{"location":"architecture/security/#securing-applications","text":"apparmor - only allow commands/applications to run specific functionality","title":"Securing Applications"},{"location":"architecture/security/#secrets","text":"sealed secrets: encrypted secrets stored in repositories","title":"Secrets"},{"location":"architecture/security/#vault","text":"an alternative to storing secrets as encrypted text in repositories that offers additional features beyond secure secret storage: dynamic secrets: generating secrets for on-demand access data encryption: encrypt and decrypt data in databases without Vault actually storing it leasing and renewl: associates a lease with all secrets that provides the ability to renew the leases or revoke the secret if expired revocation: revoke single secrets or a tree of secrets such as all secrets read by a specific user or of a particular type (useful for key rolling and locking down systems) monitor access","title":"Vault"},{"location":"architecture/security/#setup","text":"Install Manual: download vault binary and adding it to PATH Mac (Homebrew): brew install vault Windows (Chocolately): choco install vault Start a DEV server vault server -dev NOTE: Never run the DEV server in production! Save configuration Save the generated unseal key into a new file called \"unseal.key\" Add environment variables: export VAULT_ADDR=\"http://127.0.0.1:8200\" export VAULT_DEV_ROOT_TOKEN_ID=\"s.TolAAwSmTYlBmE0E1M4M0ADG\" Check server satuts vault status","title":"Setup"},{"location":"architecture/security/#add-secrets","text":"vault kv put secret/hello foo=world excited=yes","title":"Add secrets"},{"location":"architecture/security/#get-secrets","text":"vault kv get secret/hello vault kv get -field=excited secret/hello vault kv get -format=json secret/hello | jq -r .data.data.excited","title":"Get secrets"},{"location":"architecture/security/#delete-secrets","text":"vault kv delete secret/hello","title":"Delete secrets"},{"location":"architecture/security/#network-traffic","text":"","title":"Network Traffic"},{"location":"architecture/security/#ingress","text":"can be controlled and restricted using network ACLs, security groups, which are both effective firewalls, routing rules, and host based endpoint security tools which oftentimes contain firewall capabilities.","title":"Ingress"},{"location":"architecture/security/#egress","text":"handled using internet gateways and nat gateways. As with ingress traffic, egress traffic should also be controlled and restricted for a number of reasons.","title":"Egress"},{"location":"architecture/security/#security-scripts","text":"recent logins and sudo usage logged in /var/log/auth.log #!/bin/bash # # attacker - prints out the last failed login attempt # echo \"The last failed login attempt came from IP address:\" grep -i \"disconnected from\" /var/log/auth.log|tail -1| cut -d: -f4| cut -f7 -d\" \" # ## topattack - list the most persistent attackers # if [ -z \"$1\" ]; then echo -e \"\\nUsage: `basename $0` <num> - Lists the top <num> attackers by IP\" exit 0 fi echo \" \" echo \"Persistant recent attackers\" echo \" \" echo \"Attempts IP \" echo \"-----------------------\" grep \"Disconnected from authenticating user root\" /var/log/auth.log|cut -d: -f 4 |","title":"Security Scripts"},{"location":"architecture/security/#kali-linux","text":"","title":"Kali Linux"},{"location":"architecture/virtualization/","text":"Virtualization Vagrant vagrant init ubuntu/bionic64 vagrant box add centos/8 vagrant status vagrant up vagrant up <hostname> vagrant suspend stores state on RAM vagrant halt stores state on Disk vagrant destroy -f removes stored state vagrant ssh <hostname> vagrant ssh-config Boxes https://app.vagrantup.com/boxes/search Synced Folders \"By default, Vagrant shares your project directory (remember, that is the one with the Vagrantfile) to the /vagrant directory in your guest machine.\" Vagrantfile Vagrant.configure(\"3\") do |config| # create mongo VM config.vm.define \"mongo\" do |mongo| mongo.vm.box = \"bento/ubuntu-16.04\" mongo.vm.provider \"virtualbox\" do |vb| vb.memory = \"512\" end mongo.vm.network \"private_netowrk\", ip: \"10.8.0.20\" mongo.vm.provision \"file\", source: \"files/mongo.conf\", destination: \"~/mongod/conf\" # try provisioning with ansible instead of shell script mongo.vm.provision \"shell\", path: \"provisioners/install-mongo.sh\" end # create node VM config.vm.define \"node\" do |node| node.vm.box = \"bento/ubuntu-16.04\" node.vm.network \"fowarded_port\", guest: 3000, host:8000 node.vm.provider \"virtualbox\" do |vb| vb.memory = \"512\" end node.vm.network \"private_network\", ip: \"10.8.0.21\" # try provisioning with ansible instead of shell script node.vm.provision \"shell\", path: \"privisioners/install-node.sh\" end # creates 3 ubuntu VMs (5..7).each do |i| config.vm.define \"ubuntu#{i}\" do |ubuntu| ubuntu.vm.box = \"ubuntu/bionic64\" ubuntu.vm.network \"private_network\", ip: \"10.8.0.#{i}\" end end config.vm.provider \"virtualbox\" do |v| v.memory = 8192 v.cpus = 4 end end Containerization Docker docker exec docker events Docker Compose Binding the Docker Socket Running docker commands from inside a container Linux: -v /var/run/docker.sock:/var/run/docker.sock Windows: -v //var/run/docker.sock:/var/run/docker.sock nginx reverse proxy traefik reverse proxy Container Orchestration Kubernetes AKS Concepts Core Concepts Nodes : a single machine, cluster resource to store data, run jobs, maintain workloads and create network routes Node Pools : Control Plane : set of containers (can be multiple master nodes) that manages the cluster: contains controller manager, API server, etcd, scheduler, CoreDNS, etc. kubelet : kubernetes agent running on worker nodes kube-proxy : Container runtime : Kubetctl : CLI to configure kubernetes and deploy & manage applications Pods : can be comprised of multiple containers deployed on a single node, receives 1 IP Namespaces : Labels & Annotations : allows to filter on related resources in the cluster Service : network endpoint to connect to a pod, 4 types: ClusterIP: only available within the cluster NodePort: high port allocated on each node to be reachable outside of the cluster LoadBalancer: controls an external load balancer endpoint external to the cluster ExternalName: adds CNAME DNS record to CoreDNS to allow pods to use a specified DNS name to reach a service outside of the cluster Service Discovery : custom DNS server that all Pods use to resolve names of other services, IPs and ports ReplicaSets : used to scale pods to a desired state and hold the current status for the system DaemonSets : used to deploy a single instance of an application on each node (e.g. log/metric exporter) StatefulSets : used to deploy applications that require the use of the same node and data persistence Jobs/CronJobs : used to deploy a container to complete a specific workload and be destory on successful completion ConfigMaps & Secrets : key-value environment variables to be passed to running workloads to determine different runtime behaviors, secrets are the envrypted version Deployments : a set of metadata to describe the requirements of a running workload Scheduler : ensures that if pods or nodes encounter problems, additional pods are ran on healthy nodes Storage : an abstraction layer to manage data persistency to outlast the lifetime of a pod CRDs : custom resource definitions Minikube Install minikube Install kubectl Create clutser minikube start (defaults to 2GB RAM, 2 CPUs, 20GB disk space) Bigger applications: minikube start --memory=8192 --cpu=4 --disk-size=50g Get the cluster IP: minikube ip Profiles minikube start -p <profile> --memory=8192 --cpu=4 --disk-size=50g Invoke a kubernetes service minikube service <service_name> Kube context and namespace kubectx kubens add new cluster context for kubectl: add <cluster_name>-kubeconfig.yaml to ~/.kube/configs/ add to .bash_profile or .bashrc: export KUBECONFIG=$KUBECONFIG:$HOME/.kube/config:$HOME/.kube/configs/<cluster_name>-kubeconfig.yaml source ~/.bash_profile Kubectl Cheatsheet kubectl version kubectl apply -f filename.yml : Watch pods: kubectl get pods -w kubectl get pods --all-namespaces kubectl get service kubectl get nodes kubectl get all Docs: kubectl explain services --recursive Filter: kubectl explain services.spec YAML Configuration Requirements: kind apiVersion metadata spec Example of a Service & Deployment defined in the same file kind: Service apiVersion: v1 metadata: name: app-nginx-service spec: type: NodePort ports: - port: 80 selector: app: app-nginx --- kind: Deployment apiVersion: apps/v1 metadata: name: app-nginx-deployment spec: replicas: 3 selector: matchLabels: app: app-nginx template: metadata: labels: app: app-nginx mycustomlabel: \"true\" spec: containers: - name: nginx image: nginx:stable Testing Manual deploy: docker run equivalent kubectl run my-app --image app:latest Cleanup/ docker rm equivalent: kubectl delete deployment my-app Manual scale: kubectl scale deploy/my-app --replicas 2 ( kubectl scale deployment my-app also works) Manually create service: kubectl expose Logs of single pod: kubectl logs deploy/my-app --folow --tail 1 Logs of up to 5 pods: kubectl logs deploy/my-app -l run=my-app Inspect a specific pod: kubectl descibe pod/my-app-74dh8h72d7-2j6kv kubectl create deployment sample --image my-app:latest --dry-run -o yaml Helm k3d Setup Install kubectl binary with curl on Linux: Download the latest release with the command: curl -LO \"https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl\" To download a specific version, replace the $(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt) portion of the command with the specific version.For example, to download version v1.19.0 on Linux, type:curl -LO https://storage.googleapis.com/kubernetes-release/release/v1.19.0/bin/linux/amd64/kubectl Make the kubectl binary executable: chmod +x ./kubectl Move the binary in to your PATH: sudo mv ./kubectl /usr/local/bin/kubectl Test to ensure the version you installed is up-to-date: kubectl version --client kube config: k3d kubeconfig merge -d -a kubectx & kubens: sudo apt install kubectx Logging kubectl -n <namespace> logs -f deployment/<app-name> --all-containers=true --since=10m Stern stern -n <namespace> <app-name> -t --since 10m Kail kail -n <namespace> -d <deployment> --label size=large kail -n <namespace> --svc <service> --ignore size=large --since=2h","title":"Virtualization"},{"location":"architecture/virtualization/#virtualization","text":"","title":"Virtualization"},{"location":"architecture/virtualization/#vagrant","text":"vagrant init ubuntu/bionic64 vagrant box add centos/8 vagrant status vagrant up vagrant up <hostname> vagrant suspend stores state on RAM vagrant halt stores state on Disk vagrant destroy -f removes stored state vagrant ssh <hostname> vagrant ssh-config","title":"Vagrant"},{"location":"architecture/virtualization/#boxes","text":"https://app.vagrantup.com/boxes/search","title":"Boxes"},{"location":"architecture/virtualization/#synced-folders","text":"\"By default, Vagrant shares your project directory (remember, that is the one with the Vagrantfile) to the /vagrant directory in your guest machine.\"","title":"Synced Folders"},{"location":"architecture/virtualization/#vagrantfile","text":"Vagrant.configure(\"3\") do |config| # create mongo VM config.vm.define \"mongo\" do |mongo| mongo.vm.box = \"bento/ubuntu-16.04\" mongo.vm.provider \"virtualbox\" do |vb| vb.memory = \"512\" end mongo.vm.network \"private_netowrk\", ip: \"10.8.0.20\" mongo.vm.provision \"file\", source: \"files/mongo.conf\", destination: \"~/mongod/conf\" # try provisioning with ansible instead of shell script mongo.vm.provision \"shell\", path: \"provisioners/install-mongo.sh\" end # create node VM config.vm.define \"node\" do |node| node.vm.box = \"bento/ubuntu-16.04\" node.vm.network \"fowarded_port\", guest: 3000, host:8000 node.vm.provider \"virtualbox\" do |vb| vb.memory = \"512\" end node.vm.network \"private_network\", ip: \"10.8.0.21\" # try provisioning with ansible instead of shell script node.vm.provision \"shell\", path: \"privisioners/install-node.sh\" end # creates 3 ubuntu VMs (5..7).each do |i| config.vm.define \"ubuntu#{i}\" do |ubuntu| ubuntu.vm.box = \"ubuntu/bionic64\" ubuntu.vm.network \"private_network\", ip: \"10.8.0.#{i}\" end end config.vm.provider \"virtualbox\" do |v| v.memory = 8192 v.cpus = 4 end end","title":"Vagrantfile"},{"location":"architecture/virtualization/#containerization","text":"","title":"Containerization"},{"location":"architecture/virtualization/#docker","text":"docker exec docker events","title":"Docker"},{"location":"architecture/virtualization/#docker-compose","text":"","title":"Docker Compose"},{"location":"architecture/virtualization/#binding-the-docker-socket","text":"Running docker commands from inside a container Linux: -v /var/run/docker.sock:/var/run/docker.sock Windows: -v //var/run/docker.sock:/var/run/docker.sock","title":"Binding the Docker Socket"},{"location":"architecture/virtualization/#nginx-reverse-proxy","text":"","title":"nginx reverse proxy"},{"location":"architecture/virtualization/#traefik-reverse-proxy","text":"","title":"traefik reverse proxy"},{"location":"architecture/virtualization/#container-orchestration","text":"","title":"Container Orchestration"},{"location":"architecture/virtualization/#kubernetes","text":"AKS Concepts","title":"Kubernetes"},{"location":"architecture/virtualization/#core-concepts","text":"Nodes : a single machine, cluster resource to store data, run jobs, maintain workloads and create network routes Node Pools : Control Plane : set of containers (can be multiple master nodes) that manages the cluster: contains controller manager, API server, etcd, scheduler, CoreDNS, etc. kubelet : kubernetes agent running on worker nodes kube-proxy : Container runtime : Kubetctl : CLI to configure kubernetes and deploy & manage applications Pods : can be comprised of multiple containers deployed on a single node, receives 1 IP Namespaces : Labels & Annotations : allows to filter on related resources in the cluster Service : network endpoint to connect to a pod, 4 types: ClusterIP: only available within the cluster NodePort: high port allocated on each node to be reachable outside of the cluster LoadBalancer: controls an external load balancer endpoint external to the cluster ExternalName: adds CNAME DNS record to CoreDNS to allow pods to use a specified DNS name to reach a service outside of the cluster Service Discovery : custom DNS server that all Pods use to resolve names of other services, IPs and ports ReplicaSets : used to scale pods to a desired state and hold the current status for the system DaemonSets : used to deploy a single instance of an application on each node (e.g. log/metric exporter) StatefulSets : used to deploy applications that require the use of the same node and data persistence Jobs/CronJobs : used to deploy a container to complete a specific workload and be destory on successful completion ConfigMaps & Secrets : key-value environment variables to be passed to running workloads to determine different runtime behaviors, secrets are the envrypted version Deployments : a set of metadata to describe the requirements of a running workload Scheduler : ensures that if pods or nodes encounter problems, additional pods are ran on healthy nodes Storage : an abstraction layer to manage data persistency to outlast the lifetime of a pod CRDs : custom resource definitions","title":"Core Concepts"},{"location":"architecture/virtualization/#minikube","text":"Install minikube Install kubectl","title":"Minikube"},{"location":"architecture/virtualization/#create-clutser","text":"minikube start (defaults to 2GB RAM, 2 CPUs, 20GB disk space) Bigger applications: minikube start --memory=8192 --cpu=4 --disk-size=50g Get the cluster IP: minikube ip","title":"Create clutser"},{"location":"architecture/virtualization/#profiles","text":"minikube start -p <profile> --memory=8192 --cpu=4 --disk-size=50g","title":"Profiles"},{"location":"architecture/virtualization/#invoke-a-kubernetes-service","text":"minikube service <service_name>","title":"Invoke a kubernetes service"},{"location":"architecture/virtualization/#kube-context-and-namespace","text":"kubectx kubens add new cluster context for kubectl: add <cluster_name>-kubeconfig.yaml to ~/.kube/configs/ add to .bash_profile or .bashrc: export KUBECONFIG=$KUBECONFIG:$HOME/.kube/config:$HOME/.kube/configs/<cluster_name>-kubeconfig.yaml source ~/.bash_profile","title":"Kube context and namespace"},{"location":"architecture/virtualization/#kubectl","text":"Cheatsheet kubectl version kubectl apply -f filename.yml : Watch pods: kubectl get pods -w kubectl get pods --all-namespaces kubectl get service kubectl get nodes kubectl get all Docs: kubectl explain services --recursive Filter: kubectl explain services.spec","title":"Kubectl"},{"location":"architecture/virtualization/#yaml-configuration","text":"Requirements: kind apiVersion metadata spec","title":"YAML Configuration"},{"location":"architecture/virtualization/#example-of-a-service-deployment-defined-in-the-same-file","text":"kind: Service apiVersion: v1 metadata: name: app-nginx-service spec: type: NodePort ports: - port: 80 selector: app: app-nginx --- kind: Deployment apiVersion: apps/v1 metadata: name: app-nginx-deployment spec: replicas: 3 selector: matchLabels: app: app-nginx template: metadata: labels: app: app-nginx mycustomlabel: \"true\" spec: containers: - name: nginx image: nginx:stable","title":"Example of a Service &amp; Deployment defined in the same file"},{"location":"architecture/virtualization/#testing","text":"Manual deploy: docker run equivalent kubectl run my-app --image app:latest Cleanup/ docker rm equivalent: kubectl delete deployment my-app Manual scale: kubectl scale deploy/my-app --replicas 2 ( kubectl scale deployment my-app also works) Manually create service: kubectl expose Logs of single pod: kubectl logs deploy/my-app --folow --tail 1 Logs of up to 5 pods: kubectl logs deploy/my-app -l run=my-app Inspect a specific pod: kubectl descibe pod/my-app-74dh8h72d7-2j6kv kubectl create deployment sample --image my-app:latest --dry-run -o yaml","title":"Testing"},{"location":"architecture/virtualization/#helm","text":"","title":"Helm"},{"location":"architecture/virtualization/#k3d-setup","text":"Install kubectl binary with curl on Linux: Download the latest release with the command: curl -LO \"https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl\" To download a specific version, replace the $(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt) portion of the command with the specific version.For example, to download version v1.19.0 on Linux, type:curl -LO https://storage.googleapis.com/kubernetes-release/release/v1.19.0/bin/linux/amd64/kubectl Make the kubectl binary executable: chmod +x ./kubectl Move the binary in to your PATH: sudo mv ./kubectl /usr/local/bin/kubectl Test to ensure the version you installed is up-to-date: kubectl version --client kube config: k3d kubeconfig merge -d -a kubectx & kubens: sudo apt install kubectx","title":"k3d Setup"},{"location":"architecture/virtualization/#logging","text":"kubectl -n <namespace> logs -f deployment/<app-name> --all-containers=true --since=10m","title":"Logging"},{"location":"architecture/virtualization/#stern","text":"stern -n <namespace> <app-name> -t --since 10m","title":"Stern"},{"location":"architecture/virtualization/#kail","text":"kail -n <namespace> -d <deployment> --label size=large kail -n <namespace> --svc <service> --ignore size=large --since=2h","title":"Kail"},{"location":"data/analysis/","text":"Data Analysis NumPy Pandas Notes Import pandas as pd df = pd.read_csv(\u201cfile.csv\u201d) Outputs: (rows, columns) df.shape Outputs: first/last 5 rows & header df.head(5) or df.tail Check for null values df.isnull().values.any() Data fram correlation function df.corr() Delete column from data frame del df(\u2018column_name\u2019) Mold data (change True/False values to 1 or 0) column_map = { True: 1, False: 0 } df[\u2018column_name\u2019] = df[\u2018column_name\u2019].map(column_map) Creating column ratios (True/False ratio) num_true = len(df.loc[df[\u2018column_name\u2019] == True]) num_false = len(df.loc[df[\u2018column_name\u2019] == False]) Count: len(dataframe) Count group by distinct: df[COLUMN.value_counts() Count null values df['COLUMN'].isna().sum() len(df) - df['COLUMN'].count() Count distict values, use nunique: df['hID'].nunique() Count only non-null values, use count: df['hID'].count() Count total values including null values, use size attribute: df['hID'].size https://davidhamann.de/2017/06/26/pandas-select-elements-by-string/ https://proview-demo.nonprod.caqh.org/DirectAssure/api/POPracticeLocation/MatchReport/6053_20190606_144049 \"If you plan on using pandas and are just getting into it, I would highly recommend how to use df.groupby().apply() (ie split-apply-combine)and df.loc[df.loc[],:] effectively as I have found those to be the two most impactful things in the quality of my pandas programming. basically my rule for pandas is: if you are using loops there's a good chance there is a more efficient/easier way to accomplish it. And using df.apply() with custom lambda functions. They have been so amazing.\" Visualization","title":"Data Analysis"},{"location":"data/analysis/#data-analysis","text":"","title":"Data Analysis"},{"location":"data/analysis/#numpy","text":"","title":"NumPy"},{"location":"data/analysis/#pandas","text":"","title":"Pandas"},{"location":"data/analysis/#notes","text":"Import pandas as pd df = pd.read_csv(\u201cfile.csv\u201d) Outputs: (rows, columns) df.shape Outputs: first/last 5 rows & header df.head(5) or df.tail Check for null values df.isnull().values.any() Data fram correlation function df.corr() Delete column from data frame del df(\u2018column_name\u2019) Mold data (change True/False values to 1 or 0) column_map = { True: 1, False: 0 } df[\u2018column_name\u2019] = df[\u2018column_name\u2019].map(column_map) Creating column ratios (True/False ratio) num_true = len(df.loc[df[\u2018column_name\u2019] == True]) num_false = len(df.loc[df[\u2018column_name\u2019] == False]) Count: len(dataframe) Count group by distinct: df[COLUMN.value_counts() Count null values df['COLUMN'].isna().sum() len(df) - df['COLUMN'].count() Count distict values, use nunique: df['hID'].nunique() Count only non-null values, use count: df['hID'].count() Count total values including null values, use size attribute: df['hID'].size https://davidhamann.de/2017/06/26/pandas-select-elements-by-string/ https://proview-demo.nonprod.caqh.org/DirectAssure/api/POPracticeLocation/MatchReport/6053_20190606_144049 \"If you plan on using pandas and are just getting into it, I would highly recommend how to use df.groupby().apply() (ie split-apply-combine)and df.loc[df.loc[],:] effectively as I have found those to be the two most impactful things in the quality of my pandas programming. basically my rule for pandas is: if you are using loops there's a good chance there is a more efficient/easier way to accomplish it. And using df.apply() with custom lambda functions. They have been so amazing.\"","title":"Notes"},{"location":"data/analysis/#visualization","text":"","title":"Visualization"},{"location":"data/big_data/","text":"Big Data Cloudera Impala hive databases not showing up? INVALIDATE METADATA; Is impala returning 0 results for a parquet table it should be able to read? Refresh it. REFRESH db_name.table search for string SELECT COUNT(*) FROM cpsc_db.cpsc_premium WHERE INSTR(organization_name, \"SNP\") != 0; Hive built-in types https://support.treasuredata.com/hc/en-us/articles/360001450728-Hive-Built-in-Operators regex replace (remove non-ASCII chars) SELECT regexp_replace(org_plan_name, '\\\\P{ASCII}', '') FROM cpsc_stage_db.cpsc_contract WHERE year=2019; DESCRIBE PARTITION desc formatted dbname.tablename partition (name=value) ADD PARTITION hive (cpsc_stage_db)> alter table cpsc_contract add partition(year=2019, month=02) > location '/data/domain/cpsc/cpsc_stage_db/cpsc_contract/year=2019/month=02'; DROP PARTITION hive (cpsc_stage_db)> alter table cpsc_contract drop if exists partition(year=2019, month=02); NOTES: this removes the hdfs folder location of the partition data TRUNCATE TABLE TRUNCATE TABLE IF EXISTS $tablename SHELL SCRIPTING hive -e (query) hive -f (file) Skip reading header line when creating hive table from CSV TBLPROPERTIES ( 'skip.header.line.count'='1' ); Creating Hive table from CSV PARTITIONED BY ( year int ) ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde' WITH SERDEPROPERTIES ( \"separatorChar\"=\",\", \"quoteChar\"=\"\\\"\") STORED AS TEXTFILE Create partitioned Hive table Create HDFS dirs (/table/year=2019/month=01) hdfs dfs -put (the csvs to the hdfs dirs) ALTER TABLE with each partition and LOCATION '/hdfs/path/to/new/dirs/' Spark PySpark sql_c = SQLContext(sc) df = sql_c.read.csv('HDFS PATH') df.printSchema() df.show() df.write.csv('HDFS PATH/mycsv.csv') remove non-ascii chars re.sub(r'[^\\x00-\\x7f]',r'', 'hi \u00bb') MLib Oozie Sentry","title":"Big Data"},{"location":"data/big_data/#big-data","text":"","title":"Big Data"},{"location":"data/big_data/#cloudera","text":"","title":"Cloudera"},{"location":"data/big_data/#impala","text":"hive databases not showing up? INVALIDATE METADATA; Is impala returning 0 results for a parquet table it should be able to read? Refresh it. REFRESH db_name.table search for string SELECT COUNT(*) FROM cpsc_db.cpsc_premium WHERE INSTR(organization_name, \"SNP\") != 0;","title":"Impala"},{"location":"data/big_data/#hive","text":"built-in types https://support.treasuredata.com/hc/en-us/articles/360001450728-Hive-Built-in-Operators","title":"Hive"},{"location":"data/big_data/#regex-replace-remove-non-ascii-chars","text":"SELECT regexp_replace(org_plan_name, '\\\\P{ASCII}', '') FROM cpsc_stage_db.cpsc_contract WHERE year=2019;","title":"regex replace (remove non-ASCII chars)"},{"location":"data/big_data/#describe-partition","text":"desc formatted dbname.tablename partition (name=value)","title":"DESCRIBE PARTITION"},{"location":"data/big_data/#add-partition","text":"hive (cpsc_stage_db)> alter table cpsc_contract add partition(year=2019, month=02) > location '/data/domain/cpsc/cpsc_stage_db/cpsc_contract/year=2019/month=02';","title":"ADD PARTITION"},{"location":"data/big_data/#drop-partition","text":"hive (cpsc_stage_db)> alter table cpsc_contract drop if exists partition(year=2019, month=02); NOTES: this removes the hdfs folder location of the partition data","title":"DROP PARTITION"},{"location":"data/big_data/#truncate-table","text":"TRUNCATE TABLE IF EXISTS $tablename","title":"TRUNCATE TABLE"},{"location":"data/big_data/#shell-scripting","text":"hive -e (query) hive -f (file)","title":"SHELL SCRIPTING"},{"location":"data/big_data/#skip-reading-header-line-when-creating-hive-table-from-csv","text":"TBLPROPERTIES ( 'skip.header.line.count'='1' );","title":"Skip reading header line when creating hive table from CSV"},{"location":"data/big_data/#creating-hive-table-from-csv","text":"PARTITIONED BY ( year int ) ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde' WITH SERDEPROPERTIES ( \"separatorChar\"=\",\", \"quoteChar\"=\"\\\"\") STORED AS TEXTFILE","title":"Creating Hive table from CSV"},{"location":"data/big_data/#create-partitioned-hive-table","text":"Create HDFS dirs (/table/year=2019/month=01) hdfs dfs -put (the csvs to the hdfs dirs) ALTER TABLE with each partition and LOCATION '/hdfs/path/to/new/dirs/'","title":"Create partitioned Hive table"},{"location":"data/big_data/#spark","text":"","title":"Spark"},{"location":"data/big_data/#pyspark","text":"sql_c = SQLContext(sc) df = sql_c.read.csv('HDFS PATH') df.printSchema() df.show() df.write.csv('HDFS PATH/mycsv.csv') remove non-ascii chars re.sub(r'[^\\x00-\\x7f]',r'', 'hi \u00bb')","title":"PySpark"},{"location":"data/big_data/#mlib","text":"","title":"MLib"},{"location":"data/big_data/#oozie","text":"","title":"Oozie"},{"location":"data/big_data/#sentry","text":"","title":"Sentry"},{"location":"data/machine_learning/","text":"Machine Learning Shallow Learning Scikit-learn Deep Learning Tensor Flow","title":"Machine Learning"},{"location":"data/machine_learning/#machine-learning","text":"","title":"Machine Learning"},{"location":"data/machine_learning/#shallow-learning","text":"","title":"Shallow Learning"},{"location":"data/machine_learning/#scikit-learn","text":"","title":"Scikit-learn"},{"location":"data/machine_learning/#deep-learning","text":"","title":"Deep Learning"},{"location":"data/machine_learning/#tensor-flow","text":"","title":"Tensor Flow"},{"location":"data/pipelines/","text":"Data Pipelines facilitate the efficient flow of data from one point to another flow of data extraction, transformation, combination, validation, converging of data from multiple streams into one facilitate parallel processing using distributed data processing data standardization (cleanup beforehand to avoid processing errors) data processing (segregate data into flows based on business requirements & route it to specific destinations) data analysis (models and machine learning) data visualization (present gathered intel to stakeholders) data storage & security (archive and encrypt data) Data Ingestion Data collection layer Data query layer Data processing layer Data visualization layer Data storage layer Data security layer Real-time streams important for time-critical data (medical, financial, etc.) can be slow and require smaller data sets Apache Spark, Storm, Kafka, Flink are high-performant due to in-memory storage abd examples of distributed data processing engines capable of processing real-time streaming data Spark integrates seemlessly with HDFS, S3, etc. Batch analyzing trends over time can include full data sets due to it not being time-critical ETL flow Hadoop implementing MapReduce is lower performant due to disk usage and is used in batch processing data Use Cases momving big data into a Hadoop cluster to process and run analytics moving data (and continuing to update) from the primary storage / legacy systems into Elasticsearch to be indexed and searched on more quickly processing and aggregating logs and metrics from servers, services, and IoT devices into Elasticsearch or other time-series databases to be visualized and run analytics on them real-time stream processing engines for real-time events (sports, financial, medical, etc.) to dump into message queues such as Kafka or stream computation frameworks (Storm, Nifi, Spark, Flink, Samza, Kinesis, etc.) to implement real-time large-scale data processing features","title":"Data Pipelines"},{"location":"data/pipelines/#data-pipelines","text":"facilitate the efficient flow of data from one point to another flow of data extraction, transformation, combination, validation, converging of data from multiple streams into one facilitate parallel processing using distributed data processing data standardization (cleanup beforehand to avoid processing errors) data processing (segregate data into flows based on business requirements & route it to specific destinations) data analysis (models and machine learning) data visualization (present gathered intel to stakeholders) data storage & security (archive and encrypt data)","title":"Data Pipelines"},{"location":"data/pipelines/#data-ingestion","text":"Data collection layer Data query layer Data processing layer Data visualization layer Data storage layer Data security layer","title":"Data Ingestion"},{"location":"data/pipelines/#real-time-streams","text":"important for time-critical data (medical, financial, etc.) can be slow and require smaller data sets Apache Spark, Storm, Kafka, Flink are high-performant due to in-memory storage abd examples of distributed data processing engines capable of processing real-time streaming data Spark integrates seemlessly with HDFS, S3, etc.","title":"Real-time streams"},{"location":"data/pipelines/#batch","text":"analyzing trends over time can include full data sets due to it not being time-critical ETL flow Hadoop implementing MapReduce is lower performant due to disk usage and is used in batch processing data","title":"Batch"},{"location":"data/pipelines/#use-cases","text":"momving big data into a Hadoop cluster to process and run analytics moving data (and continuing to update) from the primary storage / legacy systems into Elasticsearch to be indexed and searched on more quickly processing and aggregating logs and metrics from servers, services, and IoT devices into Elasticsearch or other time-series databases to be visualized and run analytics on them real-time stream processing engines for real-time events (sports, financial, medical, etc.) to dump into message queues such as Kafka or stream computation frameworks (Storm, Nifi, Spark, Flink, Samza, Kinesis, etc.) to implement real-time large-scale data processing features","title":"Use Cases"},{"location":"data/storage/","text":"Data Storage Relational Databases PostgreSQL Command Action \\l List available databases \\c dbname Connect to a new database \\dt List available tables \\d tablename Describe the details of given table \\dn List all schemas in the current database \\df List functions in the current database \\h Get help on syntax of SQL commands \\? Lists all psql slash commands \\set System variables list \\timing Shows how long a query took to execute \\x Show expanded query results \\q Quit psql MySQL Left Join (e.g. get admin user data across multiple tables): SELECT u.username, u.first_name, u.last_name, p.can_access_cms, p.can_impersonate_user, c.name, d.name FROM schema.users_user u LEFT JOIN schema.users_userpermissions p ON u.id = p.user_id LEFT JOIN schema.clubs_club_admins ca ON u.id = ca.user_id LEFT JOIN schema.clubs_club c ON ca.club_id = c.id LEFT JOIN schema.clubs_department_admins da ON u.id = da.user_id LEFT JOIN schema.clubs_department d ON da.department_id = d.id WHERE is_superuser = 1; Concatonate multiple rows into one field e.g. combine rows that list different values of a field for the same (user) id SELECT person_id, GROUP_CONCAT(jobs SEPARATOR ', ') FROM jobs GROUP BY person_id; Rename values e.g. Display Yes or No for a binary field SELECT 'worker_id', IF('has_background_check' = 1, 'Yes', 'No') AS 'background_check' FROM workers; SQLite Oracle SQL Server NoSQL Databases MongoDB Distributed Data Storage HDFS reread Clouder_notes doc I made during THP for HDFS & Security MGMT and Big Data ETLs Command Commands hdfs dfs \u2013ls [hdfs path] hdfs dfs -mkdir [hdfs path] hdfs dfs -rm -r [hdfs path] hdfs dfs -put [local path] [hdfs path] hdfs dfs -get [hdfs path] hdfs dfs -getfacl [hdfs path] hdfs dfs -setfacl [group:name:rwx] [hdfs path] Hbase Cassandra Message Queue ZeroMQ The Official Guide great read for anyone working with distributed systems","title":"Data Storage"},{"location":"data/storage/#data-storage","text":"","title":"Data Storage"},{"location":"data/storage/#relational-databases","text":"","title":"Relational Databases"},{"location":"data/storage/#postgresql","text":"Command Action \\l List available databases \\c dbname Connect to a new database \\dt List available tables \\d tablename Describe the details of given table \\dn List all schemas in the current database \\df List functions in the current database \\h Get help on syntax of SQL commands \\? Lists all psql slash commands \\set System variables list \\timing Shows how long a query took to execute \\x Show expanded query results \\q Quit psql","title":"PostgreSQL"},{"location":"data/storage/#mysql","text":"","title":"MySQL"},{"location":"data/storage/#left-join-eg-get-admin-user-data-across-multiple-tables","text":"SELECT u.username, u.first_name, u.last_name, p.can_access_cms, p.can_impersonate_user, c.name, d.name FROM schema.users_user u LEFT JOIN schema.users_userpermissions p ON u.id = p.user_id LEFT JOIN schema.clubs_club_admins ca ON u.id = ca.user_id LEFT JOIN schema.clubs_club c ON ca.club_id = c.id LEFT JOIN schema.clubs_department_admins da ON u.id = da.user_id LEFT JOIN schema.clubs_department d ON da.department_id = d.id WHERE is_superuser = 1;","title":"Left Join (e.g. get admin user data across multiple tables):"},{"location":"data/storage/#concatonate-multiple-rows-into-one-field","text":"e.g. combine rows that list different values of a field for the same (user) id SELECT person_id, GROUP_CONCAT(jobs SEPARATOR ', ') FROM jobs GROUP BY person_id;","title":"Concatonate multiple rows into one field"},{"location":"data/storage/#rename-values","text":"e.g. Display Yes or No for a binary field SELECT 'worker_id', IF('has_background_check' = 1, 'Yes', 'No') AS 'background_check' FROM workers;","title":"Rename values"},{"location":"data/storage/#sqlite","text":"","title":"SQLite"},{"location":"data/storage/#oracle","text":"","title":"Oracle"},{"location":"data/storage/#sql-server","text":"","title":"SQL Server"},{"location":"data/storage/#nosql-databases","text":"","title":"NoSQL Databases"},{"location":"data/storage/#mongodb","text":"","title":"MongoDB"},{"location":"data/storage/#distributed-data-storage","text":"","title":"Distributed Data Storage"},{"location":"data/storage/#hdfs","text":"reread Clouder_notes doc I made during THP for HDFS & Security MGMT and Big Data ETLs","title":"HDFS"},{"location":"data/storage/#command-commands","text":"hdfs dfs \u2013ls [hdfs path] hdfs dfs -mkdir [hdfs path] hdfs dfs -rm -r [hdfs path] hdfs dfs -put [local path] [hdfs path] hdfs dfs -get [hdfs path] hdfs dfs -getfacl [hdfs path] hdfs dfs -setfacl [group:name:rwx] [hdfs path]","title":"Command Commands"},{"location":"data/storage/#hbase","text":"","title":"Hbase"},{"location":"data/storage/#cassandra","text":"","title":"Cassandra"},{"location":"data/storage/#message-queue","text":"","title":"Message Queue"},{"location":"data/storage/#zeromq","text":"The Official Guide great read for anyone working with distributed systems","title":"ZeroMQ"},{"location":"math/calculus/","text":"Calculus","title":"Calculus"},{"location":"math/calculus/#calculus","text":"","title":"Calculus"},{"location":"math/linear_algebra/","text":"Linear Algebra","title":"Linear Algebra"},{"location":"math/linear_algebra/#linear-algebra","text":"","title":"Linear Algebra"},{"location":"math/statistics/","text":"Statistics","title":"Statistics"},{"location":"math/statistics/#statistics","text":"","title":"Statistics"},{"location":"software/algorithms/","text":"Algorithms algorithm categories how long does the algorithm run with 1,000 records and does it scale to 1 million? common sense estimation O(n): simple loops (1 to n) O(n^2): nested loops O(lg(n)): binary chop (halves the set it considers each time it starts the loop) O(nlg(n)): divide and conquer (partition input, work on partitions separetly, and then combine results) degrades to O(n^2) when fed sorted imput O(2^n): combinatoric (permutations) Patterns Brute Force enumerate all possible solutions, and check them for correctness Greedy Keep track of the base answer so far, in one pass through the input Hash Table Spend space to save time by using a hash map or sometimes just an array Sorting Recursion even if the algorithm does not create any data structures, the call stack will generate O(n) space complexity Dynamic Programming Graph BFS queue will find shortest path usually takes more memory than DFS DFS stack can easily be implemented with recursion","title":"Algorithms"},{"location":"software/algorithms/#algorithms","text":"algorithm categories how long does the algorithm run with 1,000 records and does it scale to 1 million? common sense estimation O(n): simple loops (1 to n) O(n^2): nested loops O(lg(n)): binary chop (halves the set it considers each time it starts the loop) O(nlg(n)): divide and conquer (partition input, work on partitions separetly, and then combine results) degrades to O(n^2) when fed sorted imput O(2^n): combinatoric (permutations)","title":"Algorithms"},{"location":"software/algorithms/#patterns","text":"Brute Force enumerate all possible solutions, and check them for correctness Greedy Keep track of the base answer so far, in one pass through the input Hash Table Spend space to save time by using a hash map or sometimes just an array","title":"Patterns"},{"location":"software/algorithms/#sorting","text":"","title":"Sorting"},{"location":"software/algorithms/#recursion","text":"even if the algorithm does not create any data structures, the call stack will generate O(n) space complexity","title":"Recursion"},{"location":"software/algorithms/#dynamic-programming","text":"","title":"Dynamic Programming"},{"location":"software/algorithms/#graph","text":"","title":"Graph"},{"location":"software/algorithms/#bfs","text":"queue will find shortest path usually takes more memory than DFS","title":"BFS"},{"location":"software/algorithms/#dfs","text":"stack can easily be implemented with recursion","title":"DFS"},{"location":"software/best_practices/","text":"Best Practices Program Composistion (Design/Architecture) From John DeNero (UC Berkeley) - Names: To a computer, names are arbitrary symbols: \"xegyawebpi\" and \"foo\" are just as meaningful as \"tally\" and \"denominator\". To humans, comprehensible names aid immensely in comprehending programs. Choose names for your functions and values that indicate their use, purpose, and meaning. See the lecture notes section on choosing names for more suggestions. - Functions: Functions are our primary mechanism for abstraction, and so each function should ideally have a single job that can be used throughout a program. When given the choice between calling a function or copying and pasting its body, strive to call the function and maintain abstraction in your program. See the lecture notes section on composing functions for more suggestions. - Purpose: Each line of code in a program should have a purpose. Statements should be removed if they no longer have any effect (perhaps because they were useful for a previous version of the program, but are no longer needed). Large blocks of unused code, even when turned into comments, are confusing to readers. Feel free to keep your old implementations in a separate file for your own use, but don't turn them in as your finished product. - Brevity: An idea expressed in four lines of code is often clearer than the same idea expressed in forty. You do not need to try to minimize the length of your program, but look for opportunities to reduce the size of your program substantially by reusing functions you have already defined. Programmer Competency Matrix Competency Matrix Manage Growth of Requirements track scope creep communicate expectations continiously maintain a project glossary that defines all terms and how they are used in the project go the extra mile and deliver quality of life features Tracer Bullets prototype with a real and simplified version of the project it will contain all of the structure, error checking, documentation, and self-checking that production code has, it will not be fully-functional however once an end-to-end connection among components of the system is achieved, testing with users can begin adding additional functionality afterwards becomes straightforward Temporal Coupling allow for concurrency of tasks not dependent on each other reduces time-based dependency and increase flexibility Dynamic Configuration allow for the system to be highly configurable with metadata from screen colors and prompt text to algorithms, database and UI style Testing Unit Tests shows examples of how to use all the functionality of a module provides a means to build regressions tests to validate further changes to the code test subcomponents of a module first before testing the entire module use test coverage most importantly test every state of the program rather than every line of code x / (0 to 999) only has two states - testing 0 will cause an error while all 999 other tests will not Test Harness standard way to specify setup and cleanup method for selecting indivdual tests or all available tests means of analyzing output for expected or unexpected results standardized form of failure reporting and logging status xUnit (Kent Beck & Erich Gamma) Integration Tests shows that major subsystems sthat make up the project work with each other End-to-end tests does it meet the functional requirements of the system Performance testings stress test with real-world conditions (number of users/connections/transactions per second) how well does it scale? test resource exhaustion, errors, and recovery Usability testing sit down with the end user who will use the system to understand their tasks/goals that the system is trying to help them accomplish record requirements, use cases and scenarios Alistair Cockburn template: characteristic information (goal, scope, level, preconditions, success/failed end condition, primary actor, trigger) main success scenario extensions variations related information (priority, performance target, frequency, superordinate/subordinate use cases, channel to primary actor, secondary actors) schedule open issues TDD BDD DDD Testing in Production the final release is sometimes compiled/configured differently from earlier versions and should be tested all over again Testing Tests introduce bugs and ensure that the tests complain about them catch bugs once and then add them into the tests, do not let them reappear uncaught Refactoring don't refactor and add functionality simultaneously ensure that good tests are present and run them as often as possible take short, deliberate steps and test after each change DRY Code Source Control Management Documentation & Comments comments should discuss why something is done, the code already shows how documentation is another view of the same underlying model incorporate it into the development process, don't let it become an afterthought automate html documentation using code generators API Naming Conventions Code Generators Passive templates for creating new source files performing one-off conversions from one language to another producing lookup tables and other resources that are expensive to compute at runtime Active adhearing to the DRY principle allows for a single model to produce multiple views getting two disparate environments to work together can benefit greatly from an active code generator e.g. a schema ran through a code generator to produce both the software code and documentation in html Estimating check requirements analyze risk design, implement, integrate validate with users iterate understand what is being asked understand the scope (assuming these conditions, then...) ask someone else who has done something similar before come back with an answer after making calculations estimate based on a simplified model of the problem the simplified model will inevitably introduce inaccuracies doubling the effort on refining the model might only give a slight increase to accuracy however break the model into components identify their parameters that affect how the component contributes to the overall model assign values to the parameters work out which ones have the most impact on the result and concentrate on getting those right calculate the answers multiple times with varying parameter values hedge the final answer based on conditions the units used make a difference in the interpretation of the result 1-15 days: days 3-8 weeks: weeks 8-30 weeks: months 30+ weeks: think hard before delivering an estimate go back and evaluate the estimate with the reality","title":"Best Practices"},{"location":"software/best_practices/#best-practices","text":"","title":"Best Practices"},{"location":"software/best_practices/#program-composistion-designarchitecture","text":"From John DeNero (UC Berkeley) - Names: To a computer, names are arbitrary symbols: \"xegyawebpi\" and \"foo\" are just as meaningful as \"tally\" and \"denominator\". To humans, comprehensible names aid immensely in comprehending programs. Choose names for your functions and values that indicate their use, purpose, and meaning. See the lecture notes section on choosing names for more suggestions. - Functions: Functions are our primary mechanism for abstraction, and so each function should ideally have a single job that can be used throughout a program. When given the choice between calling a function or copying and pasting its body, strive to call the function and maintain abstraction in your program. See the lecture notes section on composing functions for more suggestions. - Purpose: Each line of code in a program should have a purpose. Statements should be removed if they no longer have any effect (perhaps because they were useful for a previous version of the program, but are no longer needed). Large blocks of unused code, even when turned into comments, are confusing to readers. Feel free to keep your old implementations in a separate file for your own use, but don't turn them in as your finished product. - Brevity: An idea expressed in four lines of code is often clearer than the same idea expressed in forty. You do not need to try to minimize the length of your program, but look for opportunities to reduce the size of your program substantially by reusing functions you have already defined.","title":"Program Composistion (Design/Architecture)"},{"location":"software/best_practices/#programmer-competency-matrix","text":"Competency Matrix","title":"Programmer Competency Matrix"},{"location":"software/best_practices/#manage-growth-of-requirements","text":"track scope creep communicate expectations continiously maintain a project glossary that defines all terms and how they are used in the project go the extra mile and deliver quality of life features","title":"Manage Growth of Requirements"},{"location":"software/best_practices/#tracer-bullets","text":"prototype with a real and simplified version of the project it will contain all of the structure, error checking, documentation, and self-checking that production code has, it will not be fully-functional however once an end-to-end connection among components of the system is achieved, testing with users can begin adding additional functionality afterwards becomes straightforward","title":"Tracer Bullets"},{"location":"software/best_practices/#temporal-coupling","text":"allow for concurrency of tasks not dependent on each other reduces time-based dependency and increase flexibility","title":"Temporal Coupling"},{"location":"software/best_practices/#dynamic-configuration","text":"allow for the system to be highly configurable with metadata from screen colors and prompt text to algorithms, database and UI style","title":"Dynamic Configuration"},{"location":"software/best_practices/#testing","text":"","title":"Testing"},{"location":"software/best_practices/#unit-tests","text":"shows examples of how to use all the functionality of a module provides a means to build regressions tests to validate further changes to the code test subcomponents of a module first before testing the entire module use test coverage most importantly test every state of the program rather than every line of code x / (0 to 999) only has two states - testing 0 will cause an error while all 999 other tests will not","title":"Unit Tests"},{"location":"software/best_practices/#test-harness","text":"standard way to specify setup and cleanup method for selecting indivdual tests or all available tests means of analyzing output for expected or unexpected results standardized form of failure reporting and logging status xUnit (Kent Beck & Erich Gamma)","title":"Test Harness"},{"location":"software/best_practices/#integration-tests","text":"shows that major subsystems sthat make up the project work with each other","title":"Integration Tests"},{"location":"software/best_practices/#end-to-end-tests","text":"does it meet the functional requirements of the system","title":"End-to-end tests"},{"location":"software/best_practices/#performance-testings","text":"stress test with real-world conditions (number of users/connections/transactions per second) how well does it scale? test resource exhaustion, errors, and recovery","title":"Performance testings"},{"location":"software/best_practices/#usability-testing","text":"sit down with the end user who will use the system to understand their tasks/goals that the system is trying to help them accomplish record requirements, use cases and scenarios Alistair Cockburn template: characteristic information (goal, scope, level, preconditions, success/failed end condition, primary actor, trigger) main success scenario extensions variations related information (priority, performance target, frequency, superordinate/subordinate use cases, channel to primary actor, secondary actors) schedule open issues","title":"Usability testing"},{"location":"software/best_practices/#tdd","text":"","title":"TDD"},{"location":"software/best_practices/#bdd","text":"","title":"BDD"},{"location":"software/best_practices/#ddd","text":"","title":"DDD"},{"location":"software/best_practices/#testing-in-production","text":"the final release is sometimes compiled/configured differently from earlier versions and should be tested all over again","title":"Testing in Production"},{"location":"software/best_practices/#testing-tests","text":"introduce bugs and ensure that the tests complain about them catch bugs once and then add them into the tests, do not let them reappear uncaught","title":"Testing Tests"},{"location":"software/best_practices/#refactoring","text":"don't refactor and add functionality simultaneously ensure that good tests are present and run them as often as possible take short, deliberate steps and test after each change","title":"Refactoring"},{"location":"software/best_practices/#dry-code","text":"","title":"DRY Code"},{"location":"software/best_practices/#source-control-management","text":"","title":"Source Control Management"},{"location":"software/best_practices/#documentation-comments","text":"comments should discuss why something is done, the code already shows how documentation is another view of the same underlying model incorporate it into the development process, don't let it become an afterthought automate html documentation using code generators","title":"Documentation &amp; Comments"},{"location":"software/best_practices/#api-naming-conventions","text":"","title":"API Naming Conventions"},{"location":"software/best_practices/#code-generators","text":"","title":"Code Generators"},{"location":"software/best_practices/#passive","text":"templates for creating new source files performing one-off conversions from one language to another producing lookup tables and other resources that are expensive to compute at runtime","title":"Passive"},{"location":"software/best_practices/#active","text":"adhearing to the DRY principle allows for a single model to produce multiple views getting two disparate environments to work together can benefit greatly from an active code generator e.g. a schema ran through a code generator to produce both the software code and documentation in html","title":"Active"},{"location":"software/best_practices/#estimating","text":"check requirements analyze risk design, implement, integrate validate with users iterate understand what is being asked understand the scope (assuming these conditions, then...) ask someone else who has done something similar before come back with an answer after making calculations estimate based on a simplified model of the problem the simplified model will inevitably introduce inaccuracies doubling the effort on refining the model might only give a slight increase to accuracy however break the model into components identify their parameters that affect how the component contributes to the overall model assign values to the parameters work out which ones have the most impact on the result and concentrate on getting those right calculate the answers multiple times with varying parameter values hedge the final answer based on conditions the units used make a difference in the interpretation of the result 1-15 days: days 3-8 weeks: weeks 8-30 weeks: months 30+ weeks: think hard before delivering an estimate go back and evaluate the estimate with the reality","title":"Estimating"},{"location":"software/computer_science/","text":"Computer Science Logic Gates Computer Architecture Operating Systems Languages and Compilers Computer Netowrking Embedded Programming Blockchain Nand2Tetris Notes","title":"Computer Science"},{"location":"software/computer_science/#computer-science","text":"","title":"Computer Science"},{"location":"software/computer_science/#logic-gates","text":"","title":"Logic Gates"},{"location":"software/computer_science/#computer-architecture","text":"","title":"Computer Architecture"},{"location":"software/computer_science/#operating-systems","text":"","title":"Operating Systems"},{"location":"software/computer_science/#languages-and-compilers","text":"","title":"Languages and Compilers"},{"location":"software/computer_science/#computer-netowrking","text":"","title":"Computer Netowrking"},{"location":"software/computer_science/#embedded-programming","text":"","title":"Embedded Programming"},{"location":"software/computer_science/#blockchain","text":"","title":"Blockchain"},{"location":"software/computer_science/#nand2tetris-notes","text":"","title":"Nand2Tetris Notes"},{"location":"software/data_structures/","text":"Data Structures Strings Arrays Strenghts: fast lookups fast appends cache-friendly Weaknesses: fixed size costly inserts and deletes space -> O(n) lookup -> O(1) append -> O(1) insert -> O(n) delete -> O(n) Static fast lookups O(1), but each item in the array needs to be the same size, and you need a big block of uninterrupted free memory to store the array pointer-based arrays are not reflected in time cost, even though: it's slower because it's not CPU cache-friendly (continious in RAM) pointer-based array requires less uninterrupted memory and can accommodate elements that aren't all the same size Dynamic don't have to specify the size ahead of time, but the disadvantage is that some appends can be expensive Strenghts: fast lookups variable size cache-friendly Weaknesses: slow worst-case appends costly inserts and deletes space -> O(n) lookup -> O(1) append -> O(1) insert -> O(n) delete -> O(n) Linked Lists have faster prepends and faster appends than dynamic arrays O(1), but they have slower lookups O(n) Hash Tables use hash function to translate key into an index handle hash collisions by having the value at the index be a pointer to a linked list with all keys that collide (considered rare enough with complicated balancing to still have O(1) lookups) Strenghts: fast lookups O(1) on average flexible keys allowing for most data types Weaknesses: slow worst-case lookups O(n) unordered keys single-directional lookups (O(n) key lookup) not cache-friendly (due to using linked lists) space -> O(n) lookup -> O(1) (Worst-case: O(n)) insert -> O(1) (Worst-case: O(n)) delete -> O(1) (Worst-case: O(n)) Sets similar implentation of hash maps but keys do not store associated values useful for tracking groups of items\u2014nodes visited in a graph, characters seen in a string, or colors used by neighboring nodes Trees Binary Tree a tree where each node has at most 2 children the number of total nodes on each level doubles as we move down the tree the number of nodes on the last level is equal to the sum of the number of nodes on all other levels (plus 1) total nodes = 2^h - 1 height = lg(n+1) Binary Search Tree (BST) ordered binary tree the nodes to the left are smaller than the current node the nodes to the right are larger than the current node Strengths: - good performance across the board - lookups O(lg(n)) - inserts O(lg(n)) - deletes O(lg(n)) - better worst-case performance than a hash table O(n) but not as good as its average case O(1) - if balanced: - sorted in O(n) time using an in-order traversal - finding an element closest to a value can be done in O(lg(n)) Weaknesses: - poor performance if unbalanced - no constant time O(1) operations AVL Red-Black Perfect Tree height = lg((n+1)/2) + 1 = log(n+1) Graphs Representing links. Graphs are ideal for cases where you're working with things that connect to other things. Nodes and edges could, for example, respectively represent cities and highways, routers and ethernet cables, or Facebook users and their friendships Scaling challenges. Most graph algorithms are O(n*lg(n))O(n\u2217lg(n)) or even slower. Depending on the size of your graph, running algorithms across your nodes may not be feasible. directed vs. undirected (links with direction) cyclic vs. acyclic (cyclic graphs contains at least one unbroken series of nodes with no repeating nodes or edges that connects back to itself) weighted vs. unweighted (each edge contains a weight) legal coloring vs. illegal coloring (no adjacent nodes have the same color)","title":"Data Structures"},{"location":"software/data_structures/#data-structures","text":"","title":"Data Structures"},{"location":"software/data_structures/#strings","text":"","title":"Strings"},{"location":"software/data_structures/#arrays","text":"Strenghts: fast lookups fast appends cache-friendly Weaknesses: fixed size costly inserts and deletes space -> O(n) lookup -> O(1) append -> O(1) insert -> O(n) delete -> O(n)","title":"Arrays"},{"location":"software/data_structures/#static","text":"fast lookups O(1), but each item in the array needs to be the same size, and you need a big block of uninterrupted free memory to store the array pointer-based arrays are not reflected in time cost, even though: it's slower because it's not CPU cache-friendly (continious in RAM) pointer-based array requires less uninterrupted memory and can accommodate elements that aren't all the same size","title":"Static"},{"location":"software/data_structures/#dynamic","text":"don't have to specify the size ahead of time, but the disadvantage is that some appends can be expensive Strenghts: fast lookups variable size cache-friendly Weaknesses: slow worst-case appends costly inserts and deletes space -> O(n) lookup -> O(1) append -> O(1) insert -> O(n) delete -> O(n)","title":"Dynamic"},{"location":"software/data_structures/#linked-lists","text":"have faster prepends and faster appends than dynamic arrays O(1), but they have slower lookups O(n)","title":"Linked Lists"},{"location":"software/data_structures/#hash-tables","text":"use hash function to translate key into an index handle hash collisions by having the value at the index be a pointer to a linked list with all keys that collide (considered rare enough with complicated balancing to still have O(1) lookups) Strenghts: fast lookups O(1) on average flexible keys allowing for most data types Weaknesses: slow worst-case lookups O(n) unordered keys single-directional lookups (O(n) key lookup) not cache-friendly (due to using linked lists) space -> O(n) lookup -> O(1) (Worst-case: O(n)) insert -> O(1) (Worst-case: O(n)) delete -> O(1) (Worst-case: O(n))","title":"Hash Tables"},{"location":"software/data_structures/#sets","text":"similar implentation of hash maps but keys do not store associated values useful for tracking groups of items\u2014nodes visited in a graph, characters seen in a string, or colors used by neighboring nodes","title":"Sets"},{"location":"software/data_structures/#trees","text":"","title":"Trees"},{"location":"software/data_structures/#binary-tree","text":"a tree where each node has at most 2 children the number of total nodes on each level doubles as we move down the tree the number of nodes on the last level is equal to the sum of the number of nodes on all other levels (plus 1) total nodes = 2^h - 1 height = lg(n+1)","title":"Binary Tree"},{"location":"software/data_structures/#binary-search-tree-bst","text":"ordered binary tree the nodes to the left are smaller than the current node the nodes to the right are larger than the current node Strengths: - good performance across the board - lookups O(lg(n)) - inserts O(lg(n)) - deletes O(lg(n)) - better worst-case performance than a hash table O(n) but not as good as its average case O(1) - if balanced: - sorted in O(n) time using an in-order traversal - finding an element closest to a value can be done in O(lg(n)) Weaknesses: - poor performance if unbalanced - no constant time O(1) operations","title":"Binary Search Tree (BST)"},{"location":"software/data_structures/#avl","text":"","title":"AVL"},{"location":"software/data_structures/#red-black","text":"","title":"Red-Black"},{"location":"software/data_structures/#perfect-tree","text":"height = lg((n+1)/2) + 1 = log(n+1)","title":"Perfect Tree"},{"location":"software/data_structures/#graphs","text":"Representing links. Graphs are ideal for cases where you're working with things that connect to other things. Nodes and edges could, for example, respectively represent cities and highways, routers and ethernet cables, or Facebook users and their friendships Scaling challenges. Most graph algorithms are O(n*lg(n))O(n\u2217lg(n)) or even slower. Depending on the size of your graph, running algorithms across your nodes may not be feasible. directed vs. undirected (links with direction) cyclic vs. acyclic (cyclic graphs contains at least one unbroken series of nodes with no repeating nodes or edges that connects back to itself) weighted vs. unweighted (each edge contains a weight) legal coloring vs. illegal coloring (no adjacent nodes have the same color)","title":"Graphs"},{"location":"software/design_patterns/","text":"Design Patterns Get Physical Copy of Design Patterns by the Gang of Four Pros & Cons of: - Singleton - Facade - Bridge - Strategy - Observer (PubSub) Factory Pattern","title":"Design Patterns"},{"location":"software/design_patterns/#design-patterns","text":"Get Physical Copy of Design Patterns by the Gang of Four Pros & Cons of: - Singleton - Facade - Bridge - Strategy - Observer (PubSub)","title":"Design Patterns"},{"location":"software/design_patterns/#factory-pattern","text":"","title":"Factory Pattern"},{"location":"software/functional_paradigm/","text":"Functional Paradigm Haskell","title":"Functional Paradigm"},{"location":"software/functional_paradigm/#functional-paradigm","text":"","title":"Functional Paradigm"},{"location":"software/functional_paradigm/#haskell","text":"","title":"Haskell"},{"location":"software/oop/","text":"Object Oriented Programming Encapsulation Abstraction Inheritance Polymorphism Java Install JDK Debian / Ubuntu apt-get install openjdk-11-jdk CentOS / Fedora / RHEL yum -y install java-11-openjdk java-11-openjdk-devel cat > /etc/profile.d/java11.sh <<EOF export JAVA_HOME=$(dirname $(dirname $(readlink $(readlink $(which javac))))) export PATH=\\$PATH:\\$JAVA_HOME/bin EOF source /etc/profile.d/java11.sh Windows OpenJDK Install page for JDK 11 OpenJDK Archive Download JDK Set PATH (e.g. add .../Java/jdk-11/bin to PATH ) Set JAVA_HOME (i.e. set to jdk installation path without /bin dir) Test with java --version command and configure the JDK in IntelliJ","title":"OOP"},{"location":"software/oop/#object-oriented-programming","text":"Encapsulation Abstraction Inheritance Polymorphism","title":"Object Oriented Programming"},{"location":"software/oop/#java","text":"","title":"Java"},{"location":"software/oop/#install-jdk","text":"","title":"Install JDK"},{"location":"software/oop/#debian-ubuntu","text":"apt-get install openjdk-11-jdk","title":"Debian / Ubuntu"},{"location":"software/oop/#centos-fedora-rhel","text":"yum -y install java-11-openjdk java-11-openjdk-devel cat > /etc/profile.d/java11.sh <<EOF export JAVA_HOME=$(dirname $(dirname $(readlink $(readlink $(which javac))))) export PATH=\\$PATH:\\$JAVA_HOME/bin EOF source /etc/profile.d/java11.sh","title":"CentOS / Fedora / RHEL"},{"location":"software/oop/#windows","text":"OpenJDK Install page for JDK 11 OpenJDK Archive Download JDK Set PATH (e.g. add .../Java/jdk-11/bin to PATH ) Set JAVA_HOME (i.e. set to jdk installation path without /bin dir) Test with java --version command and configure the JDK in IntelliJ","title":"Windows"},{"location":"software/python/","text":"Python Pyenv brew install pyenv brew install pyenv-virtualenv install version: pyenv instal [version] set version: pyenv global [version] list installed versions: pyenv versions list virtualenvs: pyenv virtualenvs activate virutalenv: pyenv activate Update .bashrc if command -v pyenv 1>/dev/null 2>&1; then eval \"$(pyenv init -)\" fi export PATH=\"$HOME/.pyenv/bin:$PATH\" Enumerate Iterable enumerate(iterable, start=0) for index, element in enumerate(l1,100): print index, element Higher-order functions From Composing Programs by John DeNero, Chapter 1.6.1: def summation(n, term): total, k = 0, 1 while k <= n: total, k = total + term(k), k + 1 return total def cube(x): return x*x*x def sum_cubes(n): return summation(n, cube) result = sum_cubes(3) Nest Definitions and Lexical Scope From Composing Programs by John DeNero, Chapter 1.6.3: def average(x, y): return (x + y)/2 def improve(update, close, guess=1): while not close(guess): guess = update(guess) return guess def approx_eq(x, y, tolerance=1e-3): return abs(x - y) < tolerance def sqrt(a): def sqrt_update(x): return average(x, a/x) def sqrt_close(x): return approx_eq(x * x, a) return improve(sqrt_update, sqrt_close) result = sqrt(256) Notice how the nested definition sqrt_update still has access to the a , which is a formal parameter of its enclosing function sqrt . The nested definition extends its parent environment. \"The sqrt_update function carries with it some data: the value for a referenced in the environment in which it was defined. Because they \"enclose\" information in this way, locally defined functions are often called closures .\" Decorators Using Decorators to time functions import time from functools import wraps def timethis(func): @wraps(func) def wrapper(*args, **kwargs): start = time.time() result = func(*args, **kwargs) end = time.time() print(func.__name__, end-start) return result return wrapper @timethis def square_world(world): list_len = int(len(world)**.5) new_list = [] for x in range(len(world)): if (x)%list_len == 0: new_list.append(world[x:x+list_len]) return new_list @timethis def square_world_modified(world): list_len = int(len(world)**.5) new_list = [] for x in range(0, len(world), list_len): section = world[x:x+list_len] new_list.append(section) def create_sample_world(size): world = [] for x in range(size): world.append(x) return world sample_worlds = (1000, 1000000, 10000000) for sample_size in sample_worlds: world = create_sample_world(sample_size) print(f'World size: {sample_size}') square_world(world) square_world_modified(world) print() Python Requests Lib Timeout timeout request after 30 seconds: resquests.get('https://example.org', timeout=30) Handle Errors base-class exception: except requests.exceptions.RequestException as e: raise SystemExit(e) catch each exception separately: except requests.exceptions.Timeout , TooManyRedirects , or RequestException raise exception for HTTP errors: try Response.raise_for_status() and except requests.exceptions.HTTPError as err Type Checking type checker module: pip install mypy examples: method signature: def add(a: int, b: int) -> int: variable declaration: agw: int = 99 Style Guidlines: PEP8 & Linters PEP8 PEP20: Python Zen PEP287: Doc-String PEP463: Exceptions-catching PEP484: Type Hints Formatting Docstrings for all public modules, functions, classes and methods imports separated in 3 groups by blank lines standard library 3rd party libraries local application/library Whitespace/punctuation identations: 4 spaces per lect max line length: 79 characters 2 lines between top-level functions and classes 1 line between methods inside a class Naming modules: short, lowercase names classes: CapitalizedNaming functions: lowercase_with_underscores constants: ALL_CAPS non-public names: _start_with_underscore Linters pylint checks for PEP8 and other code smells pip install pylint pylint <package> or pylint <package>/file.py add pylintrc file: pylint --generate-rcfile > pylintrc Black great for CI pipelines to enforce consistency in source code pip install black black <package> Generating Documentation PEP 257: semantics and conventions for docstrings Docstrings string as first statement statements of a module, function, class or method becomes the __doc__ atribute, callable in python always use \"\"\"three double quotes\"\"\" end phrases in a period methods should specify a return value Sphinx generates HTML, PDF, eBook, Linux man pages, etc. from source extracts docstrings from code accepts reStructured Text to build documentation pages generate API docs: sphinx-apidoc -o <package> Quickstart pip install sphinx mkdir docs && cd docs sphinx-quickstart make html clean old builds: make clean html Tip: Use reStructured Text in Docstrings to automatically generate documentation: module Docstring: \"\"\" module.py --------- This module contains a module-level docstring \"\"\" class MyClass: \"\"\" The MyClass class represents a generic class. Link to the :meth:`run` method. \"\"\" def run(self, name): \"\"\" Runs the class with the name of :attr:`name`. :param name : The name of the class :return: The new value of :attr:`name`. \"\"\" Package Management pip pip freeze > requirements.txt pip install requirements.txt pipenv Common Errors RescursionError = stack overflow Sets light_bulbs = set() light_bulbs.add('incandescent') light_bulbs.add('compact fluorescent') light_bulbs.add('LED') 'LED' in light_bulbs # True 'halogen' in light_bulbs # False","title":"Python"},{"location":"software/python/#python","text":"","title":"Python"},{"location":"software/python/#pyenv","text":"brew install pyenv brew install pyenv-virtualenv install version: pyenv instal [version] set version: pyenv global [version] list installed versions: pyenv versions list virtualenvs: pyenv virtualenvs activate virutalenv: pyenv activate","title":"Pyenv"},{"location":"software/python/#update-bashrc","text":"if command -v pyenv 1>/dev/null 2>&1; then eval \"$(pyenv init -)\" fi export PATH=\"$HOME/.pyenv/bin:$PATH\"","title":"Update .bashrc"},{"location":"software/python/#enumerate-iterable","text":"enumerate(iterable, start=0) for index, element in enumerate(l1,100): print index, element","title":"Enumerate Iterable"},{"location":"software/python/#higher-order-functions","text":"From Composing Programs by John DeNero, Chapter 1.6.1: def summation(n, term): total, k = 0, 1 while k <= n: total, k = total + term(k), k + 1 return total def cube(x): return x*x*x def sum_cubes(n): return summation(n, cube) result = sum_cubes(3)","title":"Higher-order functions"},{"location":"software/python/#nest-definitions-and-lexical-scope","text":"From Composing Programs by John DeNero, Chapter 1.6.3: def average(x, y): return (x + y)/2 def improve(update, close, guess=1): while not close(guess): guess = update(guess) return guess def approx_eq(x, y, tolerance=1e-3): return abs(x - y) < tolerance def sqrt(a): def sqrt_update(x): return average(x, a/x) def sqrt_close(x): return approx_eq(x * x, a) return improve(sqrt_update, sqrt_close) result = sqrt(256) Notice how the nested definition sqrt_update still has access to the a , which is a formal parameter of its enclosing function sqrt . The nested definition extends its parent environment. \"The sqrt_update function carries with it some data: the value for a referenced in the environment in which it was defined. Because they \"enclose\" information in this way, locally defined functions are often called closures .\"","title":"Nest Definitions and Lexical Scope"},{"location":"software/python/#decorators","text":"Using Decorators to time functions import time from functools import wraps def timethis(func): @wraps(func) def wrapper(*args, **kwargs): start = time.time() result = func(*args, **kwargs) end = time.time() print(func.__name__, end-start) return result return wrapper @timethis def square_world(world): list_len = int(len(world)**.5) new_list = [] for x in range(len(world)): if (x)%list_len == 0: new_list.append(world[x:x+list_len]) return new_list @timethis def square_world_modified(world): list_len = int(len(world)**.5) new_list = [] for x in range(0, len(world), list_len): section = world[x:x+list_len] new_list.append(section) def create_sample_world(size): world = [] for x in range(size): world.append(x) return world sample_worlds = (1000, 1000000, 10000000) for sample_size in sample_worlds: world = create_sample_world(sample_size) print(f'World size: {sample_size}') square_world(world) square_world_modified(world) print()","title":"Decorators"},{"location":"software/python/#python-requests-lib","text":"","title":"Python Requests Lib"},{"location":"software/python/#timeout","text":"timeout request after 30 seconds: resquests.get('https://example.org', timeout=30)","title":"Timeout"},{"location":"software/python/#handle-errors","text":"base-class exception: except requests.exceptions.RequestException as e: raise SystemExit(e) catch each exception separately: except requests.exceptions.Timeout , TooManyRedirects , or RequestException raise exception for HTTP errors: try Response.raise_for_status() and except requests.exceptions.HTTPError as err","title":"Handle Errors"},{"location":"software/python/#type-checking","text":"type checker module: pip install mypy examples: method signature: def add(a: int, b: int) -> int: variable declaration: agw: int = 99","title":"Type Checking"},{"location":"software/python/#style-guidlines-pep8-linters","text":"PEP8 PEP20: Python Zen PEP287: Doc-String PEP463: Exceptions-catching PEP484: Type Hints","title":"Style Guidlines: PEP8 &amp; Linters"},{"location":"software/python/#formatting","text":"Docstrings for all public modules, functions, classes and methods imports separated in 3 groups by blank lines standard library 3rd party libraries local application/library","title":"Formatting"},{"location":"software/python/#whitespacepunctuation","text":"identations: 4 spaces per lect max line length: 79 characters 2 lines between top-level functions and classes 1 line between methods inside a class","title":"Whitespace/punctuation"},{"location":"software/python/#naming","text":"modules: short, lowercase names classes: CapitalizedNaming functions: lowercase_with_underscores constants: ALL_CAPS non-public names: _start_with_underscore","title":"Naming"},{"location":"software/python/#linters","text":"","title":"Linters"},{"location":"software/python/#pylint","text":"checks for PEP8 and other code smells pip install pylint pylint <package> or pylint <package>/file.py add pylintrc file: pylint --generate-rcfile > pylintrc","title":"pylint"},{"location":"software/python/#black","text":"great for CI pipelines to enforce consistency in source code pip install black black <package>","title":"Black"},{"location":"software/python/#generating-documentation","text":"PEP 257: semantics and conventions for docstrings","title":"Generating Documentation"},{"location":"software/python/#docstrings","text":"string as first statement statements of a module, function, class or method becomes the __doc__ atribute, callable in python always use \"\"\"three double quotes\"\"\" end phrases in a period methods should specify a return value","title":"Docstrings"},{"location":"software/python/#sphinx","text":"generates HTML, PDF, eBook, Linux man pages, etc. from source extracts docstrings from code accepts reStructured Text to build documentation pages generate API docs: sphinx-apidoc -o <package>","title":"Sphinx"},{"location":"software/python/#quickstart","text":"pip install sphinx mkdir docs && cd docs sphinx-quickstart make html clean old builds: make clean html Tip: Use reStructured Text in Docstrings to automatically generate documentation: module Docstring: \"\"\" module.py --------- This module contains a module-level docstring \"\"\" class MyClass: \"\"\" The MyClass class represents a generic class. Link to the :meth:`run` method. \"\"\" def run(self, name): \"\"\" Runs the class with the name of :attr:`name`. :param name : The name of the class :return: The new value of :attr:`name`. \"\"\"","title":"Quickstart"},{"location":"software/python/#package-management","text":"","title":"Package Management"},{"location":"software/python/#pip","text":"pip freeze > requirements.txt pip install requirements.txt","title":"pip"},{"location":"software/python/#pipenv","text":"","title":"pipenv"},{"location":"software/python/#common-errors","text":"RescursionError = stack overflow","title":"Common Errors"},{"location":"software/python/#sets","text":"light_bulbs = set() light_bulbs.add('incandescent') light_bulbs.add('compact fluorescent') light_bulbs.add('LED') 'LED' in light_bulbs # True 'halogen' in light_bulbs # False","title":"Sets"},{"location":"software/webdev/","text":"Graphical User Interfaces Web Server Nginx Site Config Files Test config files (and show location: nginx -t Location: /etc/nginx/sites-enabled/*.conf enable site: ln -s /etc/nginx/sites-available/www.example.org.conf /etc/nginx/sites-enabled/ reload apache service: service nginx reload Apache check status: systemctl status apache2 Redirect HTTP to HTTPS vhost ServerAdmin dev@designtennis.com ServerName nedic.ca ServerAlias *.nedic.ca, www.nedic.ca Redirect 301 / https://nedic.ca/ HTTPS vhost config ServerAdmin dev@designtennis.com ServerName nedic.ca ServerAlias *.nedic.ca, www.nedic.ca DocumentRoot /srv/www/nedic.ca/public_html/ <Directory /srv/www/nedic.ca/public_html> Options +ExecCGI Require all granted </Directory> ErrorLog ${APACHE_LOG_DIR}/nedic.ca-error.log CustomLog ${APACHE_LOG_DIR}/nedic.ca-access.log combined Alias /static /srv/www/nedic.ca/public_html/static Alias /media /srv/www/nedic.ca/public_html/media SetOutputFilter DEFLATE AddOutputFilterByType DEFLATE text/html text/plain text/xml text/css application/rss+xml application/x-javascript application/atom_xml application/javascript WSGIDaemonProcess nedic.ca python-path=/srv/www/nedic.ca/env/lib/python3.6/site-packages:/srv/www/nedic.ca/application processes=2 threads=15 display-name=%{edit} WSGIProcessGroup nedic.ca WSGIScriptAlias / /srv/www/nedic.ca/public_html/wsgi.py Site Config Files Location: /etc/apache2/sites-available enable site: a2ensite {config file} disable site: a2dissite {config file} reload apache service: systemctl reload apache2` Canonical Domain (URI) RewriteEngine On RewriteCond %{HTTP_HOST} ^www\\.nedic\\.ca$ [NC] RewriteRule (.*) https://nedic.ca$1 [L,R=301] SSL Let's Encrypt Install certbot sudo add-apt-repository ppa:certbot/certbot sudo apt-get update sudo apt-get install certbot Renew Certificate should run automatically Ubuntu uses cronjob in /etc/cron.d/cerbot manually: certbot renew expand (include subdomain): ertbot -d {example.com,www.example.com} --expand View Certificates certbot certificates Automatically generated ssl config Include /etc/letsencrypt/options-ssl-apache.conf SSLCertificateFile /etc/letsencrypt/live/nedic.ca/fullchain.pem SSLCertificateKeyFile /etc/letsencrypt/live/nedic.ca/privkey.pem Flask Django Setup create virtualenv, pip install django, output django version to requirements.txt create project: startproject <project_name> create first app: <project_name>/manage.py startapp <app_name> Setup routing for first app add include function to /urls.py: from django.urls import path, include create urls.py in first app directory add path to first app: path('', include('<app_name>.urls')) add path function and urlpatterns list to first app's urls.py add views.py to first app's urls.py : from . import views add a path to first app's urlpatterns : path('', views.home, name='<app_name>-home') Create views create view functions: def home(request): add context: context = {...} render template: return render(request, '<app_name>/home.html', context) Add templates create templates directory within first app's directory create a app directory within the templates directory: <project_name>/<app_name>/templates/<app_name> create HTML templates within the templates/<app_name> directory add template inheritence: create a base.html template to be extended in all other templates (Optional) Add bootstrap in base.html from bootstrap starter template (metadata, css, and javascript) add HTML snippets replace anchor tag hrefs with url names: {% <app-name>-home %} Add static assests and media directories add static directory: <app_name>/static/<app_name> add main css file: <app_name>/static/<app_name>/main.css load static directory within template: {% load static %} load a static asset: <link rel=\"stylesheet\" href=\"{% static '<app-name>/main.css' %\"> in settings.py add the following: full path to static directory: STATIC_ROOT = os.path.join(BASE_DIR, 'static') public URL of the static directory and how it will be accessed through the browser: STATIC_URL = '/static/' full path to media directory: MEDIA_ROOT = os.path.join(BASE_DIR, 'media') public URL of the media directory and how it will be accessed through the browser: MEDIA_URL = '/media' Create admin user <project_name>/manage.py migrate <project_name>/manage.py createsuperuser Create models inherit from the Model class: class <model_name>(models.Model) reference a user foreign key: from django.contrib.auth.models import User delete anything referencing the user: user = models.ForeignKey(User, on_delete=models.CASCADE) retain anything referencing the user and set the reference to null: user = models.ForeignKey(User, on_delete=models.SET_NULL, null=True) util for using mutable time fields: from django.utils import timezone mutable date: date_created = models.DateTimeField(default=timezone.now) use data at time of add/edit: date_updated = models.DateTimeField(auto_now=True) add descriptive text when querying: def __str__(self): return self.name run makemigrations view raw SQL: manage.py sqlmigrate <app_name> <migration_number> apply migrations: migrate register in admin site: admin.site.register(<model_name>) import from .models import <model_name> Interact with Models in Django Python shell manage.py shell import users: from django.contrib.auth.models import User import models: from <app_name>.models import <model_name> get all, get id, first, or last objects: User.objects.all() or .get(1) or .first() or .last() filter results: user = User.objects.filter(username='<string>).first() check attributes: user.pk get all objects referencing a user: user.<model_name>_set.all() add new object for this user: user.<model_name>_set.create(<model_attributes>) Query database in view import model in views file: from .models import <model_name> add it into the response context: context = {'announcements': Announcement.objects.all() User creation default form: from django.contrib.auth.forms import UserCreationForm get form data: f request.method == 'POST': form = UserRegisterForm(request.POST) validate form: if form.is_valid(): create user: form.save() format form with django-crispy-forms add to installed apps: 'crispy_forms', set frontend framework in django settings: CRISPY_TEMPLATE_PACK = 'bootstrap4' log user in after registration required imports: from django.contrib.auth import authenticate, login if form.is_valid(): then login(request, authenticate(username=username, password=password)) update redirect to specific page after login in django settings: LOGIN_REDIRECT_URL = 'profile' Restrict pages to logged in users import decorator: from django.contrib.auth.decorators import login_required add decorator to any view function returning a restricted page: @login_required for class-based views: import mixin: from django.contrib.auth.mixins import LoginRequiredMixin include within class: class AccountView(LoginRequiredMixin, View): alternatively, include for specific class method: import: from django.utils.decorators import method_decorator method decorator: @method_decorator(login_required) Extend Users model import User model: from django.contrib.auth.models import User create extended model: class Profile(models.Model): create a one-to-one relationship with a user model: user = models.OneToOneField(User, on_delete=models.CASCADE) add to admin site in admin.py: from .models import Profile admin.site.register(Profile) Django Signals (to automatically execute some behavior) import post-save signal: from django.db.models.signals import post_save import sender: from django.contrib.auth.models import User import receiver: from django.dispatch import receiver import related models: from .models import Profile Jinja Registering custom filters from django_jinja import library @library.filter def mylower(name): \"\"\" Usage: {{ 'Hello'|mylower() }} \"\"\" return name.lower() Alertnative to register custom filter in Django settings.py TEMPLATES = [ { \"BACKEND\": \"django_jinja.backend.Jinja2\", \"APP_DIRS\": True, \"OPTIONS\": { \"match_extension\": \".jinja\", \"filters\": { \"myfilter\": \"path.to.filters.myfilterfn\", ... } } }] WordPress export db: wp db export import db: wp db import XAMPP \"env: mysql: No such file or directory after wp import \" export mysql to path: export PATH=$PATH:/Applications/XAMPP/xamppfiles/bin \"XAMPP (Mac) Virtual host showing 403\" \"To fix this you can configure Apache to run as your OS X user. Open /Applications/XAMPP/xamppfiles/etc/httpd.conf and look for the following lines:\" User/Group: The name (or #number) of the user/group to run httpd as. It is usually good practice to create a dedicated user and group for running httpd, as with most system services. User daemon Group daemon Change User to your OS X username or add new user as follow, and save the file: User yourusername Vue JavaScript install node and npm: brew install nvm list versions: nvm list install node version (comes with specific npm version): nvm install [version] show current version: nvm current use a different version: nvm use [version] CSS HTML VSCode pull up intellisense on mac: Option + Esc ( Alt on Windows) User Snippets Django code block snippets for html templates: { \"Django for-loop code block\": { \"prefix\": [\"for\", \"django\", \"code\", \"code-block\"], \"body\": [\"{% for ${1:element} in ${2:iterable} %}\", \"\\t$0\",\"{% endfor %}\"] }, \"Django if code block\": { \"prefix\": [\"if\", \"django\", \"code\", \"code-block\"], \"body\": [\"{% if ${1:condition} %}\", \"\\t$0\",\"{% endif %}\"] }, \"Django if/else code block\": { \"prefix\": [\"if/else\", \"django\", \"code\", \"code-block\"], \"body\": [\"{% if ${1:condition} %}\", \"\\t$0\", \"{% else %}\", \"\\t\", \"{% endif %}\"] }, \"Django if/elif code block\": { \"prefix\": [\"if/elif\", \"django\", \"code\", \"code-block\"], \"body\": [\"{% if ${1:condition} %}\", \"\\t$0\", \"{% elif ${2:condition} %}\", \"\\t\", \"{% endif %}\"] }, \"Django if/elif/else code block\": { \"prefix\": [\"if/elif/else\", \"django\", \"code\", \"code-block\"], \"body\": [\"{% if ${1:condition} %}\", \"\\t$0\", \"{% elif ${2:condition} %}\", \"\\t\", \"{% else %}\", \"\\t\", \"{% endif %}\"] } } Set Syntax Command Pallete > languages \"Change Language Mode\" e.g. \"SQL Formatter\" Extension Format File in Syntax Shift + Option + F (Mac) Resources Vue UI Libarry HTML Tempaltes html5up templated Notes Block entire subdomain from getting indexed by search engines: robot.txt in root dir User-agent: * Disallow: / Block a specific HTML page: <meta name=\"robots\" content=\"noindex\"> in <head>","title":"WebDev"},{"location":"software/webdev/#graphical-user-interfaces","text":"","title":"Graphical User Interfaces"},{"location":"software/webdev/#web-server","text":"","title":"Web Server"},{"location":"software/webdev/#nginx","text":"","title":"Nginx"},{"location":"software/webdev/#site-config-files","text":"Test config files (and show location: nginx -t Location: /etc/nginx/sites-enabled/*.conf enable site: ln -s /etc/nginx/sites-available/www.example.org.conf /etc/nginx/sites-enabled/ reload apache service: service nginx reload","title":"Site Config Files"},{"location":"software/webdev/#apache","text":"check status: systemctl status apache2","title":"Apache"},{"location":"software/webdev/#redirect-http-to-https-vhost","text":"ServerAdmin dev@designtennis.com ServerName nedic.ca ServerAlias *.nedic.ca, www.nedic.ca Redirect 301 / https://nedic.ca/","title":"Redirect HTTP to HTTPS vhost"},{"location":"software/webdev/#https-vhost-config","text":"ServerAdmin dev@designtennis.com ServerName nedic.ca ServerAlias *.nedic.ca, www.nedic.ca DocumentRoot /srv/www/nedic.ca/public_html/ <Directory /srv/www/nedic.ca/public_html> Options +ExecCGI Require all granted </Directory> ErrorLog ${APACHE_LOG_DIR}/nedic.ca-error.log CustomLog ${APACHE_LOG_DIR}/nedic.ca-access.log combined Alias /static /srv/www/nedic.ca/public_html/static Alias /media /srv/www/nedic.ca/public_html/media SetOutputFilter DEFLATE AddOutputFilterByType DEFLATE text/html text/plain text/xml text/css application/rss+xml application/x-javascript application/atom_xml application/javascript WSGIDaemonProcess nedic.ca python-path=/srv/www/nedic.ca/env/lib/python3.6/site-packages:/srv/www/nedic.ca/application processes=2 threads=15 display-name=%{edit} WSGIProcessGroup nedic.ca WSGIScriptAlias / /srv/www/nedic.ca/public_html/wsgi.py","title":"HTTPS vhost config"},{"location":"software/webdev/#site-config-files_1","text":"Location: /etc/apache2/sites-available enable site: a2ensite {config file} disable site: a2dissite {config file} reload apache service: systemctl reload apache2`","title":"Site Config Files"},{"location":"software/webdev/#canonical-domain-uri","text":"RewriteEngine On RewriteCond %{HTTP_HOST} ^www\\.nedic\\.ca$ [NC] RewriteRule (.*) https://nedic.ca$1 [L,R=301]","title":"Canonical Domain (URI)"},{"location":"software/webdev/#ssl","text":"","title":"SSL"},{"location":"software/webdev/#lets-encrypt","text":"","title":"Let's Encrypt"},{"location":"software/webdev/#install-certbot","text":"sudo add-apt-repository ppa:certbot/certbot sudo apt-get update sudo apt-get install certbot","title":"Install certbot"},{"location":"software/webdev/#renew-certificate","text":"should run automatically Ubuntu uses cronjob in /etc/cron.d/cerbot manually: certbot renew expand (include subdomain): ertbot -d {example.com,www.example.com} --expand","title":"Renew Certificate"},{"location":"software/webdev/#view-certificates","text":"certbot certificates","title":"View Certificates"},{"location":"software/webdev/#automatically-generated-ssl-config","text":"Include /etc/letsencrypt/options-ssl-apache.conf SSLCertificateFile /etc/letsencrypt/live/nedic.ca/fullchain.pem SSLCertificateKeyFile /etc/letsencrypt/live/nedic.ca/privkey.pem","title":"Automatically generated ssl config"},{"location":"software/webdev/#flask","text":"","title":"Flask"},{"location":"software/webdev/#django","text":"Setup create virtualenv, pip install django, output django version to requirements.txt create project: startproject <project_name> create first app: <project_name>/manage.py startapp <app_name> Setup routing for first app add include function to /urls.py: from django.urls import path, include create urls.py in first app directory add path to first app: path('', include('<app_name>.urls')) add path function and urlpatterns list to first app's urls.py add views.py to first app's urls.py : from . import views add a path to first app's urlpatterns : path('', views.home, name='<app_name>-home') Create views create view functions: def home(request): add context: context = {...} render template: return render(request, '<app_name>/home.html', context) Add templates create templates directory within first app's directory create a app directory within the templates directory: <project_name>/<app_name>/templates/<app_name> create HTML templates within the templates/<app_name> directory add template inheritence: create a base.html template to be extended in all other templates (Optional) Add bootstrap in base.html from bootstrap starter template (metadata, css, and javascript) add HTML snippets replace anchor tag hrefs with url names: {% <app-name>-home %} Add static assests and media directories add static directory: <app_name>/static/<app_name> add main css file: <app_name>/static/<app_name>/main.css load static directory within template: {% load static %} load a static asset: <link rel=\"stylesheet\" href=\"{% static '<app-name>/main.css' %\"> in settings.py add the following: full path to static directory: STATIC_ROOT = os.path.join(BASE_DIR, 'static') public URL of the static directory and how it will be accessed through the browser: STATIC_URL = '/static/' full path to media directory: MEDIA_ROOT = os.path.join(BASE_DIR, 'media') public URL of the media directory and how it will be accessed through the browser: MEDIA_URL = '/media' Create admin user <project_name>/manage.py migrate <project_name>/manage.py createsuperuser Create models inherit from the Model class: class <model_name>(models.Model) reference a user foreign key: from django.contrib.auth.models import User delete anything referencing the user: user = models.ForeignKey(User, on_delete=models.CASCADE) retain anything referencing the user and set the reference to null: user = models.ForeignKey(User, on_delete=models.SET_NULL, null=True) util for using mutable time fields: from django.utils import timezone mutable date: date_created = models.DateTimeField(default=timezone.now) use data at time of add/edit: date_updated = models.DateTimeField(auto_now=True) add descriptive text when querying: def __str__(self): return self.name run makemigrations view raw SQL: manage.py sqlmigrate <app_name> <migration_number> apply migrations: migrate register in admin site: admin.site.register(<model_name>) import from .models import <model_name> Interact with Models in Django Python shell manage.py shell import users: from django.contrib.auth.models import User import models: from <app_name>.models import <model_name> get all, get id, first, or last objects: User.objects.all() or .get(1) or .first() or .last() filter results: user = User.objects.filter(username='<string>).first() check attributes: user.pk get all objects referencing a user: user.<model_name>_set.all() add new object for this user: user.<model_name>_set.create(<model_attributes>) Query database in view import model in views file: from .models import <model_name> add it into the response context: context = {'announcements': Announcement.objects.all() User creation default form: from django.contrib.auth.forms import UserCreationForm get form data: f request.method == 'POST': form = UserRegisterForm(request.POST) validate form: if form.is_valid(): create user: form.save() format form with django-crispy-forms add to installed apps: 'crispy_forms', set frontend framework in django settings: CRISPY_TEMPLATE_PACK = 'bootstrap4' log user in after registration required imports: from django.contrib.auth import authenticate, login if form.is_valid(): then login(request, authenticate(username=username, password=password)) update redirect to specific page after login in django settings: LOGIN_REDIRECT_URL = 'profile' Restrict pages to logged in users import decorator: from django.contrib.auth.decorators import login_required add decorator to any view function returning a restricted page: @login_required for class-based views: import mixin: from django.contrib.auth.mixins import LoginRequiredMixin include within class: class AccountView(LoginRequiredMixin, View): alternatively, include for specific class method: import: from django.utils.decorators import method_decorator method decorator: @method_decorator(login_required) Extend Users model import User model: from django.contrib.auth.models import User create extended model: class Profile(models.Model): create a one-to-one relationship with a user model: user = models.OneToOneField(User, on_delete=models.CASCADE) add to admin site in admin.py: from .models import Profile admin.site.register(Profile) Django Signals (to automatically execute some behavior) import post-save signal: from django.db.models.signals import post_save import sender: from django.contrib.auth.models import User import receiver: from django.dispatch import receiver import related models: from .models import Profile","title":"Django"},{"location":"software/webdev/#jinja","text":"","title":"Jinja"},{"location":"software/webdev/#registering-custom-filters","text":"from django_jinja import library @library.filter def mylower(name): \"\"\" Usage: {{ 'Hello'|mylower() }} \"\"\" return name.lower()","title":"Registering custom filters"},{"location":"software/webdev/#alertnative-to-register-custom-filter-in-django-settingspy","text":"TEMPLATES = [ { \"BACKEND\": \"django_jinja.backend.Jinja2\", \"APP_DIRS\": True, \"OPTIONS\": { \"match_extension\": \".jinja\", \"filters\": { \"myfilter\": \"path.to.filters.myfilterfn\", ... } } }]","title":"Alertnative to register custom filter in Django settings.py"},{"location":"software/webdev/#wordpress","text":"export db: wp db export import db: wp db import","title":"WordPress"},{"location":"software/webdev/#xampp","text":"\"env: mysql: No such file or directory after wp import \" export mysql to path: export PATH=$PATH:/Applications/XAMPP/xamppfiles/bin \"XAMPP (Mac) Virtual host showing 403\" \"To fix this you can configure Apache to run as your OS X user. Open /Applications/XAMPP/xamppfiles/etc/httpd.conf and look for the following lines:\" User/Group: The name (or #number) of the user/group to run httpd as. It is usually good practice to create a dedicated user and group for running httpd, as with most system services. User daemon Group daemon Change User to your OS X username or add new user as follow, and save the file: User yourusername","title":"XAMPP"},{"location":"software/webdev/#vue","text":"","title":"Vue"},{"location":"software/webdev/#javascript","text":"install node and npm: brew install nvm list versions: nvm list install node version (comes with specific npm version): nvm install [version] show current version: nvm current use a different version: nvm use [version]","title":"JavaScript"},{"location":"software/webdev/#css","text":"","title":"CSS"},{"location":"software/webdev/#html","text":"","title":"HTML"},{"location":"software/webdev/#vscode","text":"pull up intellisense on mac: Option + Esc ( Alt on Windows)","title":"VSCode"},{"location":"software/webdev/#user-snippets","text":"Django code block snippets for html templates: { \"Django for-loop code block\": { \"prefix\": [\"for\", \"django\", \"code\", \"code-block\"], \"body\": [\"{% for ${1:element} in ${2:iterable} %}\", \"\\t$0\",\"{% endfor %}\"] }, \"Django if code block\": { \"prefix\": [\"if\", \"django\", \"code\", \"code-block\"], \"body\": [\"{% if ${1:condition} %}\", \"\\t$0\",\"{% endif %}\"] }, \"Django if/else code block\": { \"prefix\": [\"if/else\", \"django\", \"code\", \"code-block\"], \"body\": [\"{% if ${1:condition} %}\", \"\\t$0\", \"{% else %}\", \"\\t\", \"{% endif %}\"] }, \"Django if/elif code block\": { \"prefix\": [\"if/elif\", \"django\", \"code\", \"code-block\"], \"body\": [\"{% if ${1:condition} %}\", \"\\t$0\", \"{% elif ${2:condition} %}\", \"\\t\", \"{% endif %}\"] }, \"Django if/elif/else code block\": { \"prefix\": [\"if/elif/else\", \"django\", \"code\", \"code-block\"], \"body\": [\"{% if ${1:condition} %}\", \"\\t$0\", \"{% elif ${2:condition} %}\", \"\\t\", \"{% else %}\", \"\\t\", \"{% endif %}\"] } }","title":"User Snippets"},{"location":"software/webdev/#set-syntax","text":"Command Pallete > languages \"Change Language Mode\" e.g. \"SQL Formatter\" Extension","title":"Set Syntax"},{"location":"software/webdev/#format-file-in-syntax","text":"Shift + Option + F (Mac)","title":"Format File in Syntax"},{"location":"software/webdev/#resources","text":"Vue UI Libarry HTML Tempaltes html5up templated","title":"Resources"},{"location":"software/webdev/#notes","text":"Block entire subdomain from getting indexed by search engines: robot.txt in root dir User-agent: * Disallow: / Block a specific HTML page: <meta name=\"robots\" content=\"noindex\"> in <head>","title":"Notes"}]}